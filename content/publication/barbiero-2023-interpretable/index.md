---
title: Interpretable Neural-Symbolic Concept Reasoning
authors:
- Pietro Barbiero
- Gabriele Ciravegna
- Francesco Giannini
- Mateo Espinosa Zarlenga
- Lucie Charlotte Magister
- Alberto Tonda
- Pietro Lio
- Frederic Precioso
- Mateja Jamnik
- Giuseppe Marra
date: '2023-07-01'
publishDate: '2024-12-13T18:55:57.946982Z'
publication_types:
- paper-conference
publication: '*Proceedings of the 40th International Conference on Machine Learning*'
abstract: 'Deep learning methods are highly accurate, yet their opaque decision process
  prevents them from earning full human trust. Concept-based models aim to address
  this issue by learning tasks based on a set of human-understandable concepts. However,
  state-of-the-art concept-based models rely on high-dimensional concept embedding
  representations which lack a clear semantic meaning, thus questioning the interpretability
  of their decision process. To overcome this limitation, we propose the Deep Concept
  Reasoner (DCR), the first interpretable concept-based model that builds upon concept
  embeddings. In DCR, neural networks do not make task predictions directly, but they
  build syntactic rule structures using concept embeddings. DCR then executes these
  rules on meaningful concept truth degrees to provide a final interpretable and semantically-consistent
  prediction in a differentiable manner. Our experiments show that DCR: (i) improves
  up to +25% w.r.t. state-of-the-art interpretable concept-based models on challenging
  benchmarks (ii) discovers meaningful logic rules matching known ground truths even
  in the absence of concept supervision during training, and (iii), facilitates the
  generation of counterfactual examples providing the learnt rules as guidance.'
---
