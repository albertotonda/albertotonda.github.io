---
title: Interpretable AI
summary: Obtaining human-readable, interpretable data-driven models
date: 2024-12-13
featured: true
tags:
  - Artificial intelligence
  - Research
  - Interpretability
---
While generally effective, models obtained through machine learning (ML) are often black boxes, as it is practically impossible for humans to infer their decision processes, due to their sheer complexity. Some of my early works dealt with [creating white-box, human-readable ML models such as systems of Ordinary Differential Equations (ODEs)]({{< relref "/publication/gaucel-2014-learning/index.md" >}}), and [integrating]({{< relref "/publication/tonda-2014-balancing/index.md" >}}) [expert knowledge]({{< relref "/publication/tonda-2013-amemetic/index.md" >}}) with [data-driven models]({{< relref "/publication/tonda-2012-bayesian/index.md" >}}). More recently I investigated the relationships between [datasets characteristics and generalization abilities of models]({{<relref "/publication/barbiero-2020-modeling/index.md">}}), and experimented with the translation of Explainable AI (XAI) techniques from the field of image analysis to genomic data~\cite{lopezrincon2021classification}. Furthermore, I explored different approaches to feature and sample selection, the process of identifying the most compact sect of meaningful information to explain a ML algorithm decisions on a target problem~\cite{barbiero2020generating,barbiero2020making,barbiero2020anovel,ciravegna2020discovering,barbiero2019fundamental,barbiero2019beyond,barbiero2019evolutionary}. As one of the leads in this research line includes combining EAs and ML, I am recognized as one of the experts of this niche, and as a consequence I co-organized tutorials on the subject in several specialized conferences (ACM GECCO, PPSN) plus an invited lecture in the summer school organized by COST Action CA15140 \emph{Improving Applicability of Nature-Inspired Optimisation by Joining Theory and Practise}