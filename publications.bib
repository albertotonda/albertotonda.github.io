@article{gandini2010aframework,
  doi = {10.1007/s10836-010-5184-5},
  url = {https://doi.org/10.1007/s10836-010-5184-5},
  year = {2010},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {26},
  number = {6},
  pages = {689--697},
  author = {Stefano Gandini and Walter Ruzzarin and Ernesto Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {A Framework for Automated Detection of Power-related Software Errors in Industrial Verification Processes},
  journal = {Journal of Electronic Testing},
  abstract = {The complexity of cell phones is continually increasing, with regards to both hardware and software parts. As many complex devices, their components are usually designed and verified separately by specialized teams of engineers and programmers. However, even if each isolated part is working flawlessly, it often happens that bugs in one software application arise due to the interaction with other modules. Those software misbehaviors become particularly critical when they affect the residual battery life, causing power dissipation. An automatic approach to detect power-affecting software defects is proposed. The approach is intended to be part of a qualifying verification plan and complete human expertise. Motorola, always at the forefront of researching innovations in the product development chain, experimented the approach on a mobile phone prototype during a partnership with Politecnico di Torino. Software errors unrevealed by all human-designed tests have been detected by the proposed framework, two out of three critical from the power consumption point of view, thus enabling Motorola to further improve its verification plans. Details of the tests and experimental results are presented.}
}

@article{grosso2011functional,
  doi = {10.1007/s10836-011-5219-6},
  url = {https://doi.org/10.1007/s10836-011-5219-6},
  year = {2011},
  month = apr,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {27},
  number = {4},
  pages = {505--516},
  author = {Michelangelo Grosso and Wilson Javier Perez Holguin and Danilo Ravotto and Ernesto Sanchez and Matteo Sonza Reorda and Alberto Tonda and Jaime Velasco Medina},
  title = {Functional Verification of {DMA} Controllers},
  journal = {Journal of Electronic Testing},
  abstract = {Today’s SoCs are composed of a wide variety of modules, such as microprocessor cores, memories, peripherals, and customized blocks directly related to the targeted application. To effectively perform simulation-based design verification of peripheral cores, it is necessary to stimulate the description in a broad range of behavior possibilities, checking the produced results. Different strategies for generating suitable stimuli have been proposed by the research community to functionally verify these modules and their interconnection when embedded in a SoC: however, their verification often remains a largely manual and unstructured operation. In this paper we describe a general approach to develop concise and effective sets of inputs by modeling the configuration modes of a peripheral with a graph, and creating paths able to cover all of its nodes: proper stimuli for the device are then directly derived from the paths. The resulting inputs sequences are aimed at design verification of system peripherals such as DMA controllers, and can be applied via simulation by means of dedicated testbenches or by setting up an environment including a processor, which executes a proper test priogram. In the latter case, the developed programs can be exploited in later stages for testing, by adding suitable observability features. Experimental results demonstrating the method effectiveness are reported.}
}

@article{dicarlo2011increasing,
  doi = {10.1016/j.patrec.2011.05.019},
  url = {https://doi.org/10.1016/j.patrec.2011.05.019},
  year = {2011},
  month = oct,
  publisher = {Elsevier {BV}},
  volume = {32},
  number = {13},
  pages = {1594--1603},
  author = {S. Di Carlo and M. Falasconi and E. Sanchez and A. Scionti and G. Squillero and A. Tonda},
  title = {Increasing pattern recognition accuracy for chemical sensing by evolutionary based drift compensation},
  journal = {Pattern Recognition Letters},
  abstract = {Artificial olfaction systems, which mimic human olfaction by using arrays of gas chemical sensors combined with pattern recognition methods, represent a potentially low-cost tool in many areas of industry such as perfumery, food and drink production, clinical diagnosis, health and safety, environmental monitoring and process control. However, successful applications of these systems are still largely limited to specialized laboratories. Sensor drift, i.e., the lack of a sensor’s stability over time, still limits real industrial setups. This paper presents and discusses an evolutionary based adaptive drift-correction method designed to work with state-of-the-art classification systems. The proposed approach exploits a cutting-edge evolutionary strategy to iteratively tweak the coefficients of a linear transformation which can transparently correct raw sensors’ measures thus mitigating the negative effects of the drift. The method learns the optimal correction strategy without the use of models or other hypotheses on the behavior of the physical chemical sensors.}
}

@article{grosso2012software,
  doi = {10.1007/s10836-012-5287-2},
  url = {https://doi.org/10.1007/s10836-012-5287-2},
  year = {2012},
  month = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {28},
  number = {2},
  pages = {189--200},
  author = {M. Grosso and W. J. Perez Holguin and E. Sanchez and M. Sonza Reorda and A. Tonda and J. Velasco Medina},
  title = {Software-Based Testing for System Peripherals},
  journal = {Journal of Electronic Testing},
  abstract = {Software-based self-testing strategies have been mainly proposed to tackle microprocessor testing, but may also be applied to peripheral testing. However, testing system peripherals (e.g., DMA controllers, interrupt controllers, and internal counters) is a challenging task, since their observability and controllability are even more reduced when compared to microprocessors and to peripherals devoted to I/O communication (e.g., serial or parallel ports). In this paper an approach to develop functional tests for system peripherals is proposed. The presented methodology requires two correlated phases: module configuration and module operation. The first one prepares the peripheral to work in the different operation modes, whereas the second one is in charge of exciting the whole device and observing its behavior. We propose a methodology that guides the test engineer in building a compact set of test programs able to reach high structural fault coverage levels in a short time. Experimental results demonstrating the method effectiveness for two real-world case studies are finally reported.}
}

@article{tonda2012abenchmark,
  doi = {10.1007/s12293-012-0095-x},
  url = {https://doi.org/10.1007/s12293-012-0095-x},
  year = {2012},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {4},
  number = {4},
  pages = {263--277},
  author = {Alberto Tonda and Evelyne Lutton and Giovanni Squillero},
  title = {A benchmark for cooperative coevolution},
  journal = {Memetic Computing},
  abstract = {Cooperative co-evolution algorithms (CCEA) are a thriving sub-field of evolutionary computation. This class of algorithms makes it possible to exploit more efficiently the artificial Darwinist scheme, as soon as an optimisation problem can be turned into a co-evolution of interdependent sub-parts of the searched solution. Testing the efficiency of new CCEA concepts, however, it is not straightforward: while there is a rich literature of benchmarks for more traditional evolutionary techniques, the same does not hold true for this relatively new paradigm. We present a benchmark problem designed to study the behavior and performance of CCEAs, modeling a search for the optimal placement of a set of lamps inside a room. The relative complexity of the problem can be adjusted by operating on a single parameter. The fitness function is a trade-off between conflicting objectives, so the performance of an algorithm can be examined by making use of different metrics. We show how three different cooperative strategies, Parisian Evolution, Group Evolution and Allopatric Group Evolution, can be applied to the problem. Using a Classical Evolution approach as comparison, we analyse the behavior of each algorithm in detail, with respect to the size of the problem.}
}

@article{bucur2014theimpact,
  doi = {10.1016/j.asoc.2013.12.002},
  url = {https://doi.org/10.1016/j.asoc.2013.12.002},
  year = {2014},
  month = mar,
  publisher = {Elsevier {BV}},
  volume = {16},
  pages = {210--222},
  author = {Doina Bucur and Giovanni Iacca and Giovanni Squillero and Alberto Tonda},
  title = {The impact of topology on energy consumption for collection tree protocols: An experimental assessment through evolutionary computation},
  journal = {Applied Soft Computing},
  abstract = {The analysis of worst-case behavior in wireless sensor networks is an extremely difficult task, due to the complex interactions that characterize the dynamics of these systems. In this paper, we present a new methodology for analyzing the performance of routing protocols used in such networks. The approach exploits a stochastic optimization technique, specifically an evolutionary algorithm, to generate a large, yet tractable, set of critical network topologies; such topologies are then used to infer general considerations on the behaviors under analysis. As a case study, we focused on the energy consumption of two well-known ad hoc routing protocols for sensor networks: the multi-hop link quality indicator and the collection tree protocol. The evolutionary algorithm started from a set of randomly generated topologies and iteratively enhanced them, maximizing a measure of “how interesting” such topologies are with respect to the analysis. In the second step, starting from the gathered evidence, we were able to define concrete, protocol-independent topological metrics which correlate well with protocols’ poor performances. Finally, we discovered a causal relation between the presence of cycles in a disconnected network, and abnormal network traffic. Such creative processes were made possible by the availability of a set of meaningful topology examples. Both the proposed methodology and the specific results presented here – that is, the new topological metrics and the causal explanation – can be fruitfully reused in different contexts, even beyond wireless sensor networks.}
}

@article{lutton2014food,
  doi = {10.1016/j.ifset.2014.02.003},
  url = {https://doi.org/10.1016/j.ifset.2014.02.003},
  year = {2014},
  month = oct,
  publisher = {Elsevier {BV}},
  volume = {25},
  pages = {67--77},
  author = {Evelyne Lutton and Alberto Tonda and S{\'{e}}bastien Gaucel and Alain Riaublanc and Nathalie Perrot},
  title = {Food model exploration through evolutionary optimisation coupled with visualisation: Application to the prediction of a milk gel structure},
  journal = {Innovative Food Science {\&} Emerging Technologies},
  abstract = {Obtaining reliable in-silico food models is fundamental for a better understanding of these systems. The complex phenomena involved in these real-world processes reflect in the intricate structure of models, so that thoroughly exploring their behaviour and, for example, finding meaningful correlations between variables, become a relevant challenge for the experts. In this paper, we present a methodology based on visualisation and evolutionary computation to assist experts during model exploration. The proposed approach is tested on an established model of milk gel structures, and we show how experts are eventually able to find a correlation between two parameters, previously considered independent. Reverse-engineering the final outcome, the emergence of such a pattern is proved by physical laws underlying the oil–water interface colonisation. It is interesting to notice that, while the present work is focused on milk gel modelling, the proposed methodology can be straightforwardly generalised to other complex physical phenomena. Industrial relevance: Sustainability is nowadays at the heart of industrial requirements. The development of mathematical approaches should facilitate common approaches to risk/benefit assessment and nutritional quality in food research and industry. These models will enhance knowledge on process–structure–property relationships from the molecular to macroscopic level, and facilitate the creation of in-silico simulators with functional and nutritional properties. The stochastic optimisation techniques (evolutionary algorithms) employed in these works allow the users to thoroughly explore the systems: when coupled with visualisation, they make it possible to provide the experts with a restricted set of significant data, helping them to highlight eventual issues or possible improvements in the model. With regard to the complexity of the food systems and dynamics, the challenge of the mathematical approaches is to realise a complete dynamic description of food processing. In order to reach this objective, it is mandatory to use innovative strategies, exploiting the most recent advances in cognitive and complex system sciences.}
}

@article{gaudesi2016exploiting,
  doi = {10.1109/tciaig.2015.2439061},
  url = {https://doi.org/10.1109/tciaig.2015.2439061},
  year = {2016},
  month = sep,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {8},
  number = {3},
  pages = {288--300},
  author = {Marco Gaudesi and Elio Piccolo and Giovanni Squillero and Alberto Tonda},
  title = {Exploiting Evolutionary Modeling to Prevail in Iterated Prisoner's Dilemma Tournaments},
  journal = {{IEEE} Transactions on Computational Intelligence and {AI} in Games},
  abstract = {The iterated prisoner's dilemma is a famous model of cooperation and conflict in game theory. Its origin can be traced back to the Cold War, and countless strategies for playing it have been proposed so far, either designed by hand or automatically generated by computers. In the 2000s, scholars started focusing on adaptive players, that is, able to classify their opponent's behavior and adopt an effective counter-strategy. The player presented in this paper, pushes such idea even further: it builds a model of the current adversary from scratch, without relying on any pre-defined archetypes, and tweaks it as the game develops using an evolutionary algorithm; at the same time, it exploits the model to lead the game into the most favorable continuation. Models are compact nondeterministic finite state machines; they are extremely efficient in predicting opponents' replies, without being completely correct by necessity. Experimental results show that such a player is able to win several one-to-one games against strong opponents taken from the literature, and that it consistently prevails in round-robin tournaments of different sizes.}
}

@article{bucur2016optimizing,
  doi = {10.1016/j.asoc.2015.11.024},
  url = {https://doi.org/10.1016/j.asoc.2015.11.024},
  year = {2016},
  month = mar,
  publisher = {Elsevier {BV}},
  volume = {40},
  pages = {416--426},
  author = {Doina Bucur and Giovanni Iacca and Marco Gaudesi and Giovanni Squillero and Alberto Tonda},
  title = {Optimizing groups of colluding strong attackers in mobile urban communication networks with evolutionary algorithms},
  journal = {Applied Soft Computing},
  abstract = {In novel forms of the Social Internet of Things, any mobile user within communication range may help routing messages for another user in the network. The resulting message delivery rate depends both on the users’ mobility patterns and the message load in the network. This new type of configuration, however, poses new challenges to security, amongst them, assessing the effect that a group of colluding malicious participants can have on the global message delivery rate in such a network is far from trivial. In this work, after modeling such a question as an optimization problem, we are able to find quite interesting results by coupling a network simulator with an evolutionary algorithm. The chosen algorithm is specifically designed to solve problems whose solutions can be decomposed into parts sharing the same structure. We demonstrate the effectiveness of the proposed approach on two medium-sized Delay-Tolerant Networks, realistically simulated in the urban contexts of two cities with very different route topology: Venice and San Francisco. In all experiments, our methodology produces attack patterns that greatly lower network performance with respect to previous studies on the subject, as the evolutionary core is able to exploit the specific weaknesses of each target configuration.}
}

@article{squillero2016divergence,
  doi = {10.1016/j.ins.2015.09.056},
  url = {https://doi.org/10.1016/j.ins.2015.09.056},
  year = {2016},
  month = feb,
  publisher = {Elsevier {BV}},
  volume = {329},
  pages = {782--799},
  author = {Giovanni Squillero and Alberto Tonda},
  title = {Divergence of character and premature convergence: A survey of methodologies for promoting diversity in evolutionary optimization},
  journal = {Information Sciences},
  abstract = {In the past decades, different evolutionary optimization methodologies have been proposed by scholars and exploited by practitioners, in a wide range of applications. Each paradigm shows distinctive features, typical advantages, and characteristic disadvantages; however, one single problem is shared by almost all of them: the ``lack of speciation''. While natural selection favors variations toward greater divergence, in artificial evolution candidate solutions do homologize. Many authors argued that promoting diversity would be beneficial in evolutionary optimization processes, and that it could help avoiding premature convergence on sub-optimal solutions. The paper surveys the research in this area up to mid 2010s, it re-orders and re-interprets different methodologies into a single framework, and proposes a novel three-axis taxonomy. Its goal is to provide the reader with a unifying view of the many contributions in this important corpus, allowing comparisons and informed choices. Characteristics of the different techniques are discussed, and similarities are highlighted; practical ways to measure and promote diversity are also suggested.}
}


@article{perrot2016someremarks,
  doi = {10.1016/j.tifs.2015.10.003},
  url = {https://doi.org/10.1016/j.tifs.2015.10.003},
  year = {2016},
  month = feb,
  publisher = {Elsevier {BV}},
  volume = {48},
  pages = {88--101},
  author = {Nathalie Perrot and Hugo De Vries and Evelyne Lutton and Harald G.J. van Mil and Mechthild Donner and Alberto Tonda and Sophie Martin and Isabelle Alvarez and Paul Bourgine and Erik van der Linden and Monique A.V. Axelos},
  title = {Some remarks on computational approaches towards sustainable complex agri-food systems},
  journal = {Trends in Food Science {\&} Technology},
  abstract = {Background: Agri-food is one of the most important sectors of the industry in Europe and potentially a major contributor to the global warming. Sustainability issues in this context pose a huge challenge for several reasons: the variety of considered scales, the number of disciplines involved, the uncertainties, the out-of-equilibrium states, the complex quantitative and qualitative factors, the normative issues and the availability of data. Although important insight and breakthroughs have been attained in different scientific domains, an overarching and integrated analysis of these complex problems have yet to be realized. Scope and Approach: This context creates huge opportunities for research in interaction with mathematical programming, integrative models and decision-support tools. The paper propose a computational viewpoint including questions of holistic approach, multiscale reconstruction and optimization. Some directions are discussed. Key Findings and Conclusions: Several research questions based on a mathematical programming framework are emerging: how can such a framework manage uncertainty, cope with complex qualitative and quantitative information essential for social and environmental considerations, encompass diverse scales in space and time, cope with a multivariable dynamic environment and with scarcity of data. Moreover, how can it deal with different perspectives, types of models, research goals and data produced by conceptually disjoint scientific disciplines, ranging from physics and physiology to sociology and ethics? Building models is essential, but highly difficult; it will need a strong iterative interaction combining computational intensive methods, formal reasoning and the experts of the different fields. Some future research directions are proposed, involving all those dimensions: mathematical resilience, human-machine interactive learning and optimization techniques.}
}

@article{deplano2016anatomy,
  doi = {10.1007/s12065-016-0144-3},
  url = {https://doi.org/10.1007/s12065-016-0144-3},
  year = {2016},
  month = sep,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {9},
  number = {4},
  pages = {125--136},
  author = {Igor Deplano and Giovanni Squillero and Alberto Tonda},
  title = {Anatomy of a portfolio optimizer under a limited budget constraint},
  journal = {Evolutionary Intelligence},
  abstract = {Predicting the market’s behavior to profit from trading stocks is far from trivial. Such a task becomes even harder when investors do not have large amounts of money available, and thus cannot influence this complex system in any way. Machine learning paradigms have been already applied to financial forecasting, but usually with no restrictions on the size of the investor’s budget. In this paper, we analyze an evolutionary portfolio optimizer for the management of limited budgets, dissecting each part of the framework, discussing in detail the issues and the motivations that led to the final choices. Expected returns are modeled resorting to artificial neural networks trained on past market data, and the portfolio composition is chosen by approximating the solution to a multi-objective constrained problem. An investment simulator is eventually used to measure the portfolio performance. The proposed approach is tested on real-world data from New York’s, Milan’s and Paris’ stock exchanges, exploiting data from June 2011 to May 2014 to train the framework, and data from June 2014 to July 2015 to validate it. Experimental results demonstrate that the presented tool is able to obtain a more than satisfying profit for the considered time frame.}
}

@article{squillero2017overrealism,
  doi = {10.1007/s10710-017-9295-y},
  url = {https://doi.org/10.1007/s10710-017-9295-y},
  year = {2017},
  month = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {18},
  number = {3},
  pages = {391--393},
  author = {G. Squillero and A. Tonda},
  title = {(Over-)Realism in evolutionary computation: Commentary on {\textquotedblleft}On the Mapping of Genotype to Phenotype in Evolutionary Algorithms{\textquotedblright} by Peter A. Whigham,  Grant Dick,  and James Maclaurin},
  journal = {Genetic Programming and Evolvable Machines},
  abstract = {Inspiring metaphors play an important role in the beginning of an investigation, but are less important in a mature research field as the real phenomena involved are understood. Nowadays, in evolutionary computation, biological analogies should be taken into consideration only if they deliver significant advantages.}
}

@article{versino2017datadriven,
  doi = {10.1016/j.cma.2017.02.016},
  url = {https://doi.org/10.1016/j.cma.2017.02.016},
  year = {2017},
  month = may,
  publisher = {Elsevier {BV}},
  volume = {318},
  pages = {981--1004},
  author = {Daniele Versino and Alberto Tonda and Curt A. Bronkhorst},
  title = {Data driven modeling of plastic deformation},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  abstract = {In this paper the application of machine learning techniques for the development of constitutive material models is being investigated. A flow stress model, for strain rates ranging from 10^{−4} to 10^{12} (quasi-static to highly dynamic), and temperatures ranging from room temperature to over 1000 K, is obtained by beginning directly with experimental stress–strain data for Copper. An incrementally objective and fully implicit time integration scheme is employed to integrate the hypo-elastic constitutive model, which is then implemented into a finite element code for evaluation. Accuracy and performance of the flow stress models derived from symbolic regression are assessed by comparison to Taylor anvil impact data. The results obtained with the free-form constitutive material model are compared to well-established strength models such as the Preston–Tonks–Wallace (PTW) model and the Mechanical Threshold Stress (MTS) model. Preliminary results show candidate free-form models comparing well with data in regions of stress–strain space with sufficient experimental data, pointing to a potential means for both rapid prototyping in future model development, as well as the use of machine learning in capturing more data as a guide for more advanced model development.}
}

@article{barnabe2018multiscale,
  doi = {10.1016/j.ifset.2017.09.015},
  url = {https://doi.org/10.1016/j.ifset.2017.09.015},
  year = {2018},
  month = apr,
  publisher = {Elsevier {BV}},
  volume = {46},
  pages = {41--53},
  author = {M. Barnab{\'{e}} and N. Blanc and T. Chabin and J.-Y. Delenne and A. Duri and X. Frank and V. Hugouvieux and E. Lutton and F. Mabille and S. Nezamabadi and N. Perrot and F. Radjai and T. Ruiz and A. Tonda},
  title = {Multiscale modeling for bioresources and bioproducts},
  journal = {Innovative Food Science {\&} Emerging Technologies},
  abstract = {Designing and processing complex matter and materials are key objectives of bioresource and bioproduct research. Modeling approaches targeting such systems have to account for their two main sources of complexity: their intrinsic multi-scale nature; and the variability and heterogeneity inherent to all living systems. Here we provide insight into methods developed at the Food & Bioproduct Engineering division (CEPIA) of the French National Institute of Agricultural Research (INRA). This brief survey focuses on innovative research lines that tackle complexity by mobilizing different approaches with complementary objectives. On one hand cognitive approaches aim to uncover the basic mechanisms and laws underlying the emerging collective properties and macroscopic behavior of soft-matter and granular systems, using numerical and experimental methods borrowed from physics and mechanics. The corresponding case studies are dedicated to the structuring and phase behavior of biopolymers, powders and granular materials, and to the evolution of these structures caused by external constraints. On the other hand machine learning approaches can deal with process optimizations and outcome predictions by extracting useful information and correlations from huge datasets built from experiments at different length scales and in heterogeneous conditions. These predictive methods are illustrated in the context of cheese ripening, grape maturity prediction and bacterial production.}
}

@article{tonda2017insilico,
  doi = {10.1039/c7fo00830a},
  url = {https://doi.org/10.1039/c7fo00830a},
  year = {2017},
  publisher = {Royal Society of Chemistry ({RSC})},
  volume = {8},
  number = {12},
  pages = {4404--4413},
  author = {Alberto Tonda and Anita Grosvenor and Stefan Clerens and Steven Le Feunteun},
  title = {In silico modeling of protein hydrolysis by endoproteases: a case study on pepsin digestion of bovine lactoferrin},
  journal = {Food {\&} Function},
  abstract = {This paper presents a novel model of protein hydrolysis and release of peptides by endoproteases. It requires the amino-acid sequence of the protein substrate to run, and makes use of simple Monte-Carlo in silico simulations to qualitatively and quantitatively predict the peptides that are likely to be produced during the course of the proteolytic reaction. In the present study, the model is applied to the case of pepsin, the gastric protease. Unlike pancreatic proteases, pepsin has a low substrate specificity and therefore displays a stochastic behavior that is particularly challenging to model and predict. Two versions of the model are studied and compared with peptidomic data obtained during pepsin hydrolysis of bovine lactoferrin. The first version of the model takes into account cleavage probabilities according to the amino acids in position P1–P1′ only, whereas the second version also accounts for the influence of neighbor amino acids (P4, P3, P2, P2′, P3′, P4′) and peptide terminal ends. The second version of the model was able to reproduce many real-world features of the reported behavior of pepsin, such as the peptide size distribution, or the quantity of free amino-acids. More remarkably, 50\% of the experimentally monitored peptides (44/87) lay within the 120 most abundant simulated peptides. The presented methodology has the advantage of being applicable not only to different proteins, but to different enzymes as well, as long as cleavage frequency data are available.}
}

@article{djekic2018review,
  doi = {10.1016/j.jclepro.2017.11.241},
  url = {https://doi.org/10.1016/j.jclepro.2017.11.241},
  year = {2018},
  month = mar,
  publisher = {Elsevier {BV}},
  volume = {176},
  pages = {1012--1025},
  author = {Ilija Djekic and Neus Sanju{\'{a}}n and Gabriela Clemente and Anet Re{\v{z}}ek Jambrak and Aleksandra Djuki{\'{c}}-Vukovi{\'{c}} and Ur{\v{s}}ka Vrabi{\v{c}} Brodnjak and Eugen Pop and Rallou Thomopoulos and Alberto Tonda},
  title = {Review on environmental models in the food chain - Current status and future perspectives},
  journal = {Journal of Cleaner Production},
  abstract = {Diversity of food systems and their interaction with the environment has become a research topic for many years. Scientists use various models to explain environmental issues of food systems. This paper gives an overview of main streams in analyzing this topic. A literature review was performed by analyzing published scientific papers on environmental impacts in the food chain. The selection criteria were focused on different environmental approaches applied in the food chain and on the perspectives of future research. This review shows that on the one side there are generic environmental models developed by environmental scientists and as such applied on food. On the other side, there are models developed by food scientists in order to analyze food-environmental interactions. The environmental research in food industry can be categorized as product, process or system oriented. This study confirmed that the focus of product based approach is mainly performed through life-cycle assessments. The process based approach focuses on food processes such as heat transfer, cleaning and sanitation and various approaches in food waste management. Environmental systems in the food chain were the least investigated stream analyzing levels of environmental practices in place. Future research perspectives are the emerging challenges related to environmental impacts of novel food processing technologies, innovative food packaging and changes in diets and food consumption in connection with climate and environmental changes.}
}

@article{lopezrincon2018evolutionary,
  doi = {10.1016/j.asoc.2017.12.036},
  url = {https://doi.org/10.1016/j.asoc.2017.12.036},
  year = {2018},
  month = apr,
  publisher = {Elsevier {BV}},
  volume = {65},
  pages = {91--100},
  author = {Alejandro Lopez-Rincon and Alberto Tonda and Mohamed Elati and Olivier Schwander and Benjamin Piwowarski and Patrick Gallinari},
  title = {Evolutionary optimization of convolutional neural networks for cancer {miRNA} biomarkers classification},
  journal = {Applied Soft Computing},
  abstract = {Cancer diagnosis is currently undergoing a paradigm shift with the incorporation of molecular biomarkers as part of routine diagnostic panel. This breakthrough discovery directs researches to examine the role of microRNA in cancer, since its deregulation is often associated with almost all human tumors. Such differences frequently recur in tumor-specific microRNA signatures, which are helpful to diagnose tissue of origin and tumor subtypes. Nonetheless, the resulting classification problem is far from trivial, as there are hundreds of microRNA types, and tumors are non-linearly correlated to the presence of several overexpressions. In this paper, we propose to apply an evolutionary optimized convolutional neural network classifier to this complex task. The presented approach is compared against 21 state-of-the-art classifiers, on a real-world dataset featuring 8,129 patients, for 29 different classes of tumors, using 1,046 different biomarkers. As a result of the comparison, we also present a meta-analysis on the dataset, identifying the classes on which the collective performance of the considered classifiers is less effective, and thus possibly singling out types of tumors for which biomarker tests might be less reliable.}
}


@article{garciasanchez2018automated,
  doi = {10.1016/j.knosys.2018.04.030},
  url = {https://doi.org/10.1016/j.knosys.2018.04.030},
  year = {2018},
  month = aug,
  publisher = {Elsevier {BV}},
  volume = {153},
  pages = {133--146},
  author = {Pablo Garc{\'{\i}}a-S{\'{a}}nchez and Alberto Tonda and Antonio M. Mora and Giovanni Squillero and Juan Juli{\'{a}}n Merelo},
  title = {Automated Playtesting in Collectible Card Games Using Evolutionary Algorithms: A Case Study in HearthStone},
  journal = {Knowledge-Based Systems},
  abstract = {Collectible card games have been among the most popular and profitable products of the entertainment industry since the early days of Magic: The GatheringTM in the nineties. Digital versions have also appeared, with HearthStone: Heroes of WarCraftTM being one of the most popular. In Hearthstone, every player can play as a hero, from a set of nine, and build his/her deck before the game from a big pool of available cards, including both neutral and hero-specific cards. This kind of games offers several challenges for researchers in artificial intelligence since they involve hidden information, unpredictable behaviour, and a large and rugged search space. Besides, an important part of player engagement in such games is a periodical input of new cards in the system, which mainly opens the door to new strategies for the players. Playtesting is the method used to check the new card sets for possible design flaws, and it is usually performed manually or via exhaustive search; in the case of Hearthstone, such test plays must take into account the chosen hero, with its specific kind of cards. In this paper, we present a novel idea to improve and accelerate the playtesting process, systematically exploring the space of possible decks using an Evolutionary Algorithm (EA). This EA creates HearthStone decks which are then played by an AI versus established human-designed decks. Since the space of possible combinations that are play-tested is huge, search through the space of possible decks has been shortened via a new heuristic mutation operator, which is based on the behaviour of human players modifying their decks. Results show the viability of our method for exploring the space of possible decks and automating the play-testing phase of game design. The resulting decks, that have been examined for balancedness by an expert player, outperform human-made ones when played by the AI; the introduction of the new heuristic operator helps to improve the obtained solutions, and basing the study on the whole set of heroes shows its validity through the whole range of decks.}
}

@article{karpov2018valis,
  doi = {10.1007/s10710-018-9331-6},
  url = {https://doi.org/10.1007/s10710-018-9331-6},
  year = {2018},
  month = aug,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {19},
  number = {3},
  pages = {453--471},
  author = {Peter Karpov and Giovanni Squillero and Alberto Tonda},
  title = {{VALIS}: an Evolutionary Classification Algorithm},
  journal = {Genetic Programming and Evolvable Machines},
  abstract = {VALIS is an effective and robust classification algorithm with a focus on understandability. Its name stems from Vote-ALlocating Immune System, as it evolves a population of artificial antibodies that can bind to the input data, and performs classification through a voting process. In the beginning of the training, VALIS generates a set of random candidate antibodies; at each iteration, it selects the most useful ones to produce new candidates, while the least, are discarded; the process is iterated until a user-defined stopping condition. The paradigm allows the user to get a visual insight of the learning dynamics, helping to supervise the process, pinpoint problems, and tweak feature engineering. VALIS is tested against nine state-of-the-art classification algorithms on six popular benchmark problems; results demonstrate that it is competitive with well-established black-box techniques, and superior in specific corner cases.}
}

@article{accatino2019tradeoffs,
  doi = {10.1016/j.agsy.2018.08.002},
  url = {https://doi.org/10.1016/j.agsy.2018.08.002},
  year = {2019},
  month = jan,
  publisher = {Elsevier {BV}},
  volume = {168},
  pages = {58--72},
  author = {Francesco Accatino and Alberto Tonda and Camille Dross and Fran{\c{c}}ois L{\'{e}}ger and Muriel Tichit},
  title = {Trade-offs and Synergies Between Livestock Production and Other Ecosystem Services},
  journal = {Agricultural Systems},
  abstract = {One of the biggest challenges today is to satisfy an increasing food demand while preserving ecosystem services. Farming systems have a huge impact on land cover and land use, it is therefore vital to understand how land cover and land use allocation can promote synergies between food production and other ecosystem services. Livestock production has multiple interactions with other ecosystem services and can promote synergies especially in grasslands. We investigated the interactions between livestock production and other ecosystem services and explored strategies to soften trade-offs and enhance synergies. We considered four ecosystem services (livestock production, crop production, carbon sequestration, and timber growth) in France. We considered 709 land units covering a wide range of farming systems where both food production and other ecosystem services are provided. For each land unit, we built ecological production functions that are models measuring the statistical influence of driving variables (i.e. land cover, land use, pesticide expense, and climate) on the provision of ecosystem services. Using an optimization procedure, we studied the extent to which livestock production could be increased without reducing other ecosystem services and without increasing total pesticide expense. We found that a 20\% increase in livestock production could be achieved by all farming systems in France under those general constraints. The 709 land units could be grouped based on similar combinations of increases or decreases in specific ecosystem services during the optimization. 48\% of land units were specialised on food production, 24\% were specialised on other ecosystem services, 16\% were specialised on the mixed provision of food production and other ecosystem services, whereas the remaining 12\% showed decrease or no change in all ecosystem services. Livestock production was either in trade-off or in synergy with the other ecosystem services. The trade-offs could be softened through intensified use of cultivated land and spatial segregation of livestock production. The synergies could be enhanced only through major grassland expansion.}
}

@article{atzeni2018countering,
  doi = {10.1109/access.2018.2874502},
  url = {https://doi.org/10.1109/access.2018.2874502},
  year = {2018},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {6},
  pages = {59540--59556},
  author = {Andrea Atzeni and Fernando Diaz and Andrea Marcelli and Antonio Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {Countering Android Malware: A Scalable Semi-Supervised Approach for Family-Signature Generation},
  journal = {{IEEE} Access},
  abstract = {Reducing the effort required by humans in countering malware is of utmost practical value. We describe a scalable, semi-supervised framework to dig into massive data sets of Android applications and identify new malware families. Until 2010, the industrial standard for the detection of malicious applications has been mainly based on signatures; as each tiny alteration in malware makes them ineffective, new signatures are frequently created –- a task that requires a considerable amount of time and resources from skilled experts. The framework we propose is able to automatically cluster applications in families and suggest formal rules for identifying them with 100\% recall and quite high precision. The families are used either to safely extend experts’ knowledge on new samples or to reduce the number of applications requiring thorough analyses. We demonstrated the effectiveness and the scalability of the approach running experiments on a database of 1.5 million Android applications. In 2018, the framework has been successfully deployed on Koodous, a collaborative anti-malware platform.}
}

@article{thomopoulos2019multicriteria,
  doi = {10.1007/s12393-018-9186-x},
  url = {https://doi.org/10.1007/s12393-018-9186-x},
  year = {2019},
  month = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {11},
  number = {1},
  pages = {44--60},
  author = {R. Thomopoulos and C. Baudrit and N. Boukhelifa and R. Boutrou and P. Buche and E. Guichard and V. Guillard and E. Lutton and P. S. Mirade and A. Ndiaye and N. Perrot and F. Taillandier and T. Thomas-Danguin and A. Tonda},
  title = {Multi-Criteria Reverse Engineering for Food: Genesis and Ongoing Advances},
  journal = {Food Engineering Reviews},
  abstract = {Multi-criteria reverse engineering (MRE) has arisen from the cross-fertilization of advances in mathematics and shifts in social demand. MRE, thus, marks a progressive switch (a) from empirical to formal approaches able to simultaneously factor in diverse parameters, such as environment, economics, and health; (b) from mono-criterion optimization to multi-criteria decision analysis; (c) from forward engineering, observing the results of process conditions, to reverse engineering, selecting the right process conditions for a target output. The food sector has been slow to adopt reverse engineering, but interest is surging now that the industry is looking to shift production towards personalized food. MRE has followed a heterogeneous development trajectory and found applications in different disciplines. The scope of this review spans MRE applications in the food sector covering food packaging and food consumption and focuses on demonstrating potentialities of MRE in a complex field like food. We explain how MRE enables the development of sustainable processes, looking at similar approaches used in sectors other than food. Building on this extensive review, we sketch out some guidelines on approaches to be used in future MRE applications in food, working up from the problem statement.}
}

@article{djekic2019crosseuropean,
  doi = {10.1016/j.jfoodeng.2019.06.007},
  url = {https://doi.org/10.1016/j.jfoodeng.2019.06.007},
  year = {2019},
  month = nov,
  publisher = {Elsevier {BV}},
  volume = {261},
  pages = {109--116},
  author = {Ilija Djekic and Alen Muj{\v{c}}inovi{\'{c}} and Aleksandra Nikoli{\'{c}} and Anet Re{\v{z}}ek Jambrak and Photis Papademas and Aberham Hailu Feyissa and Kamal Kansou and Rallou Thomopoulos and Heiko Briesen and Nickolas G. Kavallieratos and Christos G. Athanassiou and Cristina L.M. Silva and Alexandrina Sirbu and Alexandru Mihnea Moisescu and Igor Tomasevic and Ur{\v{s}}ka Vrabi{\v{c}} Brodnjak and Maria Charalambides and Alberto Tonda},
  title = {Cross-European Initial Survey on the Use of Mathematical Models in Food Industry},
  journal = {Journal of Food Engineering},
  abstract = {Mathematical modelling plays an important role in food engineering having various mathematical models tailored for different food topics. However, mathematical models are followed by limited information on their application in food companies. This paper aims to discuss the extent and the conditions surrounding the usage of mathematical models in the context of European food and drinks industry. It investigates the knowledge, nature and current use of modelling approaches in relation to the industry main characteristics. A total of 203 food companies from 12 European countries were included in this research. Results reveal that the country where the company operates, and size of the company, are more important predictors on the usage of mathematical models followed by the type of food sector. The more developed countries are positioned at the higher level of knowledge and use of available models. Similar pattern was observed at the micro level showing that small or medium sized companies exhibit lack of knowledge, resources and limiting usage of models.}
}

@article{gu2019amathematical,
  doi = {10.1016/j.foodcont.2019.106729},
  url = {https://doi.org/10.1016/j.foodcont.2019.106729},
  year = {2019},
  month = dec,
  publisher = {Elsevier {BV}},
  volume = {106},
  pages = {106729},
  author = {Yingying Gu and Laurent Bouvier and Alberto Tonda and Guillaume Delaplace},
  title = {A Mathematical Model for the Prediction of the Whey Protein Fouling Mass in a Pilot Scale Plate Heat Exchanger},
  journal = {Food Control},
  abstract = {A better understanding of protein fouling during the thermal treatment of whey protein concentrate (WPC) solutions is critical for better fouling control. In order to understand the impact of various parameters on the total whey protein fouling mass, a dimensional analysis was applied to the experimental data obtained from a pilot scale plate heat exchanger, setting total fouling mass as the target variable. A model was developed to predict the total fouling mass, covering a series of variables including whey protein solution concentration (2.5–25 g/L), calcium concentration (70-120 ppm), running time (90-330 min), fouling solution flow rate (200-500 L/h), total fouling surface area, outlet temperature (82-97 °C) and differences in whey protein concentrate powders. In addition to temperature dimensionless parameters, the main parameters involved in the model are the Reynolds number (2000-5000) and the calcium to β-lactoglobulin molar ratio (2.7–34.7). The model developed concerns only pure whey proteins solutions since all the testing solutions were casein free. This model has allowed us to provide guidelines as to how the above parameters influence fouling within the plate heat exchanger, as well as empirical correlations for predicting such fouling development.}
}

@article{gesanguiziou2019annotation,
  doi = {10.1016/j.dib.2019.104204},
  url = {https://doi.org/10.1016/j.dib.2019.104204},
  year = {2019},
  month = aug,
  publisher = {Elsevier {BV}},
  volume = {25},
  pages = {104204},
  author = {Genevi{\`{e}}ve G{\'{e}}san-Guiziou and Aude Alaphilippe and Mathieu Andro and Joël Aubin and Christian Bockstaller and Raphaëlle Botreau and Patrice Buche and Catherine Collet and Nicole Darmon and Monique Delabuis and Agn{\`{e}}s Girard and R{\'{e}}gis Grateau and Kamal Kansou and Vincent Martinet and Jeanne-Marie Membr{\'{e}} and R{\'{e}}gis Sabbadin and Louis-Georges Soler and Marie Thiollet-Scholtus and Alberto Tonda and Hayo Van-Der-Werf},
  title = {Annotation Data About Multi Criteria Assessment Methods Used in the Agri-food Research: The French National Institute for Agricultural Research ({INRA}) Experience},
  journal = {Data in Brief},
  abstract = {This data article contains annotation data characterizing Multi Criteria Assessment (MCA) Methods proposed in the agri-food sector by researchers from INRA, Europe's largest agricultural research institute (INRA, \url{http://institut.inra.fr/en}). MCA can be used to assess and compare agricultural and food systems, and support multi-actor decision making and design of innovative systems for crop production, animal production and processing of agricultural products. These data are stored in a public repository managed by INRA (\url{https://data.inra.fr/; https://doi.org/10.15454/WB51LL}).}
}

@article{djekic2019scientific,
  doi = {10.3390/foods8080301},
  url = {https://doi.org/10.3390/foods8080301},
  year = {2019},
  month = aug,
  publisher = {{MDPI} {AG}},
  volume = {8},
  number = {8},
  pages = {301},
  author = {Ilija Djekic and Milica Poji{\'{c}} and Alberto Tonda and Predrag Putnik and Danijela Bursa{\'{c}} Kova{\v{c}}evi{\'{c}} and Anet Re{\v{z}}ek-Jambrak and Igor Tomasevic},
  title = {Scientific Challenges in Performing Life-Cycle Assessment in the Food Supply Chain},
  journal = {Foods},
  abstract = {This paper gives an overview of scientific challenges that occur when performing life-cycle assessment (LCA) in the food supply chain. In order to evaluate these risks, the Failure Mode and Effect Analysis tool has been used. Challenges related to setting the goal and scope of LCA revealed four hot spots: system boundaries of LCA; used functional units; type and quality of data categories, and main assumptions and limitations of the study. Within the inventory analysis, challenging issues are associated with allocation of material and energy flows and waste streams released to the environment. Impact assessment brings uncertainties in choosing appropriate environmental impacts. Finally, in order to interpret results, a scientifically sound sensitivity analysis should be performed to check how stable calculations and results are. Identified challenges pave the way for improving LCA of food supply chains in order to enable comparison of results.}
}


@article{lopezrincon2019automatic,
  doi = {10.1186/s12859-019-3050-8},
  url = {https://doi.org/10.1186/s12859-019-3050-8},
  year = {2019},
  month = sep,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {20},
  number = {1},
  author = {Alejandro Lopez-Rincon and Marlet Martinez-Archundia and Gustavo U. Martinez-Ruiz and Alexander Schoenhuth and Alberto Tonda},
  title = {Automatic Discovery of 100-{miRNA} Signature for Cancer Classification Using Ensemble Feature Selection},
  journal = {{BMC} Bioinformatics},
  abstract = {Background: MicroRNAs (miRNAs) are noncoding RNA molecules heavily involved in human tumors, in which few of them circulating the human body. Finding a tumor-associated signature of miRNA, that is, the minimum miRNA entities to be measured for discriminating both different types of cancer and normal tissues, is of utmost importance. Feature selection techniques applied in machine learning can help however they often provide naive or biased results. Results: An ensemble feature selection strategy for miRNA signatures is proposed. miRNAs are chosen based on consensus on feature relevance from high-accuracy classifiers of different typologies. This methodology aims to identify signatures that are considerably more robust and reliable when used in clinically relevant prediction tasks. Using the proposed method, a 100-miRNA signature is identified in a dataset of 8023 samples, extracted from TCGA. When running eight-state-of-the-art classifiers along with the 100-miRNA signature against the original 1046 features, it could be detected that global accuracy differs only by 1.4\%. Importantly, this 100-miRNA signature is sufficient to distinguish between tumor and normal tissues. The approach is then compared against other feature selection methods, such as UFS, RFE, EN, LASSO, Genetic Algorithms, and EFS-CLA. The proposed approach provides better accuracy when tested on a 10-fold cross-validation with different classifiers and it is applied to several GEO datasets across different platforms with some classifiers showing more than 90\% classification accuracy, which proves its cross-platform applicability. Conclusions: The 100-miRNA signature is sufficiently stable to provide almost the same classification accuracy as the complete TCGA dataset, and it is further validated on several GEO datasets, across different types of cancer and platforms. Furthermore, a bibliographic analysis confirms that 77 out of the 100 miRNAs in the signature appear in lists of circulating miRNAs used in cancer studies, in stem-loop or mature-sequence form. The remaining 23 miRNAs offer potentially promising avenues for future research.}
}

@article{garciasanchez2020optimizing,
  doi = {10.1016/j.knosys.2019.105032},
  url = {https://doi.org/10.1016/j.knosys.2019.105032},
  year = {2020},
  month = jan,
  publisher = {Elsevier {BV}},
  volume = {188},
  pages = {105032},
  author = {Pablo Garc{\'{\i}}a-S{\'{a}}nchez and Alberto Tonda and Antonio J. Fern{\'{a}}ndez-Leiva and Carlos Cotta},
  title = {Optimizing HearthStone Agents Using an Evolutionary Algorithm},
  journal = {Knowledge-Based Systems},
  abstract = {Digital collectible card games are not only a growing part of the video game industry, but also an interesting research area for the field of computational intelligence. This game genre allows researchers to deal with hidden information, uncertainty and planning, among other aspects. This paper proposes the use of evolutionary algorithms (EAs) to develop agents who play a card game, Hearthstone, by optimizing a data-driven decision-making mechanism that takes into account all the elements currently in play. Agents feature self-learning by means of a competitive coevolutionary training approach, whereby no external sparring element defined by the user is required for the optimization process. One of the agents developed through the proposed approach was runner-up (best 6\%) in an international Hearthstone Artificial Intelligence (AI) competition. Our proposal performed remarkably well, even when it faced state-of-the-art techniques that attempted to take into account future game states, such as Monte-Carlo Tree search. This outcome shows how evolutionary computation could represent a considerable advantage in developing AIs for collectible card games such as Hearthstone.}
}

@article{tonda2019inspyred,
  doi = {10.1007/s10710-019-09367-z},
  url = {https://doi.org/10.1007/s10710-019-09367-z},
  year = {2019},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {21},
  number = {1-2},
  pages = {269--272},
  author = {Alberto Tonda},
  title = {Inspyred: Bio-inspired algorithms in Python},
  journal = {Genetic Programming and Evolvable Machines},
  abstract = {Review of the Python package ``inspyred'', for the Springer journal Genetic Programming and Evolvable Machines.}
}

@article{deponte2020two,
  doi = {10.1016/j.compchemeng.2020.106733},
  url = {https://doi.org/10.1016/j.compchemeng.2020.106733},
  year = {2020},
  month = apr,
  publisher = {Elsevier {BV}},
  volume = {135},
  pages = {106733},
  author = {Hannes Deponte and Alberto Tonda and Nathalie Gottschalk and Laurent Bouvier and Guillaume Delaplace and Wolfgang Augustin and Stephan Scholl},
  title = {Two Complementary Methods for the Computational Modeling of Cleaning Processes in Food Industry},
  journal = {Computers {\&} Chemical Engineering},
  abstract = {Insufficient cleaning in the food industry can create serious hygienic risks. However, when attempting to avoid these risks, food-processing plants frequently tend to clean for too long, at extremely high temperatures, or with too many chemicals, resulting in high cleaning costs and severe environmental impacts. Therefore, the optimization of cleaning processes in the food industry has significant economic and ecological potential. Unfortunately, in-situ assessments of cleaning processes are difficult, and the multitude of different cleaning situations complicates the definition of a comprehensive approach. In this study, two methodological approaches for the comprehensive modeling of cleaning processes are introduced. The resulting models facilitate comparisons of different cleaning processes and they can be scaled up for processes with similar conditions, using cleaning time as a response. A dimensional analysis is performed to obtain general results and to allow transfer of the approaches to other cleaning situations. The models are established according to the statistical rules for the deduction of multiple regression equations for the prediction of the response based on the input parameters. The terms of the model equation are confirmed with a significance analysis. A machine learning approach is also used to create model equations with symbolic regression. Both methods and the obtained model equations are validated. The two applied approaches reveal similar significant terms and models. Significant dimensionless numbers are the Reynolds number, the density number that describes the ratio of the density of the soil to the density of the cleaning agent, and the soil number, which is a new dimensionless number that characterizes the properties of food soils. The methodology of both approaches is transparent; therefore, the resulting equations can be compared and similarities are found. Both methods are deemed applicable for the computational modeling of cleaning processes in food industry.}
}

@article{lopezrincon2020machine,
  doi = {10.3390/cancers12071785},
  url = {https://doi.org/10.3390/cancers12071785},
  year = {2020},
  month = jul,
  publisher = {{MDPI} {AG}},
  volume = {12},
  number = {7},
  pages = {1785},
  author = {Alejandro Lopez-Rincon and Lucero Mendoza-Maldonado and Marlet Martinez-Archundia and Alexander Sch\"{o}nhuth and Aletta D. Kraneveld and Johan Garssen and Alberto Tonda},
  title = {Machine Learning-Based Ensemble Recursive Feature Selection of Circulating {miRNAs} for Cancer Tumor Classification},
  journal = {Cancers},
  abstract = {Circulating microRNAs (miRNA) are small noncoding RNA molecules that can be detected in bodily fluids without the need for major invasive procedures on patients. miRNAs have shown great promise as biomarkers for tumors to both assess their presence and to predict their type and subtype. Recently, thanks to the availability of miRNAs datasets, machine learning techniques have been successfully applied to tumor classification. The results, however, are difficult to assess and interpret by medical experts because the algorithms exploit information from thousands of miRNAs. In this work, we propose a novel technique that aims at reducing the necessary information to the smallest possible set of circulating miRNAs. The dimensionality reduction achieved reflects a very important first step in a potential, clinically actionable, circulating miRNA-based precision medicine pipeline. While it is currently under discussion whether this first step can be taken, we demonstrate here that it is possible to perform classification tasks by exploiting a recursive feature elimination procedure that integrates a heterogeneous ensemble of high-quality, state-of-the-art classifiers on circulating miRNAs. Heterogeneous ensembles can compensate inherent biases of classifiers by using different classification algorithms. Selecting features then further eliminates biases emerging from using data from different studies or batches, yielding more robust and reliable outcomes. The proposed approach is first tested on a tumor classification problem in order to separate 10 different types of cancer, with samples collected over 10 different clinical trials, and later is assessed on a cancer subtype classification task, with the aim to distinguish triple negative breast cancer from other subtypes of breast cancer. Overall, the presented methodology proves to be effective and compares favorably to other state-of-the-art feature selection methods.}
}

@article{carvalho2021modelling,
  doi = {10.3390/foods10010082},
  url = {https://doi.org/10.3390/foods10010082},
  year = {2021},
  month = jan,
  publisher = {{MDPI} {AG}},
  volume = {10},
  number = {1},
  pages = {82},
  author = {Otilia Carvalho and Maria N. Charalambides and Ilija Djeki{\'{c}} and Christos Athanassiou and Serafim Bakalis and Jose Benedito and Aurelien Briffaz and Cristina Casta{\~{n}}{\'{e}} and Guy Della Valle and Isabel Maria Nunes de Sousa and Ferruh Erdogdu and Aberham Hailu Feyissa and Nickolas G. Kavallieratos and Alexandros Koulouris and Milica Poji{\'{c}} and Anabela Raymundo and Jordi Riudavets and Fabrizio Sarghini and Pasquale Trematerra and Alberto Tonda},
  title = {Modelling Processes and Products in the Cereal Chain},
  journal = {Foods},
  abstract = {In recent years, modelling techniques have become more frequently adopted in the field of food processing, especially for cereal-based products, which are among the most consumed foods in the world. Predictive models and simulations make it possible to explore new approaches and optimize proceedings, potentially helping companies reduce costs and limit carbon emissions. Nevertheless, as the different phases of the food processing chain are highly specialized, advances in modelling are often unknown outside of a single domain, and models rarely take into account more than one step. This paper introduces the first high-level overview of modelling techniques employed in different parts of the cereal supply chain, from farming to storage, from drying to milling, from processing to consumption. This review, issued from a networking project including researchers from over 30 different countries, aims at presenting the current state of the art in each domain, showing common trends and synergies, to finally suggest promising future venues for research.}
}

@article{lopezrincon2021classification,
  doi = {10.1038/s41598-020-80363-5},
  url = {https://doi.org/10.1038/s41598-020-80363-5},
  year = {2021},
  month = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {11},
  number = {1},
  author = {Alejandro Lopez-Rincon and Alberto Tonda and Lucero Mendoza-Maldonado and Daphne G. J. C. Mulders and Richard Molenkamp and Carmina A. Perez-Romero and Eric Claassen and Johan Garssen and Aletta D. Kraneveld},
  title = {Classification and Specific Primer Design for Accurate Detection of {SARS}-{CoV}-2 Using Deep Learning},
  journal = {Scientific Reports},
  abstract = {In this paper, deep learning is coupled with explainable artificial intelligence techniques for the discovery of representative genomic sequences in SARS-CoV-2. A convolutional neural network classifier is first trained on 553 sequences from the National Genomics Data Center repository, separating the genome of different virus strains from the Coronavirus family with 98.73\% accuracy. The network’s behavior is then analyzed, to discover sequences used by the model to identify SARS-CoV-2, ultimately uncovering sequences exclusive to it. The discovered sequences are validated on samples from the National Center for Biotechnology Information and Global Initiative on Sharing All Influenza Data repositories, and are proven to be able to separate SARS-CoV-2 from different virus strains with near-perfect accuracy. Next, one of the sequences is selected to generate a primer set, and tested against other state-of-the-art primer sets, obtaining competitive results. Finally, the primer is synthesized and tested on patient samples (n = 6 previously tested positive), delivering a sensitivity similar to routine diagnostic methods, and 100\% specificity. The proposed methodology has a substantial added value over existing methods, as it is able to both automatically identify promising primer sets for a virus from a limited amount of data, and deliver effective results in a minimal amount of time. Considering the possibility of future pandemics, these characteristics are invaluable to promptly create specific detection methods for diagnostics.}
}

@article{iacca2021evolutionary,
  doi = {10.1016/j.simpa.2021.100107},
  url = {https://doi.org/10.1016/j.simpa.2021.100107},
  year = {2021},
  month = jul,
  publisher = {Elsevier {BV}},
  pages = {100107},
  author = {Giovanni Iacca and Kateryna Konotopska and Doina Bucur and Alberto Tonda},
  title = {An Evolutionary Framework for Maximizing Influence Propagation in Social Networks},
  journal = {Software Impacts},
  abstract = {Social networks are one the main sources of information transmission nowadays. However, not all nodes in social networks are equal: in fact, some nodes are more influential than others, i.e., their information tends to spread more. Finding the most influential nodes in a network – the so-called Influence Maximization problem – is an NP-hard problem with great social and economical implications. Here, we introduce a framework based on Evolutionary Algorithms that includes various graph-aware techniques (spread approximations, domain-specific operators, and node filtering) that facilitate the optimization process. The framework can be applied straightforwardly to various social network datasets, e.g., those in the SNAP repository.}
}

@article{hannachi2020vers,
  TITLE = {{Vers une Action Collective {\`a} l'{\'E}chelle des Paysages}},
  AUTHOR = {Hannachi, Mourad and Souch{\`e}re, V{\'e}ronique and Bu{\`e}che, Samuel and Dupayage, Marc and Boquet, Bastien and Pardoux, J.-P. and Berthet, Elsa and Deredec, Anne and Tonda, Alberto and Pluquet, P. and Leroy, J.P. and Albaut, Aur{\'e}lie and Blarel, Jacques and Lecuyer, J{\'e}r{\^o}me and Gazet, Claude and Leuba, Muriel and Gagliardi, {\'E}lodie and Leleu, Karine and Leclercq, Philippe and Quilliot, {\'E}milien and Pernel, J{\'e}r{\^o}me and Declemy, Marc and Chauvel, Bruno and Walker, Anne-Sophie},
  JOURNAL = {{Phytoma. La D{\'e}fense des V{\'e}g{\'e}taux}},
  VOLUME = {733},
  YEAR = {2020},
  MONTH = Apr,
}

@article{mora2022looking,
title = {Looking for Archetypes: Applying Game Data Mining to Hearthstone Decks},
journal = {Entertainment Computing},
pages = {100498},
year = {2022},
month = may,
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2022.100498},
url = {https://www.sciencedirect.com/science/article/pii/S1875952122000222},
author = {Antonio M. Mora and Alberto Tonda and Antonio J. Fernández-Ares and Pablo García-Sánchez},
abstract = {Digital Collectible Cards Games such as Hearthstone have become a very prolific test-bed for Artificial Intelligence algorithms. The main researches have focused on the implementation of autonomous agents (bots) able to effectively play the game. However, this environment is also very attractive for the use of Data Mining (DM) and Machine Learning (ML) techniques, for analysing and extracting useful knowledge from game data. The objective of this work is to apply existing Game Mining techniques in order to study more than 600,000 real decks (groups of cards) created by players with many different skill levels. Data visualisation and analysis tools have been applied, namely, Graph representations and Clustering techniques. Then, an expert player has conducted a deep analysis of the results yielded by these methods, aiming to identify the use of standard - and well-known - archetypes defined by the play methods will also make it possible for the expert to discover hidden relationships between cards that could lead to finding better combinations of them, enhancing players’ decks or, otherwise, identify unbalanced cards that could lead to a disappointing game experience. Moreover, although this work is mostly focused on data analysis and visualization, the obtained results can be applied to improve Hearthstone Bots’ behaviour, e.g. predicting opponent’s actions after identifying a specific archetype in his/her deck.}
}

@article{giovannitti2022virtual,
  doi = {10.1007/s10845-022-01934-z},
  url = {https://doi.org/10.1007/s10845-022-01934-z},
  year = {2022},
  month = apr,
  publisher = {Springer Science and Business Media {LLC}},
  author = {Eliana Giovannitti and Sayyidshahab Nabavi and Giovanni Squillero and Alberto Tonda},
  title = {A Virtual Sensor for Backlash in Robotic Manipulators},
  journal = {Journal of Intelligent Manufacturing},
  abstract = {Gear backlash is a quite serious problem in industrial robots, it causes vibrations and impairs the robot positioning accuracy. Backlash estimation allows targeted maintenance interventions, preserving robot performances and avoiding unforeseen equipment breakdowns. However, a direct measure of the backlash is hard to obtain, and dedicated auxiliary sensors are required for the measurement. This paper presents a method for estimating backlash in robotic joints that does not require the installation of extra devices. It only relies on data gathered from the motor encoder, which is always present in a robotic joint. The approach is based on the observation of a characteristic vibration pattern arising on the motor speed signal when backlash affects the joint transmission. By looking at the amplitude of this vibration some information about the entity of the backlash in the joint is gathered. Experimental results on simulated data are reported in the study to show the robustness of the method, also with respect of noise. Furthermore, tests on real-world data, gathered from robots installed in a production plant, demonstrate the efficacy of the technique. The approach is cost-effective, fast, and easily automatable, therefore convenient for the industrial world.}
}

@article{mejeanperrot2022decision,
title = {A Decision-Support System to Predict Grape Berry Quality and Wine Potential for a Chenin Vineyard},
journal = {Computers and Electronics in Agriculture},
volume = {200},
pages = {107167},
year = {2022},
month = sep,
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107167},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922004847},
author = {Nathalie {Mejean Perrot} and Alberto Tonda and Ilaria Brunetti and Hervé Guillemin and Bruno Perret and Etienne Goulet and Laurence Guerin and Daniel Picque},
abstract = {Grape berry ripening is a complex process, and predicting the quality of wine starting from the ripening kinetics of grape berries is a challenging task. To tackle this problem, we present a decision-support system based on coupling expert know-how with probability laws encapsulated in a probabilistic model, a dynamic Bayesian network. The proposed approach predicts the ripening kinetics of grape berries starting from initial measurements and weather conditions, and then exploits the information to evaluate the potential of the wine that will produced from them. The results show that the dynamic Bayesian network predicts the total acidity concentration and the sugar content of the grape berries with a small amount of error (mean of 6\% for total acidity concentration, 10\% for sugar content) that is considered satisfying by the experts, making it possible to predict the ideal moment for harvesting the grapes up to two weeks in advance. Moreover, feeding the results from the probabilistic model to a fuzzy expert model, the predicted trajectories are compared to an ideal trajectory described by wine experts and formalized mathematically. From this comparison, it is possible to anticipate drifts in wine sensory quality right from the step of grape ripening.}
}

%% 2023
@article{sicard2023aprimer,
  doi = {10.1111/jfpe.14325},
  url = {https://doi.org/10.1111/jfpe.14325},
  year = {2023},
  month = mar,
  publisher = {Wiley},
  author = {Jason Sicard and Sophie Barbe and Rachel Boutrou and Laurent Bouvier and Guillaume Delaplace and Gwenaëlle Lashermes and Laëtitia Th{\'{e}}ron and Olivier Vitrac and Alberto Tonda},
  title = {A primer on predictive techniques for food and bioresources transformation processes},
  journal = {Journal of Food Process Engineering},
  abstract = {To meet current societal demand for more sustainable transformation processes and bioresources, these processes must be optimized and new ones developed. The evolution of various systems (raw material, food, or process attributes) can be predicted to optimize the uses of biomass for better quality, safety, economic benefit, and sustainability. Predictive modeling can guide the necessary changes and influence industrials, governmental policies and consumers decision-making. However, achieving good predictive capability requires reflection on the models and model validation, which can be difficult. This review aims to help scientists begin to predict by presenting the techniques currently used in predictive science for food and related bioproducts. First, a guideline helps readers initiate a prediction process along with final tips and a warning about the risks involved, with a particular focus on the crucial validation step. Three broad categories of techniques are then presented: empirical, mechanistic, and artificial intelligence (or ``data-driven''). For each category, the advantages and limitations of current techniques for prediction are explained in light of their current domains of applications, illustrated with literature studies and a detailed example. Thus this article provides engineering researchers information about predictive modeling which is a recent relevant development in optimization of both food and nonfood bioresources processes.}
}

@article{shi2023handling,
  doi = {10.1007/s10980-023-01635-9},
  url = {https://doi.org/10.1007/s10980-023-01635-9},
  year = {2023},
  month = mar,
  publisher = {Springer Science and Business Media {LLC}},
  author = {Yong Shi and Alberto Tonda and Francesco Accatino},
  title = {Handling ecosystem service trade-offs: the importance of the spatial scale at which no-loss constraints are posed},
  journal = {Landscape Ecology},
  abstract = {Context: Managing land use to promote an ecosystem service (ES) without reducing others is challenging. The spatial scale at which no-loss constraints are imposed is relevant. Objectives: We examined the influence of the spatial scale of no-loss constraints on ESs when one ES was optimised. Specifically, we investigated how carbon sequestration could be maximized at different spatial scales in France with constraints of no-loss on other ESs. Methods: We used a statistical model linking land use and land cover variables to ESs [carbon sequestration (CS), crop production (CP), livestock production, timber growth] in French small agricultural regions (SARs). We optimised CS at the country scale posing no-loss constraints on other ESs at increasing spatial scales, i.e., SARs (scenario ``SARs''), department (``DEP''), administrative region (``REG''), and France (``FRANCE''). We analysed differences between optimized and initial configurations. Results: Optimized CS at the country scale increased with the spatial scale at which no-loss constraints were posed (+0.51\% for DEP and +2.05\% for FRANCE). The variability of ES variation among the SARs similarly increased. This suggested that constraints at larger scales lead to ES segregation. Correlations among ES variations changed with the scenarios (Spearman’s $\rho$ between CS and CP was -0.43 for DEP and -0.70 for FRANCE). This indicated that different land use strategies produce different degrees of enhancement/softening of ES trade-offs/synergies. Conclusions: A trade-off was highlighted: larger spatial scales promoted better performance of the target ES but also spatial inequality. We argue that addressing smaller scales will lead to land-sharing solutions that avoid the local environmental impacts of land-sparing strategies.}
}

@article{smetana2023environmental,
    author = {Smetana, Sergiy and Bhatia, Anita and Batta, Uday and Mouhrim, Nisrine and Tonda, Alberto},
    title = "{Environmental impact potential of insect production chains for food and feed in Europe}",
    journal = {Animal Frontiers},
    volume = {13},
    number = {4},
    pages = {112-120},
    year = {2023},
    month = {08},
    issn = {2160-6056},
    doi = {10.1093/af/vfad033},
    url = {https://doi.org/10.1093/af/vfad033},
    abstract = {Insects can address sustainability issues associated with current food systems by providing an alternative protein source to address hunger and disease. Only the production systems that rely on side-stream heat and alternate energy sources may benefit from replacing compound feed production with insect value chains. 75\% percent to 93\% of the effects of compound feed production on global warming potential, land use, and fossil resource shortages can be avoided. To fully assess the potential of insect production, it is critical to consider a wide range of sustainability indicators, including social, economic, and environmental aspects.}
}

@article{tonda2023anintercontinental,
    title = {An intercontinental machine learning analysis of factors explaining consumer awareness of food risk},
    journal = {Future Foods},
    volume = {7},
    pages = {100233},
    year = {2023},
    issn = {2666-8335},
    doi = {https://doi.org/10.1016/j.fufo.2023.100233},
    url = {https://www.sciencedirect.com/science/article/pii/S2666833523000199},
    author = {Alberto Tonda and Christian Reynolds and Rallou Thomopoulos},
    abstract = {Food safety is a common concern at the household level, with important variations across different countries and cultures. Nevertheless, identifying the factors that best explain similarities and differences in consumer awareness pertaining to this topic is not straightforward. Starting from a questionnaire administered in seven countries from four continents (Argentina, Brazil, Colombia, Ghana, India, Peru, and the United Kingdom), we present an analysis of the answers related to food safety concerns, aimed at identifying possible explanatory factors. As classical statistical approaches can be limited when dealing with complex datasets, we propose an analysis with machine learning techniques, that can take into account both categorical and numerical values. With the questionnaire as a base, we task a machine learning algorithm, Random Forest, with predicting consumers’ answers to the target questions using information from all other answers. Once the algorithm is trained, it becomes possible to obtain a ranking of the questions considered the most important for the prediction, with the top-ranked questions likely representing explanatory factors. Top-ranked questions are then analyzed using a Random Forest regression algorithm, to test possible correlations. The results show that the most significant explanatory variables of safety concerns seem to be estimates of carbon footprints and calories associated with food products, and primarily with beef and chicken meat. These results tend to indicate that people who are most concerned about food safety are also those who are highly aware of environmental and nutritional impacts of food, hinting at differences in food education as a possible underlying explanation for the data.}
}

@article{papoutsoglou2023machine,
    doi = {10.3389/fmicb.2023.1261889},
    url = {https://doi.org/10.3389/fmicb.2023.1261889},
    year = {2023},
    month = sep,
    publisher = {Frontiers Media {SA}},
    volume = {14},
    author = {Georgios Papoutsoglou and Sonia Tarazona and Marta B. Lopes and Thomas Klammsteiner and Eliana Ibrahimi and Julia Eckenberger and Pierfrancesco Novielli and Alberto Tonda and Andrea Simeon and Rajesh Shigdel and St{\'{e}}phane B{\'{e}}reux and Giacomo Vitali and Sabina Tangaro and Leo Lahti and Andriy Temko and Marcus J. Claesson and Magali Berland},
    title = {Machine learning approaches in microbiome research: challenges and best practices},
    journal = {Frontiers in Microbiology},
    abstract = {Microbiome data predictive analysis within a machine learning (ML) workflow presents numerous domain-specific challenges involving preprocessing, feature selection, predictive modeling, performance estimation, model interpretation, and the extraction of biological information from the results. To assist decision-making, we offer a set of recommendations on algorithm selection, pipeline creation and evaluation, stemming from the COST Action ML4Microbiome. We compared the suggested approaches on a multi-cohort shotgun metagenomics dataset of colorectal cancer patients, focusing on their performance in disease diagnosis and biomarker discovery. It is demonstrated that the use of compositional transformations and filtering methods as part of data preprocessing does not always improve the predictive performance of a model. In contrast, the multivariate feature selection, such as the Statistically Equivalent Signatures algorithm, was effective in reducing the classification error. When validated on a separate test dataset, this algorithm in combination with random forest modeling, provided the most accurate performance estimates. Lastly, we showed how linear modeling by logistic regression coupled with visualization techniques such as Individual Conditional Expectation (ICE) plots can yield interpretable results and offer biological insights. These findings are significant for clinicians and non-experts alike in translational applications.}
}

@article{perezromero2023aninnovative,
    doi = {10.1038/s41598-023-42348-y},
    url = {https://doi.org/10.1038/s41598-023-42348-y},
    year = {2023},
    month = sep,
    publisher = {Springer Science and Business Media {LLC}},
    volume = {13},
    number = {1},
    author = {Carmina Angelica Perez-Romero and Lucero Mendoza-Maldonado and Alberto Tonda and Etienne Coz and Patrick Tabeling and Jessica Vanhomwegen and John MacSharry and Joanna Szafran and Lucina Bobadilla-Morales and Alfredo Corona-Rivera and Eric Claassen and Johan Garssen and Aletta D. Kraneveld and Alejandro Lopez-Rincon},
    title = {An Innovative {AI}-based primer design tool for precise and accurate detection of {SARS}-{CoV}-2 variants of concern},
    journal = {Scientific Reports},
    abstract = {As the COVID-19 pandemic winds down, it leaves behind the serious concern that future, even more disruptive pandemics may eventually surface. One of the crucial steps in handling the SARS-CoV-2 pandemic was being able to detect the presence of the virus in an accurate and timely manner, to then develop policies counteracting the spread. Nevertheless, as the pandemic evolved, new variants with potentially dangerous mutations appeared. Faced by these developments, it becomes clear that there is a need for fast and reliable techniques to create highly specific molecular tests, able to uniquely identify VOCs. Using an automated pipeline built around evolutionary algorithms, we designed primer sets for SARS-CoV-2 (main lineage) and for VOC, B.1.1.7 (Alpha) and B.1.1.529 (Omicron). Starting from sequences openly available in the GISAID repository, our pipeline was able to deliver the primer sets for the main lineage and each variant in a matter of hours. Preliminary in-silico validation showed that the sequences in the primer sets featured high accuracy. A pilot test in a laboratory setting confirmed the results: the developed primers were favorably compared against existing commercial versions for the main lineage, and the specific versions for the VOCs B.1.1.7 and B.1.1.529 were clinically tested successfully.}
}

@article{kidwai2023arobust,
  title = {A robust mRNA signature obtained via recursive ensemble feature selection predicts the responsiveness of omalizumab in moderate‐to‐severe asthma},
  volume = {13},
  ISSN = {2045-7022},
  url = {http://dx.doi.org/10.1002/clt2.12306},
  DOI = {10.1002/clt2.12306},
  number = {11},
  journal = {Clinical and Translational Allergy},
  publisher = {Wiley},
  author = {Kidwai,  Sarah and Barbiero,  Pietro and Meijerman,  Irma and Tonda,  Alberto and Perez‐Pardo,  Paula and Lio ́,  Pietro and van der Maitland‐Zee,  Anke H. and Oberski,  Daniel L. and Kraneveld,  Aletta D. and Lopez‐Rincon,  Alejandro},
  year = {2023},
  month = nov,
  abstract = {Background: Not being well controlled by therapy with inhaled corticosteroids and long-acting \beta2 agonist bronchodilators is a major concern for severe-asthma patients. The current treatment option for these patients is the use of biologicals such as anti-IgE treatment, omalizumab, as an add-on therapy. Despite the accepted use of omalizumab, patients do not always benefit from it. Therefore, there is a need to identify reliable biomarkers as predictors of omalizumab response. Methods: Two novel computational algorithms, machine-learning based Recursive Ensemble Feature Selection (REFS) and rule-based algorithm Logic Explainable Networks (LEN), were used on open accessible mRNA expression data from moderate-to-severe asthma patients to identify genes as predictors of omalizumab response. Results: With REFS, the number of features was reduced from 28,402 genes to 5 genes while obtaining a cross-validated accuracy of 0.975. The 5 responsiveness predictive genes encode the following proteins: Coiled-coil domain- containing protein 113 (CCDC113), Solute Carrier Family 26 Member 8 (SLC26A), Protein Phosphatase 1 Regulatory Subunit 3D (PPP1R3D), C-Type lectin Domain Family 4 member C (CLEC4C) and LOC100131780 (not annotated). The LEN algorithm found 4 identical genes with REFS: CCDC113, SLC26A8 PPP1R3D and LOC100131780. Literature research showed that the 4 identified responsiveness predicting genes are associated with mucosal immunity, cell metabolism, and airway remodeling.}
}

@article{perrot2023predicting,
  title = {Predicting odor profile of food from its chemical composition: Towards an approach based on artificial intelligence and flavorists expertise},
  volume = {20},
  ISSN = {1551-0018},
  url = {http://dx.doi.org/10.3934/mbe.2023908},
  DOI = {10.3934/mbe.2023908},
  number = {12},
  journal = {Mathematical Biosciences and Engineering},
  publisher = {American Institute of Mathematical Sciences (AIMS)},
  author = {Perrot,  N. Mejean and Roche,  Alice and Tonda,  Alberto and Lutton,  Evelyne and Thomas-Danguin,  Thierry},
  year = {2023},
  pages = {20528–20552},
  abstract = {Odor is central to food quality. Still, a major challenge is to understand how the odorants present in a given food contribute to its specific odor profile, and how to predict this olfactory outcome from the chemical composition. In this proof-of-concept study, we seek to develop an integrative model that combines expert knowledge, fuzzy logic, and machine learning to predict the quantitative odor description of complex mixtures of odorants. The model output is the intensity of relevant odor sensory attributes calculated on the basis of the content in odor-active comounds. The core of the model is the mathematically formalized knowledge of four senior flavorists, which provided a set of optimized rules describing the sensory-relevant combinations of odor qualities the experts have in mind to elaborate the target odor sensory attributes. The model first queries analytical and sensory databases in order to standardize, homogenize, and quantitatively code the odor descriptors of the odorants. Then the standardized odor descriptors are translated into a limited number of odor qualities used by the experts thanks to an ontology. A third step consists of aggregating all the information in terms of odor qualities across all the odorants found in a given product. The final step is a set of knowledge-based fuzzy membership functions representing the flavorist expertise and ensuring the prediction of the intensity of the target odor sensory descriptors on the basis of the products' aggregated odor qualities; several methods of optimization of the fuzzy membership functions have been tested. Finally, the model was applied to predict the odor profile of 16 red wines from two grape varieties for which the content in odorants was available. The results showed that the model can predict the perceptual outcome of food odor with a certain level of accuracy, and may also provide insights into combinations of odorants not mentioned by the experts.}
}

@article{mouhrim2023optimization,
  title = {Optimization models for sustainable insect production chains},
  ISSN = {2352-4588},
  url = {http://dx.doi.org/10.1163/23524588-20230148},
  DOI = {10.1163/23524588-20230148},
  journal = {Journal of Insects as Food and Feed},
  publisher = {Brill},
  author = {Mouhrim,  N. and Peguero,  D.A. and Green,  A. and Silva,  B. and Bhatia,  A. and Ristic,  D. and Tonda,  A. and Mathys,  A. and Smetana,  S.},
  year = {2023},
  month = nov,
  pages = {1–19},
  abstract = {Insect value chains are a complex system with non-linear links between many economic, environmental, and social variables. Multi-objective optimization (MOO) algorithms for finding optimal options for complex system functioning can provide a valuable insight in the development of sustainable insect chains. This review proposes a framework for MOO application that is based on gradual implementation, beginning with factors that have an immediate impact on insect production (feed qualities, resource utilization, yield), and progressing to integrated units (environmental, social, and economic impacts). The review introduces the key hotspots of insect production chains, which have been developed in suitable MOO objectives. They represent aspects of resource use, feed quality and its conversion by insects, labor safety and wage fairness, as well as environmental impacts. The capacity of the suggested MOO framework to describe all facets of sustainability may have certain limits. To determine the framework’s applicability and the specific MOO algorithms that can perform the function, modeling and further testing on real insect production chains would be necessary for the intended objectives.}
}

@article{rojasvelazquez2024methodology,
  title = {Methodology for biomarker discovery with reproducibility in microbiome data using machine learning},
  volume = {25},
  ISSN = {1471-2105},
  url = {http://dx.doi.org/10.1186/s12859-024-05639-3},
  DOI = {10.1186/s12859-024-05639-3},
  number = {1},
  journal = {BMC Bioinformatics},
  publisher = {Springer Science and Business Media LLC},
  author = {Rojas-Velazquez,  David and Kidwai,  Sarah and Kraneveld,  Aletta D. and Tonda,  Alberto and Oberski,  Daniel and Garssen,  Johan and Lopez-Rincon,  Alejandro},
  year = {2024},
  month = jan,
  abstract = {Background: In recent years, human microbiome studies have received increasing attention as this field is considered a potential source for clinical applications. With the advancements in omics technologies and AI, research focused on the discovery for potential biomarkers in the human microbiome using machine learning tools has produced positive outcomes. Despite the promising results, several issues can still be found in these studies such as datasets with small number of samples, inconsistent results, lack of uniform processing and methodologies, and other additional factors lead to lack of reproducibility in biomedical research. In this work, we propose a methodology that combines the DADA2 pipeline for 16s rRNA sequences processing and the Recursive Ensemble Feature Selection (REFS) in multiple datasets to increase reproducibility and obtain robust and reliable results in biomedical research.
  Results: Three experiments were performed analyzing microbiome data from patients/cases in Inflammatory Bowel Disease (IBD), Autism Spectrum Disorder (ASD), and Type 2 Diabetes (T2D). In each experiment, we found a biomarker signature in one dataset and applied to 2 other as further validation. The effectiveness of the proposed methodology was compared with other feature selection methods such as K-Best with F-score and random selection as a base line. The Area Under the Curve (AUC) was employed as a measure of diagnostic accuracy and used as a metric for comparing the results of the proposed methodology with other feature selection methods. Additionally, we use the Matthews Correlation Coefficient (MCC) as a metric to evaluate the performance of the methodology as well as for comparison with other feature selection methods.
  Conclusions: We developed a methodology for reproducible biomarker discovery for 16s rRNA microbiome sequence analysis, addressing the issues related with data dimensionality, inconsistent results and validation across independent datasets. The findings from the three experiments, across 9 different datasets, show that the proposed methodology achieved higher accuracy compared to other feature selection methods. This methodology is a first approach to increase reproducibility, to provide robust and reliable results.}
}

@article{jarmatz2024development,
  title = {Development of a soft sensor for fouling prediction in pipe fittings using the example of particulate deposition from suspension flow},
  volume = {145},
  ISSN = {0960-3085},
  url = {http://dx.doi.org/10.1016/j.fbp.2024.02.009},
  DOI = {10.1016/j.fbp.2024.02.009},
  journal = {Food and Bioproducts Processing},
  publisher = {Elsevier BV},
  author = {Jarmatz,  Niklas and Augustin,  Wolfgang and Scholl,  Stephan and Tonda,  Alberto and Delaplace,  Guillaume},
  year = {2024},
  month = may,
  pages = {116–127},
  abstract = {Fouling is the unwanted accumulation of material on a processing surface which is an especially problematic issue in the food industry. Characterizing or predicting fouling through traditional methods or models is a challenge due to the complexity of fouling mechanisms. Machine learning (ML) techniques can overcome this challenge by creating models for prediction directly from experimental data. Unfortunately, the results can be hard to interpret depending on the algorithm. Here, a soft sensor is generated from an extensive data set to predict the fouling of a model particle material system. This is performed inside two different pipe fittings, an inaccessible and accessible fitting (e.g., for sensor measurements). Additionally, dimensional analysis (DA) is conducted to identify the correlations responsible for fouling while keeping descriptors with physical meaning. The resulting dimensionless numbers (DNs) are further processed by three ML algorithms: linear regression (LR), symbolic regression (SR), and random forest (RF). The soft sensor generated using a RF outperformed the other two regressors for the dimensional (Q2=0.90±0.08) and for the dimensionless data (Q2=0.88±0.09). The parameter time and particle mass fraction were determined to be most influential. Furthermore, seven DNs were obtained allowing a reduced experimental design.}
}
@inproceedings{squillero2008anovel,
  doi = {10.1145/1388969.1389049},
  url = {https://doi.org/10.1145/1388969.1389049},
  year = {2008},
  publisher = {{ACM} Press},
  author = {Giovanni Squillero and Alberto Tonda},
  title = {A novel methodology for diversity preservation in evolutionary algorithms},
  booktitle = {Proceedings of the 2008 {GECCO} conference companion on Genetic and evolutionary computation - {GECCO} {\textquotesingle}08},
  abstract = {In this paper we describe an improvement of an entropy-based diversity preservation approach for evolutionary algorithms. This approach exploits the information contained not only in the parts that compose an individual, but also in their position and relative order. We executed a set of preliminary experiments in order to test the new approach, using two different problems in which diversity preservation plays a major role in obtaining good solutions.}
}

@inproceedings{perez2009onthegeneration,
  doi = {10.1109/ats.2009.37},
  url = {https://doi.org/10.1109/ats.2009.37},
  year = {2009},
  publisher = {{IEEE}},
  author = {W. J. Perez H. and Danilo Ravotto and Ernesto Sanchez and Matteo Sonza Reorda and Alberto Tonda},
  title = {On the Generation of Functional Test Programs for the Cache Replacement Logic},
  booktitle = {2009 Asian Test Symposium},
  abstract = {Caches are crucial components in modern processors (both stand-alone or integrated into SoCs) and their test is a challenging task, especially when addressing complex and high-frequency devices. While the test of the memory array within the cache is usually accomplished resorting to BIST circuitry implementing March test inspired solutions, testing the cache controller logic poses some specific issues, mainly stemming from its limited accessibility. One possible solution consists in letting the processor execute suitable test programs, allowing the detection of possible faults by looking at the results they produce. In this paper we face the issue of generating suitable programs for testing the replacement logic in set-associative caches that implement a deterministic replacement policy. A test program generation approach based on modeling the replacement mechanism as a finite state machine (FSM) is proposed. Experimental results with a cache implementing a LRU policy are provided to assess the effectiveness of the method.}
}

%%% CONTINUE THE PROCESS OF ADDING ABSTRACTS AND REVISING NAMING CONVENTION ABOVE THIS LINE

@inproceedings{gandini2009automatic,
  doi = {10.1145/1569901.1570238},
  url = {https://doi.org/10.1145/1569901.1570238},
  year = {2009},
  publisher = {{ACM} Press},
  author = {Sergio Gandini and Danilo Ravotto and Walter Ruzzarin and Ernesto Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {Automatic detection of software defects},
  booktitle = {Proceedings of the 11th Annual conference on Genetic and evolutionary computation - {GECCO} {\textquotesingle}09},
  abstract = {Mobile phones are becoming more and more complex devices, both from the hardware and from the software point of view. Consequently, their various parts are often developed separately. Each sub-system or application may be worked out by a specialized team of engineers and programmers. Frequently, bugs in one component are triggered by the complex interaction between the different applications. Those errors sometimes lead to power dissipation and other misbehaviors that lower residual battery life, a catastrophic event from the user perspective. In this paper we propose a model-based automatic approach to uncover software bugs, which is intended to complement human expertise and complete a qualifying verification plan. The system has been applied on the prototype of a Motorola mobile phone during a partnership with Politecnico di Torino. We demonstrate that our approach is effective by detecting three distinct software misbehaviours that escape all traditional tests. The paper details the methodology, tests and results.}
}

@inproceedings{dicarlo2010towards,
  doi = {10.1145/1830483.1830727},
  url = {https://doi.org/10.1145/1830483.1830727},
  year = {2010},
  publisher = {{ACM} Press},
  author = {Stefano Di Carlo and Ernesto Sanchez and Alberto Scionti and Giovanni Squillero and Alberto Paolo Tonda and Matteo Falasconi},
  title = {Towards drift correction in chemical sensors using an evolutionary strategy},
  booktitle = {Proceedings of the 12th annual conference on Genetic and evolutionary computation - {GECCO} {\textquotesingle}10},
  abstract = {Gas chemical sensors are strongly affected by the so-called drift, i.e., changes in sensors' response caused by poisoning and aging that may significantly spoil the measures gathered. The paper presents a mechanism able to correct drift, that is: delivering a correct unbiased fingerprint to the end user. The proposed system exploits a state-of-the-art evolutionary strategy to iteratively tweak the coefficients of a linear transformation. The system operates continuously. The optimal correction strategy is learnt without a-priori models or other hypothesis on the behavior of physical-chemical sensors. Experimental results demonstrate the efficacy of the approach on a real problem.}
}

@incollection{sanchez2010evolving,
  doi = {10.1007/978-3-642-12239-2_2},
  url = {https://doi.org/10.1007/978-3-642-12239-2_2},
  year = {2010},
  publisher = {Springer Berlin Heidelberg},
  pages = {11--20},
  author = {Ernesto Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {Evolving Individual Behavior in a Multi-agent Traffic Simulator},
  booktitle = {Applications of Evolutionary Computation},
  abstract = {In this paper, we illustrate the use of evolutionary agents in a multi-agent system designed to describe the behavior of car drivers. Each agent has the selfish objective to reach its destination in the shortest time possible, and a preference in terms of paths to take, based on the presence of other agents and on the width of the roads. Those parameters are changed with an evolutionary strategy, to mimic the adaptation of a human driver to different traffic conditions. The system proposed is then tested by giving the agents the ability to perceive the presence of other agents in a given radius. Experimental results show that knowing the position of all the car drivers in the map leads the agents to obtain a better performance, thanks to the evolution of their behavior. Even the system as a whole gains some benefits from the evolution of the agents' individual choices.}
}

@incollection{dicarlo2010exploiting,
  doi = {10.1007/978-3-642-12239-2_43},
  url = {https://doi.org/10.1007/978-3-642-12239-2_43},
  year = {2010},
  publisher = {Springer Berlin Heidelberg},
  pages = {412--421},
  author = {Stefano Di Carlo and Matteo Falasconi and Ernesto S{\'{a}}nchez and Alberto Scionti and Giovanni Squillero and Alberto Tonda},
  title = {Exploiting Evolution for an Adaptive Drift-Robust Classifier in Chemical Sensing},
  booktitle = {Applications of Evolutionary Computation},
  abstract = {Gas chemical sensors are strongly affected by drift, i.e., changes in sensors’ response with time, that may turn statistical models commonly used for classification completely useless after a period of time. This paper presents a new classifier that embeds an adaptive stage able to reduce drift effects. The proposed system exploits a state-of-the-art evolutionary strategy to iteratively tweak the coefficients of a linear transformation able to transparently transform raw measures in order to mitigate the negative effects of the drift. The system operates continuously. The optimal correction strategy is learnt without a-priori models or other hypothesis on the behavior of physical-chemical sensors. Experimental results demonstrate the efficacy of the approach on a real problem.}
}

@inproceedings{perez2011functional,
  doi = {10.1109/latw.2011.5985898},
  url = {https://doi.org/10.1109/latw.2011.5985898},
  year = {2011},
  month = mar,
  publisher = {{IEEE}},
  author = {W. J. H. Perez and Ernesto Sanchez and Matteo Sonza Reorda and Alberto Tonda and Juan Velasco Medina},
  title = {Functional Test Generation for the {pLRU} Replacement Mechanism of Embedded Cache Memories},
  booktitle = {2011 12th Latin American Test Workshop ({LATW})},
  abstract = {Testing cache memories is a challenging task, especially when targeting complex and high-frequency devices such as modern processors. While the memory array in a cache is usually tested exploiting BIST circuits that implement March-based solutions, there is no established methodology to tackle the cache controller logic, mainly due to its limited accessibility. One possible approach is Software-Based Self Testing (SBST): however, devising test programs able to thoroughly excite the replacement logic and made the results observable is not trivial. A test program generation approach, based on a Finite State Machine (FSM) model of the replacement mechanism, is proposed in this paper. The effectiveness of the method is assessed on a case study considering a data cache implementing the pLRU replacement policy.}
}

@inproceedings{dicarlo2011covariance,
  doi = {10.1063/1.3626293},
  url = {https://doi.org/10.1063/1.3626293},
  year = {2011},
  publisher = {{AIP}},
  author = {Stefano Di Carlo and Matteo Falasconi and Ernesto Sanchez and Giovanni Sberveglieri and Alberto Scionti and Giovanni Squillero and Alberto Tonda and Perena Gouma},
  title = {Covariance Matrix Adaptation Evolutionary Strategy for Drift Correction of Electronic Nose Data},
  booktitle = {Proceedings of International Symposium on Olfaction and Electronic Nose 2011},
  abstract = {Electronic Noses (ENs) might represent a simple, fast, high sample throughput and economic alternative to conventional analytical instruments. However, gas sensors drift still limits the EN adoption in real industrial setups due to high recalibration effort and cost. In fact, pattern recognition (PaRC) models built in the training phase become useless after a period of time, in some cases a few weeks. Although algorithms to mitigate the drift date back to the early 90 this is still a challenging issue for the chemical sensor community. Among other approaches, adaptive drift correction methods adjust the PaRC model in parallel with data acquisition without need of periodic calibration. Self‐Organizing Maps (SOMs) and Adaptive Resonance Theory (ART) networks have been already tested in the past with fair success. This paper presents and discusses an original methodology based on a Covariance Matrix Adaptation Evolution Strategy (CMA‐ES), suited for stochastic optimization of complex problems.}
}

@inproceedings{sanchez2011group, % Sanchez2011a
  doi = {10.1109/cec.2011.5949951},
  url = {https://doi.org/10.1109/cec.2011.5949951},
  year = {2011},
  month = jun,
  publisher = {{IEEE}},
  author = {Ernesto Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {Group evolution: Emerging synergy through a coordinated effort},
  booktitle = {2011 {IEEE} Congress of Evolutionary Computation ({CEC})},
  abstract = {A huge number of optimization problems, in the CAD area as well as in many other fields, require a solution composed by a set of structurally homogeneous elements. Each element tackles a subset of the original task, and they cumulatively solve the whole problem. Sub-tasks, however, have exactly the same structure, and the splitting is completely arbitrary. Even the number of sub-tasks is not known and cannot be determined a-priori. Individual elements are structurally homogeneous, and their contribution to the main solution can be evaluated separately. We propose an evolutionary algorithm able to optimize groups of individuals for solving this class of problems. An individual of the best solution may be sub-optimal when considered alone, but the set of individuals cumulatively represent the optimal group able to completely solve the whole problem. Results of preliminary experiments show that our algorithm performs better than other techniques commonly applied in the CAD field.}
}

@incollection{sanchez2011evolution, % was Sanchez2011b
  doi = {10.1007/978-3-642-20520-0_17},
  url = {https://doi.org/10.1007/978-3-642-20520-0_17},
  year = {2011},
  publisher = {Springer Berlin Heidelberg},
  pages = {162--171},
  author = {Ernesto Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {Evolution of Test Programs Exploiting a {FSM} Processor Model},
  booktitle = {Applications of Evolutionary Computation},
  abstract = {Microprocessor testing is becoming a challenging task, due to the increasing complexity of modern architectures. Nowadays, most architectures are tackled with a combination of scan chains and Software-Based Self-Test (SBST) methodologies. Among SBST techniques, evolutionary feedback-based ones prove effective in microprocessor testing: their main disadvantage, however, is the considerable time required to generate suitable test programs. A novel evolutionary-based approach, able to appreciably reduce the generation time, is presented. The proposed method exploits a high-level representation of the architecture under test and a dynamically built Finite State Machine (FSM) model to assess fault coverage without resorting to time-expensive simulations on low-level models. Experimental results, performed on an OpenRISC processor, show that the resulting test obtains a nearly complete fault coverage against the targeted fault model.}
}

@inproceedings{sanchez2011evolutionary, % was Sanchez2011c
  doi = {10.1145/2001858.2001985},
  url = {https://doi.org/10.1145/2001858.2001985},
  year = {2011},
  publisher = {{ACM} Press},
  author = {Ernesto Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {Evolutionary failing-test generation for modern microprocessors},
  booktitle = {Proceedings of the 13th annual conference companion on Genetic and evolutionary computation - {GECCO} {\textquotesingle}11},
  abstract = {The incessant progress in manufacturing technology is posing new challenges to microprocessor designers. Nowadays, comprehensive verification of a chip can only be performed after tape-out, when the first silicon prototypes are available. Several activities that were originally supposed to be part of the pre-silicon design phase are migrating to this post-silicon time as well. The short paper describes a post-silicon methodology that can be exploited to devise functional failing tests. Such tests are essential to analyze and debug speed paths during verification, speed-stepping, and other critical activities. The proposed methodology is based on the Genetic Programming paradigm, and exploits a versatile toolkit named μGP. The paper demonstrates that an evolutionary algorithm can successfully tackle a significant and still open industrial problem. Moreover, it shows how to take into account complex hardware characteristics and architectural details of such complex devices.}
}

@inproceedings{sanchez2011onthefunctional, % was Sanchez2011d
  doi = {10.1109/vlsisoc.2011.6081650},
  url = {https://doi.org/10.1109/vlsisoc.2011.6081650},
  year = {2011},
  month = oct,
  publisher = {{IEEE}},
  author = {Ernesto Sanchez and Matteo Sonza Reorda and Alberto Tonda},
  title = {On the functional test of Branch Prediction Units based on Branch History Table},
  booktitle = {2011 {IEEE}/{IFIP} 19th International Conference on {VLSI} and System-on-Chip},
  abstract = {Branch Prediction Units (BPUs) are highly efficient modules that can significantly decrease the negative impact of branches in superscalar and RISC processors. Traditional test solutions, mainly based on scan test, are often inadequate to tackle the complexity of these architectures, especially when dealing with delay faults that require at-speed stimuli application. Moreover, scan test does not represent a viable solution when Incoming Inspection or on-line test are considered. In this paper a functional approach targeting BPU test is proposed, allowing to generate a suitable test program whose effectiveness is independent on the specific implementation of the BPU. The effectiveness of the approach is validated on a Branch History Table (BHT) resorting to an open-source computer architecture simulator and to an ad hoc developed HDL testbench. Experimental results show that the proposed method is able to thoroughly test the BHT, reaching complete static fault coverage.}
}

@inproceedings{sanchez2011postsilicon, % was Sanchez2011e
  doi = {10.1109/vlsisoc.2011.6081667},
  url = {https://doi.org/10.1109/vlsisoc.2011.6081667},
  year = {2011},
  month = oct,
  publisher = {{IEEE}},
  author = {Ernesto Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {Post-silicon failing-test generation through evolutionary computation},
  booktitle = {2011 {IEEE}/{IFIP} 19th International Conference on {VLSI} and System-on-Chip},
  abstract = {The incessant progress in manufacturing technology is posing new challenges to microprocessor designers. Several activities that were originally supposed to be part of the pre-silicon design phase are migrating after tape-out, when the first silicon prototypes are available. The paper describes a post-silicon methodology for devising functional failing tests. Therefore, suited to be exploited by microprocessor producer to detect, analyze and debug speed paths during verification, speed-stepping, or other critical activities. The proposed methodology is based on an evolutionary algorithm and exploits a versatile toolkit named μGP. The paper describes how to take into account complex hardware characteristics and architectural details of such complex devices. The experimental evaluation clearly demonstrates the potential of this line of research.}
}

@incollection{tonda2011lamps,
  doi = {10.1007/978-3-642-24094-2_7},
  url = {https://doi.org/10.1007/978-3-642-24094-2_7},
  year = {2011},
  publisher = {Springer Berlin Heidelberg},
  pages = {101--120},
  author = {Alberto Tonda and Evelyne Lutton and Giovanni Squillero},
  title = {Lamps: A Test Problem for Cooperative Coevolution},
  booktitle = {Nature Inspired Cooperative Strategies for Optimization ({NICSO} 2011)},
  abstract = {We present an analysis of the behaviour of Cooperative Co-evolution algorithms (CCEAs) on a simple test problem, that is the optimal placement of a set of lamps in a square room, for various problems sizes. Cooperative Co-evolution makes it possible to exploit more efficiently the artificial Darwinism scheme, as soon as it is possible to turn the optimisation problem into a co-evolution of interdependent sub-parts of the searched solution. We show here how two cooperative strategies, Group Evolution (GE) and Parisian Evolution (PE) can be built for the lamps problem. An experimental analysis then compares a classical evolution to GE and PE, and analyses their behaviour with respect to scale.}
}

@inproceedings{sanchez2011automatic,
  doi = {10.1109/mtv.2011.19},
  url = {https://doi.org/10.1109/mtv.2011.19},
  year = {2011},
  month = dec,
  publisher = {{IEEE}},
  author = {Ernesto Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {Automatic Generation of Software-based Functional Failing Test for Speed Debug and On-silicon Timing Verification},
  booktitle = {2011 12th International Workshop on Microprocessor Test and Verification},
  abstract = {The 40 years since the appearance of the Intel 4004 deeply changed how microprocessors are designed. Today, essential steps in the validation process are performed relying on physical dices, analyzing the actual behavior under appropriate stimuli. This paper presents a methodology that can be used to devise assembly programs suitable for a range of on-silicon activities, like speed debug, timing verification or speed binning. The methodology is fully automatic. It exploits the feedback from the microprocessor under examination and does not rely on information about its microarchitecture, nor does it require design-for-debug features. The experimental evaluation performed on a Intel Pentium Core i7-950 demonstrates the feasibility of the approach.}
}

@incollection{tonda2012bayesian,
  doi = {10.1007/978-3-642-29139-5_22},
  url = {https://doi.org/10.1007/978-3-642-29139-5_22},
  year = {2012},
  publisher = {Springer Berlin Heidelberg},
  pages = {254--265},
  author = {Alberto Paolo Tonda and Evelyne Lutton and Romain Reuillon and Giovanni Squillero and Pierre-Henri Wuillemin},
  title = {Bayesian Network Structure Learning from Limited Datasets through Graph Evolution},
  booktitle = {Lecture Notes in Computer Science},
  abstract = {Bayesian networks are stochastic models, widely adopted to encode knowledge in several fields. One of the most interesting features of a Bayesian network is the possibility of learning its structure from a set of data, and subsequently use the resulting model to perform new predictions. Structure learning for such models is a NP-hard problem, for which the scientific community developed two main approaches: score-and-search metaheuristics, often evolutionary-based, and dependency-analysis deterministic algorithms, based on stochastic tests. State-of-the-art solutions have been presented in both domains, but all methodologies start from the assumption of having access to large sets of learning data available, often numbering thousands of samples. This is not the case for many real-world applications, especially in the food processing and research industry. This paper proposes an evolutionary approach to the Bayesian structure learning problem, specifically tailored for learning sets of limited size. Falling in the category of score-and-search techniques, the methodology exploits an evolutionary algorithm able to work directly on graph structures, previously used for assembly language generation, and a scoring function based on the Akaike Information Criterion, a well-studied metric of stochastic model performance. Experimental results show that the approach is able to outperform a state-of-the-art dependency-analysis algorithm, providing better models for small datasets.}
}

@inproceedings{ciganda2012automatic,
  doi = {10.1109/mtv.2012.17},
  url = {https://doi.org/10.1109/mtv.2012.17},
  year = {2012},
  month = dec,
  publisher = {{IEEE}},
  author = {Lyl Mercedes Ciganda and Marco Gaudesi and Evelyne Lutton and Ernesto Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {Automatic Generation of On-Line Test Programs through a Cooperation Scheme},
  booktitle = {2012 13th International Workshop on Microprocessor Test and Verification ({MTV})},
  abstract = {Test programs for Software-based Self-Test (SBST) can be exploited during the mission phase of microprocessor-based systems to periodically assess hardware integrity. However, several additional constraints must be imposed due to the coexistence of test programs with the mission application. This paper proposes a method for the generation of SBST on-line test programs for embedded RISC processors, systems where the impact of on-line constraints is significant. The proposed strategy exploits an evolutionary optimizer that is able to create a complete test set of programs relying on a new cooperative scheme. Experimental results showed high fault coverage values on two different modules of a MIPS-like processor core. These two case studies demonstrate the effectiveness of the technique and the low human effort required for its implementation.}
}

@inproceedings{gaudesi2013evolutionary,
  doi = {10.1145/2480362.2480400},
  url = {https://doi.org/10.1145/2480362.2480400},
  year = {2013},
  publisher = {{ACM} Press},
  author = {Marco Gaudesi and Andrea Marion and Tommaso Musner and Giovanni Squillero and Alberto Tonda},
  title = {Evolutionary optimization of wetlands design},
  booktitle = {Proceedings of the 28th Annual {ACM} Symposium on Applied Computing - {SAC} {\textquotesingle}13},
  abstract = {Wetlands are artificial ponds, designed to filter and purify running water through the contact with plant stems and roots. Wetland layouts are traditionally designed by experts through a laborious and time-consuming procedure: in principle, small patches of vegetation with purifying properties are tentatively placed, then the resulting water flow is verified by fluid dynamics simulators and when a satisfying outcome is reached, the wetland final layout is decided. This paper proposes to automate wetland design exploiting an evolutionary algorithm: a population of candidate solutions is cultivated by the evolutionary core, and their efficiency is evaluated using a state-of-the-art fluid-dynamics simulation framework. Experimental results show that the results obtained by the proposed approach are qualitatively comparable with those provided by experts, despite the complete absence of human intervention during the optimization process.}
}

@incollection{tonda2013amemetic,
  doi = {10.1007/978-3-642-37192-9_11},
  url = {https://doi.org/10.1007/978-3-642-37192-9_11},
  year = {2013},
  publisher = {Springer Berlin Heidelberg},
  pages = {102--111},
  author = {Alberto Tonda and Evelyne Lutton and Giovanni Squillero and Pierre-Henri Wuillemin},
  title = {A Memetic Approach to Bayesian Network Structure Learning},
  booktitle = {Applications of Evolutionary Computation},
  abstract = {Bayesian networks are graphical statistical models that represent inference between data. For their effectiveness and versatility, they are widely adopted to represent knowledge in different domains. Several research lines address the NP-hard problem of Bayesian network structure learning starting from data: over the years, the machine learning community delivered effective heuristics, while different Evolutionary Algorithms have been devised to tackle this complex problem. This paper presents a Memetic Algorithm for Bayesian network structure learning, that combines the exploratory power of an Evolutionary Algorithm with the speed of local search. Experimental results show that the proposed approach is able to outperform state-of-the-art heuristics on two well-studied benchmarks.}
}

@incollection{bucur2013anevolutionary,
  doi = {10.1007/978-3-642-37192-9_1},
  url = {https://doi.org/10.1007/978-3-642-37192-9_1},
  year = {2013},
  publisher = {Springer Berlin Heidelberg},
  pages = {1--11},
  author = {Doina Bucur and Giovanni Iacca and Giovanni Squillero and Alberto Tonda},
  title = {An Evolutionary Framework for Routing Protocol Analysis in Wireless Sensor Networks},
  booktitle = {Applications of Evolutionary Computation},
  abstract = {Wireless Sensor Networks (WSNs) are widely adopted for applications ranging from surveillance to environmental monitoring. While powerful and relatively inexpensive, they are subject to behavioural faults which make them unreliable. Due to the complex interactions between network nodes, it is difficult to uncover faults in a WSN by resorting to formal techniques for verification and analysis, or to testing. This paper proposes an evolutionary framework to detect anomalous behaviour related to energy consumption in WSN routing protocols. Given a collection protocol, the framework creates candidate topologies and evaluates them through simulation on the basis of metrics measuring the radio activity on nodes. Experimental results using the standard Collection Tree Protocol show that the proposed approach is able to unveil topologies plagued by excessive energy depletion over one or more nodes, and thus could be used as an offline debugging tool to understand and correct the issues before network deployment and during the development of new protocols.}
}

@incollection{gaudesi2013anevolutionary,
  doi = {10.1007/978-3-642-37189-9_16},
  url = {https://doi.org/10.1007/978-3-642-37189-9_16},
  year = {2013},
  publisher = {Springer Berlin Heidelberg},
  pages = {177--187},
  author = {Marco Gaudesi and Andrea Marion and Tommaso Musner and Giovanni Squillero and Alberto Tonda},
  title = {An Evolutionary Approach to Wetlands Design},
  booktitle = {Evolutionary Computation,  Machine Learning and Data Mining in Bioinformatics},
  abstract = {Wetlands are artificial basins that exploit the capabilities of some species of plants to purify water from pollutants. The design process is currently long and laborious: such vegetated areas are inserted within the basin by trial and error, since there is no automatic system able to maximize the efficiency in terms of filtering. Only at the end of several attempts, experts are able to determine which is the most convenient configuration and choose up a layout. This paper proposes the use of an evolutionary algorithm to automate both the placement and the sizing of vegetated areas within a basin. The process begins from a random population of solutions and, evaluating their efficiency with an state-of-the-art fluid-dynamics simulation framework, the evolutionary algorithm is able to automatically find optimized solution whose performance are comparable with those achieved by human experts.}
}

@inproceedings{gaudesi2013anefficient,
  doi = {10.1145/2463372.2463495},
  url = {https://doi.org/10.1145/2463372.2463495},
  year = {2013},
  publisher = {{ACM} Press},
  author = {Marco Gaudesi and Giovanni Squillero and Alberto Tonda},
  title = {An efficient distance metric for linear genetic programming},
  booktitle = {Proceeding of the fifteenth annual conference on Genetic and evolutionary computation conference - {GECCO} {\textquotesingle}13},
  abstract = {Defining a distance measure over the individuals in the population of an Evolutionary Algorithm can be exploited for several applications, ranging from diversity preservation to balancing exploration and exploitation. When individuals are encoded as strings of bits or sets of real values, computing the distance between any two can be a straightforward process; when individuals are represented as trees or linear graphs, however, quite often the user must resort to phenotype-level problem-specific distance metrics. This paper presents a generic genotype-level distance metric for Linear Genetic Programming: the information contained by an individual is represented as a set of symbols, using n-grams to capture significant recurring structures inside the genome. The difference in information between two individuals is evaluated resorting to a symmetric difference. Experimental evaluations show that the proposed metric has a strong correlation with phenotype-level problem-specific distance measures in two problems where individuals represent string of bits and Assembly-language programs, respectively.}
}

@incollection{tonda2014balancing,
  doi = {10.1007/978-3-319-11683-9_17},
  url = {https://doi.org/10.1007/978-3-319-11683-9_17},
  year = {2014},
  publisher = {Springer International Publishing},
  pages = {211--223},
  author = {Alberto Tonda and Andre Spritzer and Evelyne Lutton},
  title = {Balancing User Interaction and Control in {BNSL}},
  booktitle = {Lecture Notes in Computer Science},
  abstract = {In this paper we present a study based on an evolutionary framework to explore what would be a reasonable compromise between interaction and automated optimisation in finding possible solutions for a complex problem, namely the learning of Bayesian network structures, an NP-hard problem where user knowledge can be crucial to distinguish among solutions of equal fitness but very different physical meaning. Even though several classes of complex problems can be effectively tackled with Evolutionary Computation, most possess qualities that are difficult to directly encode in the fitness function or in the individual’s genotype description. Expert knowledge can sometimes be used to integrate the missing information, but new challenges arise when searching for the best way to access it: full human interaction can lead to the well-known problem of user-fatigue, while a completely automated evolutionary process can miss important contributions by the expert. For our study, we developed a GUI-based prototype application that lets an expert user guide the evolution of a network by alternating between fully-interactive and completely automatic steps. Preliminary user tests were able to show that despite still requiring some improvements with regards to its efficiency, the proposed approach indeed achieves its goal of delivering satisfying results for an expert user.}
}

@inproceedings{cani2014towards,
  doi = {10.1145/2554850.2555157},
  url = {https://doi.org/10.1145/2554850.2555157},
  year = {2014},
  publisher = {{ACM} Press},
  author = {Andrea Cani and Marco Gaudesi and Ernesto Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {Towards automated malware creation},
  booktitle = {Proceedings of the 29th Annual {ACM} Symposium on Applied Computing - {SAC} {\textquotesingle}14},
  abstract = {This short paper proposes two different ways for exploiting an evolutionary algorithm to devise malware: the former targeting heuristic-based anti-virus scanner; the latter optimizing a Trojan attack. An extended internal on the same the subject can be downloaded from \url{https://www.cad.polito.it/downloads/}}
}

@incollection{gaucel2014learning,
  doi = {10.1007/978-3-662-44303-3_3},
  url = {https://doi.org/10.1007/978-3-662-44303-3_3},
  year = {2014},
  publisher = {Springer Berlin Heidelberg},
  pages = {25--36},
  author = {S{\'{e}}bastien Gaucel and Maarten Keijzer and Evelyne Lutton and Alberto Tonda},
  title = {Learning Dynamical Systems Using Standard Symbolic Regression},
  booktitle = {Lecture Notes in Computer Science},
  abstract = {Symbolic regression has many successful applications in learning free-form regular equations from data. Trying to apply the same approach to differential equations is the logical next step: so far, however, results have not matched the quality obtained with regular equations, mainly due to additional constraints and dependencies between variables that make the problem extremely hard to tackle. In this paper we propose a new approach to dynamic systems learning. Symbolic regression is used to obtain a set of first-order Eulerian approximations of differential equations, and mathematical properties of the approximation are then exploited to reconstruct the original differential equations. Advantages of this technique include the de-coupling of systems of differential equations, that can now be learned independently; the possibility of exploiting established techniques for standard symbolic regression, after trivial operations on the original dataset; and the substantial reduction of computational effort, when compared to existing ad-hoc solutions for the same purpose. Experimental results show the efficacy of the proposed approach on an instance of the Lotka-Volterra model.}
}

@inproceedings{gaudesi2014universal,
  doi = {10.1145/2598394.2598440},
  url = {https://doi.org/10.1145/2598394.2598440},
  year = {2014},
  publisher = {{ACM} Press},
  author = {Marco Gaudesi and Giovanni Squillero and Alberto Tonda},
  title = {Universal information distance for genetic programming},
  booktitle = {Proceedings of the 2014 conference companion on Genetic and evolutionary computation companion - {GECCO} Comp {\textquotesingle}14},
  abstract = {This paper presents a genotype-level distance metric for Genetic Programming (GP) based on the symmetric difference concept: first, the information contained in individuals is expressed as a set of symbols (the content of each node, its position inside the tree, and recurring parent-child structures); then, the difference between two individuals is computed considering the number of elements belonging to one, but not both, of their symbol sets.}
}

@inproceedings{bucur2014thetradeoffs,
  doi = {10.1145/2576768.2598384},
  url = {https://doi.org/10.1145/2576768.2598384},
  year = {2014},
  publisher = {{ACM} Press},
  author = {Doina Bucur and Giovanni Iacca and Giovanni Squillero and Alberto Tonda},
  title = {The tradeoffs between data delivery ratio and energy costs in wireless sensor networks},
  booktitle = {Proceedings of the 2014 conference on Genetic and evolutionary computation - {GECCO} {\textquotesingle}14},
  abstract = {Wireless sensor network (WSN) routing protocols, e.g., the Collection Tree Protocol (CTP), are designed to adapt in an ad-hoc fashion to the quality of the environment. WSNs thus have high internal dynamics and complex global behavior. Classical techniques for performance evaluation (such as testing or verification) fail to uncover the cases of extreme behavior which are most interesting to designers. We contribute a practical framework for performance evaluation of WSN protocols. The framework is based on multi-objective optimization, coupled with protocol simulation and evaluation of performance factors. For evaluation, we consider the two crucial functional and non-functional performance factors of a WSN, respectively: the ratio of data delivery from the network (DDR), and the total energy expenditure of the network (COST). We are able to discover network topological configurations over which CTP has unexpectedly low DDR and/or high COST performance, and expose full Pareto fronts which show what the possible performance tradeoffs for CTP are in terms of these two performance factors. Eventually, Pareto fronts allow us to bound the state space of the WSN, a fact which provides essential knowledge to WSN protocol designers.}
}

@inproceedings{gaudesi2014turan,
  doi = {10.1109/cec.2014.6900564},
  url = {https://doi.org/10.1109/cec.2014.6900564},
  year = {2014},
  month = jul,
  publisher = {{IEEE}},
  author = {Marco Gaudesi and Elio Piccolo and Giovanni Squillero and Alberto Tonda},
  title = {{TURAN}: Evolving non-deterministic players for the iterated prisoner{\textquotesingle}s dilemma},
  booktitle = {2014 {IEEE} Congress on Evolutionary Computation ({CEC})},
  abstract = {The iterated prisoner's dilemma is a widely known model in game theory, fundamental to many theories of cooperation and trust among self-interested beings. There are many works in literature about developing efficient strategies for this problem, both inside and outside the machine learning community. This paper shift the focus from finding a ``good strategy'' in absolute terms, to dynamically adapting and optimizing the strategy against the current opponent. Turan evolves competitive non-deterministic models of the current opponent, and exploit them to predict its moves and maximize the payoff as the game develops. Experimental results show that the proposed approach is able to obtain good performances against different kind of opponent, whether their strategies can or cannot be implemented as finite state machines.}
}

@inproceedings{descamps2015modeling,
  year = {2015},
  author = {Etienne Descamps and Alberto Tonda and S\'{e}bastien Gaucel and Ioan Cristian Trelea and Evelyne Lutton and Nathalie M\'{e}jean-Perrot},
  title = {Modeling Competition Phenomena in a Dairy Oil-in-water Emulsion Using Hybrid Kinetic Monte Carlo Simulations},
  booktitle = {Proceedings of the 6th International Symposium on Delivery of Functionality in Complex Food Systems 2015},
  abstract = {The design of models for dairy products raises a series of difficult issues. For instance, considering dairy oil in water emulsions stabilized with milk proteins, texture depends in a non-trivial manner on the initial concentration and type of proteins, nature of heat treatment and type of homogenisation. Those emulsions, involving competitive adsorption of mixed particles in a turbulent way at the oil/water interface, are not thermodynamically controlled. Classical models like the Langmuir one are thus not able to predict its behaviour with precision (Dickinson, 2011). Hybrid models (Descamps, 2014) have been recently proved to be promising for dealing with those complex phenomena. We present an extension of this approach, using an individual-­‐based framework whose implementation is based on a kinetic Monte Carlo approach (MC) combined with a mean field model. MC schemes are widely used in chemical science to deal with discrete events (Gillespie, 75). Individual-based models (also known as agent-­‐based) are convenient for representing local rules at the nano/micro scale, with macroscopic properties appearing as a consequence of an emergence process. Individual-­‐based frames, however, often rely on stochastic simulations and require time-consuming computations to yield a robust estimation for the emergent quantities. In the proposed methodology, computational efficiency is provided by performing appropriate simplifications along the simulation process thanks to an ODE-based continuous mean field formulation.}
}

@inproceedings{grosvenor2015invitro,
  year = {2015},
  author = {Anita Grosvenor and Alberto Tonda and Steven Le Feunteun and Stefan Clerens},
  title = {In Vitro and In Silico Modelling of Protein Hydrolysis by Pepsin: A Case Study with Lactoferrin},
  booktitle = {Proceedings of the 3rd International Conference on Food Structures, Digestion \& Health},
  abstract = {Unlike other digestive proteases, pepsin specificity is low. It is therefore particularly difficult to determine a priori which peptides will be released during gastric digestion, and hence in the small intestine. Detailed information about food protein truncation during digestion is however critical to understanding and optimizing the availability of bioactives, or limiting allergen release. A stochastic model that dynamically simulates the nature and quantity of peptides that are likely to be produced during pepsin hydrolysis is presented. Model predictions are compared with experimental data taken from (Grosvenor et al. 2014), which includes a list of 89 monitored peptides arising from pepsin hydrolysis of bovine lactoferrin.}
}

@incollection{bucur2015black,
  doi = {10.1007/978-3-319-16549-3_3},
  url = {https://doi.org/10.1007/978-3-319-16549-3_3},
  year = {2015},
  publisher = {Springer International Publishing},
  pages = {29--41},
  author = {Doina Bucur and Giovanni Iacca and Giovanni Squillero and Alberto Tonda},
  title = {Black Holes and Revelations: Using Evolutionary Algorithms to Uncover Vulnerabilities in Disruption-Tolerant Networks},
  booktitle = {Applications of Evolutionary Computation},
  abstract = {A challenging aspect in open ad hoc networks is their resilience against malicious agents. This is especially true in complex, urban-scale scenarios where numerous moving agents carry mobile devices that create a peer-to-peer network without authentication. A requirement for the proper functioning of such networks is that all the peers act legitimately, forwarding the needed messages, and concurring to the maintenance of the network connectivity. However, few malicious agents may easily exploit the movement patterns in the network to dramatically reduce its performance. We propose a methodology where an evolutionary algorithm evolves the parameters of different malicious agents, determining their types and mobility patterns in order to minimize the data delivery rate and maximize the latency of communication in the network. As a case study, we consider a fine-grained simulation of a large-scale disruption-tolerant network in the city of Venice. By evolving malicious agents, we uncover situations where even a single attacker can hamper the network performance, and we correlate the performance decay to the number of malicious agents.}
}

@inproceedings{chabin2015isglobal,
  doi = {10.1145/2739482.2764675},
  url = {https://doi.org/10.1145/2739482.2764675},
  year = {2015},
  publisher = {{ACM} Press},
  author = {Thomas Chabin and Alberto Tonda and Evelyne Lutton},
  title = {Is Global Sensitivity Analysis Useful to Evolutionary Computation?},
  booktitle = {Proceedings of the Companion Publication of the 2015 on Genetic and Evolutionary Computation Conference - {GECCO} Companion {\textquotesingle}15},
  abstract = {Global Sensitivity Analysis (GSA) studies how uncertainty in the inputs of a system influences uncertainty in its outputs. GSA is extensively used by experts to gather information about the behavior of models, through computationally-intensive stochastic sampling of parameters' space. Some studies propose to make use of the considerable quantity of data acquired in this way to optimize the model parameters, often resorting to Evolutionary Algorithms (EAs). Nevertheless, efficiently exploiting information gathered from GSA might not be so straightforward. In this paper, we present a counterexample followed by experimental results to prove how naively combining GSA and EA can bring about negative outcomes.}
}

@inproceedings{belluz2015operator,
  doi = {10.1145/2739480.2754712},
  url = {https://doi.org/10.1145/2739480.2754712},
  year = {2015},
  publisher = {{ACM} Press},
  author = {Jany Belluz and Marco Gaudesi and Giovanni Squillero and Alberto Tonda},
  title = {Operator Selection using Improved Dynamic Multi-Armed Bandit},
  booktitle = {Proceedings of the 2015 on Genetic and Evolutionary Computation Conference - {GECCO} {\textquotesingle}15},
  abstract = {Evolutionary algorithms greatly benefit from an optimal application of the different genetic operators during the optimization process: thus, it is not surprising that several research lines in literature deal with the self-adapting of activation probabilities for operators. The current state of the art revolves around the use of the Multi-Armed Bandit (MAB) and Dynamic Multi-Armed bandit (D-MAB) paradigms, that modify the selection mechanism based on the rewards of the different operators. Such methodologies, however, update the probabilities after each operator's application, creating possible issues with positive feedbacks and impairing parallel evaluations, one of the strongest advantages of evolutionary computation in an industrial perspective. Moreover, D-MAB techniques often rely upon measurements of population diversity, that might not be applicable to all real-world scenarios. In this paper, we propose a generalization of the D-MAB approach, paired with a simple mechanism for operator management, that aims at removing several limitations of other D-MAB strategies, allowing for parallel evaluations and self-adaptive parameter tuning. Experimental results show that the approach is particularly effective with frameworks containing many different operators, even when some of them are ill-suited for the problem at hand, or are sporadically failing, as it commonly happens in the real world.}
}

@inproceedings{gaudesi2015malware,
  doi = {10.1145/2739482.2764940},
  url = {https://doi.org/10.1145/2739482.2764940},
  year = {2015},
  publisher = {{ACM} Press},
  author = {Marco Gaudesi and Andrea Marcelli and Ernesto Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {Malware Obfuscation through Evolutionary Packers},
  booktitle = {Proceedings of the Companion Publication of the 2015 on Genetic and Evolutionary Computation Conference - {GECCO} Companion {\textquotesingle}15},
  abstract = {A malicious botnet is a collection of compromised hosts coordinated by an external entity. The malicious software, or malware, that infect the systems are its basic units and they are responsible for its global behavior. Anti Virus software and Intrusion Detection Systems detect botnets by analyzing network and files, looking for signature and known behavioral patterns. Thus, the malware hiding capability is a crucial aspect. This paper describes a new obfuscation mechanism based on evolutionary algorithms: an evolutionary core is embedded in the malware to generate a different, optimized hiding strategy for every single infection. Such always-changing, hard-to-detect malware can be used by security industries to stress the analysis methodologies and to test the ability to react to malware mutations. This research is the first step in a more ambitious research project, where a whole botnet, composed of different malware and Anti Virus software, is analyzed as a prey-predator ecosystem.}
}

@inproceedings{garciasanchez2015towards,
  doi = {10.1109/cig.2015.7317940},
  url = {https://doi.org/10.1109/cig.2015.7317940},
  year = {2015},
  month = aug,
  publisher = {{IEEE}},
  author = {Pablo Garcia-Sanchez and Alberto Tonda and Antonio M. Mora and Giovanni Squillero and Juan J. Merelo},
  title = {Towards automatic {StarCraft} strategy generation using genetic programming},
  booktitle = {2015 {IEEE} Conference on Computational Intelligence and Games ({CIG})},
  abstract = {Among Real-Time Strategy games few titles have enjoyed the continued success of StarCraft. Many research lines aimed at developing Artificial Intelligences, or ``bots'', capable of challenging human players, use StarCraft as a platform. Several characteristics make this game particularly appealing for researchers, such as: asymmetric balanced factions, considerable complexity of the technology trees, large number of units with unique features, and potential for optimization both at the strategical and tactical level. In literature, various works exploit evolutionary computation to optimize particular aspects of the game, from squad formation to map exploration; but so far, no evolutionary approach has been applied to the development of a complete strategy from scratch. In this paper, we present the preliminary results of StarCraftGP, a framework able to evolve a complete strategy for StarCraft, from the building plan, to the composition of squads, up to the set of rules that define the bot's behavior during the game. The proposed approach generates strategies as C++ classes, that are then compiled and executed inside the OpprimoBot open-source framework. In a first set of runs, we demonstrate that StarCraftGP ultimately generates a competitive strategy for a Zerg bot, able to defeat several human-designed bots.}
}

@incollection{chabin2016how,
  doi = {10.1007/978-3-319-31471-6_4},
  url = {https://doi.org/10.1007/978-3-319-31471-6_4},
  year = {2016},
  publisher = {Springer International Publishing},
  pages = {44--57},
  author = {Thomas Chabin and Alberto Tonda and Evelyne Lutton},
  title = {How to Mislead an Evolutionary Algorithm Using Global Sensitivity Analysis},
  booktitle = {Lecture Notes in Computer Science},
  abstract = {The idea of exploiting Global Sensitivity Analysis (GSA) to make Evolutionary Algorithms more effective seems very attractive: intuitively, a probabilistic analysis can prove useful to a stochastic optimisation technique. GSA, that gathers information about the behaviour of functions receiving some inputs and delivering one or several outputs, is based on computationally-intensive stochastic sampling of a parameter space. Nevertheless, efficiently exploiting information gathered from GSA might not be so straightforward. In this paper, we present three mono- and multi-objective counterexamples to prove how naively combining GSA and EA may mislead an optimisation process.}
}

@inproceedings{tonda2016foodmc,
  title = {{FoodMC: A European COST Action on Food Modelling}},
  author = {Tonda, Alberto},
  booktitle = {Proceedings of FoodSIM 2016, 9th bi-annual International Conference on Modelling and Simulation in Food Engineering},
  address = {Ghent, Belgium},
  year = {2016},
  month = apr,
  abstract = {Methodologies and tools from Maths and Computer Science (MCS) are emerging as key contributors to modernization and optimization of processes in various disciplines: the agri-food sector, however, is not a traditional domain of application for MCS, and at the moment there is no community organized around solving the issues of this field. The COST Action FoodMC brings together scientists and practitioners from MCS and agri-food domains, stimulating the emergence of new research, and structuring a new community to coordinate further investigation efforts. Exploiting approaches originating at different sub-fields of MCS, from applied mathematical models to knowledge engineering, this COST Action will cover two main topics: understanding and controlling agri-food processes; and eco-design of agri-food products.}
}

@inproceedings{boukhelifa2016research,
  TITLE = {{Research Prospects in the Design and Evaluation of Interactive Evolutionary Systems for Art and Science}},
  AUTHOR = {Boukhelifa, Nadia and Bezerianos, Anastasia and Tonda, Alberto and Lutton, Evelyne},
  BOOKTITLE = {{CHI workshop on Human Centred Machine Learning}},
  ADDRESS = {San Jose, United States},
  YEAR = {2016},
  abstract = {We report on our experience in designing and evaluating \emph{seven} applications from \emph{seven} different domains using an interactive evolutionary approach. We conducted extensive evaluations for some of these applications, both quantitative and qualitative, and collected rich feedback from our ongoing collaborations with end-user scientists and artists. To ground our discussion, we refer to two applications, from art and science, as exemplars of our work in order to identify strengths and weaknesses in our approach. We argue that human-centered design could play an important role in addressing some of the identified issues such as the ``black box'' and the ``user-bottleneck'' effects.  We discuss research opportunities requiring human-computer interaction methodologies in order to support both the visible and hidden roles that humans play in interactive evolutionary computation and  machine learning.}
}

@incollection{deplano2016portfolio,
  doi = {10.1007/978-3-319-31204-0_5},
  url = {https://doi.org/10.1007/978-3-319-31204-0_5},
  year = {2016},
  publisher = {Springer International Publishing},
  pages = {58--72},
  author = {Igor Deplano and Giovanni Squillero and Alberto Tonda},
  title = {Portfolio Optimization,  a Decision-Support Methodology for Small Budgets},
  booktitle = {Applications of Evolutionary Computation},
  abstract = {Several machine learning paradigms have been applied to financial forecasting, attempting to predict the market’s behavior, with the final objective of profiting from trading shares. While anticipating the performance of such a complex system is far from trivial, this issue becomes even harder when the investors do not have large amounts of money available. In this paper, we present an evolutionary portfolio optimizer for the management of small budgets. The expected returns are modeled resorting to Multi-layer Perceptrons, trained on past market data, and the portfolio composition is chosen by approximating the solution to a multi-objective constrained problem. An investment simulator is then used to measure the portfolio performance. The proposed approach is tested on real-world data from Milan stock exchange, exploiting information from January 2000 to June 2010 to train the framework, and data from July 2010 to August 2011 to validate it. The presented tool is finally proven able to obtain a more than satisfying profit for the considered time frame.}
}

@incollection{gaudesi2016challenging,
  doi = {10.1007/978-3-319-31153-1_11},
  url = {https://doi.org/10.1007/978-3-319-31153-1_11},
  year = {2016},
  publisher = {Springer International Publishing},
  pages = {149--162},
  author = {Marco Gaudesi and Andrea Marcelli and Ernesto Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {Challenging Anti-virus Through Evolutionary Malware Obfuscation},
  booktitle = {Applications of Evolutionary Computation},
  abstract = {The use of anti-virus software has become something of an act of faith. A recent study showed that more than 80\% of all personal computers have anti-virus software installed. However, the protection mechanisms in place are far less effective than users would expect. Malware analysis is a classical example of cat-and-mouse game: as new anti-virus techniques are developed, malware authors respond with new ones to thwart analysis. Every day, anti-virus companies analyze thousands of malware that has been collected through honeypots, hence they restrict the research to only already existing viruses. This article describes a novel method for malware obfuscation based an evolutionary opcode generator and a special ad-hoc packer. The results can be used by the security industry to test the ability of their system to react to malware mutations.}
}

@inproceedings{lutton2016complex,
  title={Complex systems in food science: Human factor issues},
  author={Lutton, Evelyne and Tonda, Alberto and Boukhelifa, Nadia and Perrot, Nathalie},
  BOOKTITLE = {{Proceedings of FoodSIM 2016, 9th bi-annual International Conference on Modelling and Simulation in Food Engineering
}},
  ADDRESS = {Ghent, Belgium},
%  PAGES = {np},
  YEAR = {2016},
  MONTH = Apr,
  abstract = {Building in-silico decision making systems is essential in the food domain, albeit highly difficult. This task strongly relies on multidisciplinary research and in particular on advanced techniques from artificial intelligence. The success of such systems depends on how well they cope with the complex properties of food processes, such as the large variety of interacting components including those related to human expertise; and their dynamic, non-linear, multi-scale, uncertain and non-equilibrium behaviors. Robust stochastic optimization techniques, evolutionary computation and in particular Interactive Evolutionary Computation (IEC) seem to be a fruitful framework for developing food science models. A Human-Centered approach to Interactive Evolutionary Computation is discussed in this paper as a possible pertinent way to cope with challenges related to human factors in this context.}
}

@incollection{marino2016ageneralpurpose,
  doi = {10.1007/978-3-319-45823-6_32},
  url = {https://doi.org/10.1007/978-3-319-45823-6_32},
  year = {2016},
  publisher = {Springer International Publishing},
  pages = {345--352},
  author = {Francesco Marino and Giovanni Squillero and Alberto Tonda},
  title = {A General-Purpose Framework for Genetic Improvement},
  booktitle = {Parallel Problem Solving from Nature {\textendash} {PPSN} {XIV}},
  abstract = {Genetic Improvement is an evolutionary-based technique. Despite its relatively recent introduction, several successful applications have been already reported in the scientific literature: it has been demonstrated able to modify the code complex programs without modifying their intended behavior; to increase performance with regards to speed, energy consumption or memory use. Some results suggest that it could be also used to correct bugs, restoring the software’s intended functionalities. Given the novelty of the technique, however, instances of Genetic Improvement so far rely upon ad-hoc, language-specific implementations. In this paper, we propose a general framework based on the software engineering’s idea of mutation testing coupled with Genetic Programming, that can be easily adapted to different programming languages and objective. In a preliminary evaluation, the framework efficiently optimizes the code of the md5 hash function in C, Java, and Python.}
}

@inproceedings{garciasanchez2016evolutionary,
  doi = {10.1109/cig.2016.7860426},
  url = {https://doi.org/10.1109/cig.2016.7860426},
  year = {2016},
  month = sep,
  publisher = {{IEEE}},
  author = {Pablo Garcia-Sanchez and Alberto Tonda and Giovanni Squillero and Antonio Mora and Juan J. Merelo},
  title = {Evolutionary deckbuilding in HearthStone},
  booktitle = {2016 {IEEE} Conference on Computational Intelligence and Games ({CIG})},
  abstract = {One of the most notable features of collectible card games is deckbuilding, that is, defining a personalized deck before the real game. Deckbuilding is a challenge that involves a big and rugged search space, with different and unpredictable behaviour after simple card changes and even hidden information. In this paper, we explore the possibility of automated deckbuilding: a genetic algorithm is applied to the task, with the evaluation delegated to a game simulator that tests every potential deck against a varied and representative range of human-made decks. In these preliminary experiments, the approach has proven able to create quite effective decks, a promising result that proves that, even in this challenging environment, evolutionary algorithms can find good solutions.}
}

@inproceedings{grosvenor2017pepsin,
  year = {2017},
  author = {Alberto Tonda and Anita Grosvenor and Stefan Clerens and Steven Le Feunteun},
  title = {In-silico Predictions of Pepsin-released Peptides},
  booktitle = {Proceedings of the International conference on Food Digestion 2017},
  abstract = {Pepsin is the first protease encountered within the digestive tract. Unlike other digestive proteases, its specificity is low. It is therefore particularly difficult to determine a priori which peptides will be released during gastric digestion. Detailed information about food protein truncation during digestion is however critical to understanding and optimizing the availability of bioactives, or limiting allergen release. In this study, a stochastic model which tries to reproduce the dynamics of protein hydrolysis by pepsin is presented. The model is based on pepsin cleavage frequency tables taken from the literature, and makes use of Monte-Carlo in silico simulations to quantitatively predict peptides that are likely to be produced by pepsin during the course of the reaction. The proposed model, which requires the expected hydrolysis kinetics and the amino-acid sequence of the studied protein to run, was applied to bovine lactoferrin. Model predictions were then compared with 89 peptides experimentally observed with a peptidomic approach using isobaric labelling during 2h gastric digestion experiments (Grosvenor et al., Food and Function, 2014). The model was found to reproduce many real-world features of the case study, such as the relative peptide abundance summary maps along the protein sequence (peptide patterns) or peptide size distribution. It even appeared that 50\% of experimentally observed peptides (45/89) fall within the 164 most abundant predicted peptides (over a total of \~ 1500 predicted peptides). These first results illustrate that in silico modelling of pepsin hydrolysis is a promising approach to determine which peptides are likely to be released during gastric digestion of foods.}
}

@incollection{bucur2017multiobjective,
  doi = {10.1007/978-3-319-55849-3_15},
  url = {https://doi.org/10.1007/978-3-319-55849-3_15},
  year = {2017},
  publisher = {Springer International Publishing},
  pages = {221--233},
  author = {Doina Bucur and Giovanni Iacca and Andrea Marcelli and Giovanni Squillero and Alberto Tonda},
  title = {Multi-objective Evolutionary Algorithms for Influence Maximization in Social Networks},
  booktitle = {Applications of Evolutionary Computation},
  abstract = {As the pervasiveness of social networks increases, new NPhard related problems become interesting for the optimization community. The objective of influence maximization is to contact the largest
possible number of nodes in a network, starting from a small set of seed
nodes, and assuming a model for information propagation. This problem
is of utmost practical importance for applications ranging from social
studies to marketing. The influence maximization problem is typically
formulated assuming that the number of the seed nodes is a parameter.
Differently, in this paper, we choose to formulate it in a multi-objective
fashion, considering the minimization of the number of seed nodes among
the goals, and we tackle it with an evolutionary approach. As a result,
we are able to identify sets of seed nodes of different size that spread
influence the best, providing factual data to trade-off costs with quality
of the result. The methodology is tested on two real-world case studies,
using two different influence propagation models, and compared against
state-of-the-art heuristic algorithms. The results show that the proposed
approach is almost always able to outperform the heuristics.}
}

@inproceedings{chabin2017interactive,
  doi = {10.1145/3067695.3075992},
  url = {https://doi.org/10.1145/3067695.3075992},
  year = {2017},
  month = jul,
  publisher = {{ACM}},
  author = {Thomas Chabin and Marc Barnab{\'{e}} and Nadia Boukhelifa and Fernanda Fonseca and Alberto Tonda and H{\'{e}}l{\`{e}}ne Velly and Nathalie M\'{e}jean-Perrot and Evelyne Lutton},
  title = {Interactive evolutionary modelling of living complex food systems},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  abstract = {Modelling the production and stabilisation process of lactic acid starters has several practical applications, ranging from assessing the efficacy of new industrial methods, to proposing alternative sustainable systems of food production. In order to reach this objective, however, it is necessary to overcome several obstacles, tied to the complex nature and interactions of the target processes. In this paper, we present a novel complex system modelling approach, exploiting both stand-alone evolutionary search and visual interaction with the user. The presented framework is then tested on a real-world case study, for which it shows promising results.}
}

@inproceedings{tonda2017foodmc,
  year = {2017},
  author = {Alberto Tonda},
  title = {FoodMC: a COST Action to Promote Modeling in Food Science and Industry},
  booktitle = {Proceedings of the IOBC conference on Integrated Protection of Stored Products},
  abstract = {Methodologies and tools from Maths and Computer Science (MCS) are emerging as key contributors to modernization and optimization of processes in various disciplines: the agri-food sector, however, is not a traditional domain of application for MCS, and at the moment there is no community organized around solving the issues of this field. The COST Action FoodMC brings together scientists and practitioners from MCS and agri-food domains, stimulating the emergence of new research, and structuring a new community to coordinate further investigation efforts. Exploiting approaches originating at different subfields of MCS, from applied mathematical models to knowledge engineering, this COST Action will cover two main topics: understanding and controlling agri-food processes; and eco-design of agri-food products. During its first year of existence, COST Action FoodMC helped fund several international collaborations between European researchers, fostered the drafting of survey papers on food modelling, organized meetings for discussion, and co-funded a training school.}
}

@incollection{chabin2018lideogram,
  doi = {10.1007/978-3-319-78133-4_14},
  url = {https://doi.org/10.1007/978-3-319-78133-4_14},
  year = {2018},
  publisher = {Springer International Publishing},
  pages = {189--201},
  author = {Thomas Chabin and Marc Barnab{\'{e}} and Nadia Boukhelifa and Fernanda Fonseca and Alberto Tonda and H{\'{e}}l{\`{e}}ne Velly and Benjamin Lemaitre and Nathalie Perrot and Evelyne Lutton},
  title = {{LIDeOGraM}: An Interactive Evolutionary Modelling Tool},
  booktitle = {Lecture Notes in Computer Science},
  abstract = {Building complex models from available data is a challenge in many domains, and in particular in food science. Numerical data are often not enough structured, or simply not enough to elucidate complex structures : human choices have thus a major impact at various levels (data and model structuration, choice of representative scales, parameter ranges, uncertainty assessment and management, expert knowledge). LIDeOGraM is an interactive modelling framework adapted to cases where numerical data and expert knowledge have to be combined for building an ecient model. Exploiting both stand-alone evolutionary search and visual interaction with the user, the proposed methodology aims at obtaining an accurate global model for the system, balancing expert knowledge with information automatically extracted from available data. The presented framework is tested on a real-world case study from food science : the production and stabilisation of lactic acid bacteria, which has several important practical applications, ranging from assessing the ecacy of new industrial methods, to proposing alternative sustainable systems of food production.}
}

@incollection{bucur2018improving,
  doi = {10.1007/978-3-319-77538-8_9},
  url = {https://doi.org/10.1007/978-3-319-77538-8_9},
  year = {2018},
  publisher = {Springer International Publishing},
  pages = {117--124},
  author = {Doina Bucur and Giovanni Iacca and Andrea Marcelli and Giovanni Squillero and Alberto Tonda},
  title = {Improving Multi-objective Evolutionary Influence Maximization in Social Networks},
  booktitle = {Applications of Evolutionary Computation},
  abstract = {In the context of social networks, maximizing influence means contacting the largest possible number of nodes starting from a set of seed nodes, and assuming a model for influence propagation. The real-world applications of influence maximization are of uttermost importance, and range from social studies to marketing campaigns. Building on a previous work on multi-objective evolutionary influence maximization, we propose improvements that not only speed up the optimization process considerably, but also deliver higher-quality results. State-of-the-art heuristics are run for different sizes of the seed sets, and the results are then used to initialize the population of a multi-objective evolutionary algorithm. The proposed approach is tested on three publicly available real-world networks, where we show that the evolutionary algorithm is able to improve upon the solutions found by the heuristics, while also converging faster than an evolutionary algorithm started from scratch.}
}

@inproceedings{tonda2018alongshorttermmemory,
  year = {2018},
  author = {Tonda, Alberto and M\'{e}jean-Perrot, Nathalie},
  title = {A Long-Short-Term Memory Network Model for Biscuit Baking},
  booktitle = {Proceedings of FoodSIM 2018, 10th bi-annual International Conference on Modelling and Simulation in Food Engineering},
  isbn = {978-9492859-01-3},
  abstract = {Long-Short-Term Memory (LSTM) networks are a relatively recent addition to the field of Artificial Neural Networks (ANNs). LSTM networks are specifically tailored for machine learning of time series, where the outputs of a system are not just a function of their inputs, but also of a internal state. The state itself can be seen as dependent on the historical series of all inputs seen by the system up to that point in time. In this paper, we present an application of LSTM networks to the modeling of biscuit baking. Starting from 16 real-world time series of biscuit baking, gathered by the United Biscuits company under different conditions, we show how the proposed LSTM network can correctly predict unseen values. Remarkably, the network is also able to reproduce a dynamic behavior up to variations that might be overlooked as noise.}
}

@inproceedings{mejean2018human,
  year = {2018},
  author = {M\'{e}jean-Perrot, Nathalie and Boukhelifa, Nadia and Tonda, Alberto and Chabin, Thomas and Barnab\'{e}, Marc and Swennen, Dominique and Roche, Alice and Thomas-Danguin, Thomas and Lutton, Evelyne},
  title = {Human in the Loop for Modelling Food and Biological Systems: a Novel Perspective coupling Artificial Intelligence and Life Science},
  booktitle = {Proceedings of FoodSIM 2018, 10th bi-annual International Conference on Modelling and Simulation in Food Engineering},
  isbn = {978-9492859-01-3},
  abstract = {Since centuries, agriculture, food and biological systems are strongly linked to human expertise, albeit such knowledge has been capitalized and shared often at a local level, only. Since the beginning of the last century, swept away by productivism, modern agriculture and food production have put cumulated human knowledge aside. Facing new challenges like sustainability in a changing context, holistic approaches cannot be managed ``manually'' ab initio and there is a clear need for computing decision-support tools to tackle these new issues. Moreover, new approaches should be built centred on humans and for humans. The heart of our purpose is to shift the focus again on human and local expertise, guided by powerful computing interactive systems.}
}

@inproceedings{chabin2018asemiautomatic,
  year = {2018},
  author = {Chabin, Thomas and Barnab\'{e}, Marc and Tonda, Alberto and Boukhelifa, Nadia and Fonseca, Fernanda and Dugat-Bony, Eric and Velly, H{\'{e}}l{\`{e}}ne and Lutton, Evelyne and M\'{e}jean-Perrot, Nathalie},
  title = {A Semi-automatic Modeling Approach for the Production and Freeze-drying of Lactic Acid Bacteria},
  booktitle = {Proceedings of FoodSIM 2018, 10th bi-annual International Conference on Modelling and Simulation in Food Engineering},
  isbn = {978-9492859-01-3},
  abstract = {The production system of freeze-dried lactic acid bacteria involves several processes, but its impact on bacteria resistance is still not well understood. This system can be defined as a complex one since it depends on multiple scales: the Genomic, the Cellular and the Population scale. The scarcity of data available for building models leads us to propose an approach that makes use of expert knowledge. In this paper we present a semiautomatic modelling tool, LIDEOGRAM and discuss how it contributes to insight formulation and rapid hypothesis testing. New results show that LIDEOGRAM is able to produce more robust modelling hypotheses when experts can interact and revisit the genomic data preprocessing.}
}

@inproceedings{bucur2018evaluating,
  doi = {10.1145/3205651.3208238},
  url = {https://doi.org/10.1145/3205651.3208238},
  year = {2018},
  month = jul,
  publisher = {{ACM}},
  author = {Doina Bucur and Giovanni Iacca and Andrea Marcelli and Giovanni Squillero and Alberto Tonda},
  title = {Evaluating surrogate models for multi-objective influence maximization in social networks},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  abstract = {One of the most relevant problems in social networks is influence maximization, that is the problem of finding the set of the most influential nodes in a network, for a given influence propagation model. As the problem is NP-hard, recent works have attempted to solve it by means of computational intelligence approaches, for instance Evolutionary Algorithms. However, most of these methods are of limited applicability for real-world large-scale networks, for two reasons: on the one hand, they require a large number of candidate solution evaluations to converge; on the other hand, each evaluation is computationally expensive in that it needs a considerable number of Monte Carlo simulations to obtain reliable values. In this work, we consider a possible solution to such limitations, by evaluating a surrogate-assisted Multi-Objective Evolutionary Algorithm that uses an approximate model of influence propagation (instead of Monte Carlo simulations) to find the minimum-sized set of most influential nodes. Experiments carried out on two social networks datasets suggest that approximate models should be carefully considered before using them in influence maximization approaches, as the errors induced by these models are in some cases too big to benefit the algorithmic performance.}
}

@inproceedings{tonda2018building,
  year = {2018},
  author = {Alberto Tonda},
  title = {Building a Multidisciplinary Community on Mathematics and Computer Science for the Food Industry: The Case of FoodMC},
  booktitle = {Book of Abstracts of the 5th International {ISEKI}\_{F}ood Conference},
  isbn = {978-3-900932-57-2},
  abstract = {Presentation of COST Action CA15118 FoodMC.}
}

@incollection{barbiero2019understanding,
  doi = {10.1007/978-981-13-8950-4_26},
  url = {https://doi.org/10.1007/978-981-13-8950-4_26},
  year = {2019},
  month = sep,
  publisher = {Springer Singapore},
  pages = {281--290},
  author = {Pietro Barbiero and Andrea Bertotti and Gabriele Ciravegna and Giansalvo Cirrincione and Elio Piccolo and Alberto Tonda},
  title = {Understanding Cancer Phenomenon at Gene Expression Level by using a Shallow Neural Network Chain},
  booktitle = {Neural Approaches to Dynamics of Signal Exchanges},
  abstract = {Exploiting the availability of the largest collection of patient-derived xenografts from metastatic colorectal cancer annotated for a response to therapies, this manuscript aims to characterize the biological phenomenon from a mathematical point of view. In particular, we design an experiment in order to investigate how genes interact with each other. By using a shallow neural network model, we find reduced feature subspaces where the resistance phenomenon may be much easier to understand and analyze.}
}

@incollection{barbiero2019fundamental,
  doi = {10.1007/978-3-030-16692-2_37},
  url = {https://doi.org/10.1007/978-3-030-16692-2_37},
  year = {2019},
  publisher = {Springer International Publishing},
  pages = {550--564},
  author = {Pietro Barbiero and Alberto Tonda},
  title = {Fundamental Flowers: Evolutionary Discovery of Coresets for Classification},
  booktitle = {Applications of Evolutionary Computation},
  abstract = {In an optimization problem, a coreset can be defined as a subset of the input points, such that a good approximation to the optimization problem can be obtained by solving it directly on the coreset, instead of using the whole original input. In machine learning, coresets are exploited for applications ranging from speeding up training time, to helping humans understand the fundamental properties of a class, by considering only a few meaningful samples. The problem of discovering coresets, starting from a dataset and an application, can be defined as identifying the minimal amount of samples that do not significantly lower performance with respect to the performance on the whole dataset. Specialized literature offers several approaches to finding coresets, but such algorithms often disregard the application, or explicitly ask the user for the desired number of points. Starting from the consideration that finding coresets is an intuitively multi-objective problem, as minimizing the number of points goes against maintaining the original performance, in this paper we propose a multi-objective evolutionary approach to identifying coresets for classification. The proposed approach is tested on classical machine learning classification benchmarks, using 6 state-of-the-art classifiers, comparing against 7 algorithms for coreset discovery. Results show that not only the proposed approach is able to find coresets representing different compromises between compactness and performance, but that different coresets are identified for different classifiers, reinforcing the assumption that coresets might be closely linked to the specific application.}
}

@incollection{barbiero2020generating,
  doi = {10.1007/978-3-030-38227-8_6},
  url = {https://doi.org/10.1007/978-3-030-38227-8_6},
  year = {2020},
  publisher = {Springer International Publishing},
  pages = {45--52},
  author = {Pietro Barbiero and Gabriele Ciravegna and Giansalvo Cirrincione and Alberto Tonda and Giovanni Squillero},
  title = {Generating Neural Archetypes to Instruct Fast and Interpretable Decisions},
  booktitle = {Advances in Intelligent Systems and Computing},
  abstract = {In the field of artificial intelligence, agents learn how to take decisions by fitting their parameters on a set of samples called training set. Similarly, a core set is a subset of the training samples such that, if an agent exploits this set to fit its parameters instead of the whole training set, then the quality of the inferences does not change significantly. Relaxing the constraint that restricts the search for core sets to the available data, neural networks may be used to generate virtual samples, called archetype set, containing the same kind of information. This work illustrates the features of GH-ARCH, a recently proposed self-organizing hierarchical neural network for archetype discovery. Experiments show how the use of archetypes allows both ML agents to make fast and accurate predictions and human experts to make sense of such decisions by analyzing few important samples.}
}

@incollection{barbiero2020making,
  doi = {10.1007/978-3-030-38227-8_19},
  url = {https://doi.org/10.1007/978-3-030-38227-8_19},
  year = {2020},
  publisher = {Springer International Publishing},
  pages = {162--170},
  author = {Pietro Barbiero and Alberto Tonda},
  title = {Making Sense of Economics Datasets with Evolutionary Coresets},
  booktitle = {Advances in Intelligent Systems and Computing},
  abstract = {Machine learning agents learn to take decisions extracting information from training data. When similar inferences can be obtained using a small subset of the same training set of samples, the subset is called coreset. Coresets discovery is an active line of research as it may be used to reduce the training speed as well as to allow human experts to gain a better understanding of both the phenomenon and the decisions, by reducing the number of samples to be examined. For classification problems, the state-of-the-art in coreset discovery is EvoCore, a multi-objective evolutionary algorithm. In this work EvoCore is exploited both on synthetic and on real data sets, showing how coresets may be useful in explaining decisions taken by machine learning classifiers.}
}

@inproceedings{barbiero2019beyond,
  doi = {10.1145/3319619.3326789},
  url = {https://doi.org/10.1145/3319619.3326789},
  year = {2019},
  month = jul,
  publisher = {{ACM}},
  author = {Pietro Barbiero and Giovanni Squillero and Alberto Tonda},
  title = {Beyond coreset discovery},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  abstract = {In machine learning a coreset is defined as a subset of the training set using which an algorithm obtains performances similar to what it would deliver if trained over the whole original data. Advantages of coresets include improving training speed and easing human understanding. Coreset discovery is an open line of research as limiting the training might also impair the quality of the result. Differently, virtual points, here called archetypes, might be far more informative for a machine learning algorithm. Starting from this intuition, a novel evolutionary approach to archetype set discovery is presented: starting from a population seeded with candidate coresets, a multi-objective evolutionary algorithm is set to modify them and eventually create archetype sets, to minimize both number of points in the set and classification error. Experimental results on popular benchmarks show that the proposed approach is able to deliver results that allow a classifier to obtain lower error and better ability of generalizing on unseen data than state-of-the-art coreset discovery techniques.}
}

@inproceedings{barbiero2019evolutionary,
  doi = {10.1145/3319619.3326846},
  url = {https://doi.org/10.1145/3319619.3326846},
  year = {2019},
  month = jul,
  publisher = {{ACM}},
  author = {Pietro Barbiero and Giovanni Squillero and Alberto Tonda},
  title = {Evolutionary discovery of coresets for classification},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  abstract = {When a machine learning algorithm is able to obtain the same performance given a complete training set, and a small subset of samples from the same training set, the subset is termed coreset. As using a coreset improves training speed and allows human experts to gain a better understanding of the data, by reducing the number of samples to be examined, coreset discovery is an active line of research. Often in literature the problem of coreset discovery is framed as i. single-objective, attempting to find the candidate coreset that best represents the training set, and ii. independent from the machine learning algorithm used. In this work, an approach to evolutionary coreset discovery is presented. Building on preliminary results, the proposed approach uses a multi-objective evolutionary algorithm to find compromises between two conflicting objectives, i. minimizing the number of samples in a candidate coreset, and ii. maximizing the accuracy of a target classifier, trained with the coreset, on the whole original training set. Experimental results on popular classification benchmarks show that the proposed approach is able to identify candidate coresets with better accuracy and generality than state-of-the-art coreset discovery algorithms found in literature.}
}

@incollection{ciravegna2020discovering,
  doi = {10.1007/978-981-15-5093-5_24},
  url = {https://doi.org/10.1007/978-981-15-5093-5_24},
  year = {2020},
  month = jul,
  publisher = {Springer Singapore},
  pages = {255--267},
  author = {Gabriele Ciravegna and Pietro Barbiero and Giansalvo Cirrincione and Giovanni Squillero and Alberto Tonda},
  title = {Discovering Hierarchical Neural Archetype Sets},
  booktitle = {Progresses in Artificial Intelligence and Neural Systems},
  abstract = {In the field of machine learning, coresets are defined as subsets of the training set that can be used to obtain a good approximation of the behavior that a given algorithm would have on the whole training set. Advantages of using coresets instead of the training set include improving training speed and allowing for a better human understanding of the dataset. Not surprisingly, coreset discovery is an active research line, with several notable contributions in literature. Nevertheless, restricting the search for representative samples to the available data points might impair the final result. In this work, neural networks are used to create sets of virtual data points, named archetypes, with the objective to represent the information contained in a training set, in the same way a coreset does. Starting from a given training set, a hierarchical clustering neural network is trained and the weight vectors of the leaves are used as archetypes on which the classifiers are trained. Experimental results on several benchmarks show that the proposed approach is competitive with traditional coreset discovery techniques, delivering results with higher accuracy, and showing a greater ability to generalize to unseen test data.}
}

@incollection{giovannitti2020virtual,
  doi = {10.1007/978-3-030-37838-7_17},
  url = {https://doi.org/10.1007/978-3-030-37838-7_17},
  year = {2020},
  publisher = {Springer International Publishing},
  pages = {189--200},
  author = {Eliana Giovannitti and Giovanni Squillero and Alberto Tonda},
  title = {Virtual Measurement of the Backlash Gap in Industrial Manipulators},
  booktitle = {Communications in Computer and Information Science},
  abstract = {Industrial manipulators are robots used to replace humans in dangerous or repetitive tasks. Also, these devices are often used for applications where high precision and accuracy is required. The increase of backlash caused by wear, that is, the increase of the amount by which teeth space exceeds the thickness of gear teeth, might be a significant problem, that could lead to impaired performances or even abrupt failures. However, maintenance is difficult to schedule because backlash cannot be directly measured and its effects only appear in closed loops. This paper proposes a novel technique, based on an Evolutionary Algorithm, to estimate the increase of backlash in a robot joint transmission. The peculiarity of this method is that it only requires measurements from the motor encoder. Experimental evaluation on a real-world test case demonstrates the effectiveness of the approach.}
}

@incollection{barbiero2020anovel,
  doi = {10.1007/978-3-030-45715-0_6},
  url = {https://doi.org/10.1007/978-3-030-45715-0_6},
  year = {2020},
  publisher = {Springer International Publishing},
  pages = {68--81},
  author = {Pietro Barbiero and Evelyne Lutton and Giovanni Squillero and Alberto Tonda},
  title = {A Novel Outlook on Feature Selection as a Multi-objective Problem},
  booktitle = {Lecture Notes in Computer Science},
  abstract = {Feature selection is the process of choosing, or removing, features to obtain the most informative feature subset of minimal size. Such subsets are used to improve performance of machine learning algorithms and enable human understanding of the results. Approaches to feature selection in literature exploit several optimization algorithms. Multi-objective methods also have been proposed, minimizing at the same time the number of features and the error. While most approaches assess error resorting to the average of a stochastic K-fold cross-validation, comparing averages might be misleading. In this paper, we show how feature subsets with different average error might in fact be non-separable when compared using a statistical test. Following this idea, clusters of non-separable optimal feature subsets are identified. The performance in feature selection can thus be evaluated by verifying how many of these optimal feature subsets an algorithm is able to identify. We thus propose a multi-objective optimization approach to feature selection, EvoFS, with the objectives to i. minimize feature subset size, ii. minimize test error on a 10-fold cross-validation using a specific classifier, iii. maximize the analysis of variance value of the lowest-performing feature in the set. Experiments on classification datasets whose feature subsets can be exhaustively evaluated show that our approach is able to always find the best feature subsets. Further experiments on a high-dimensional classification dataset, that cannot be exhaustively analyzed, show that our approach is able to find more optimal feature subsets than state-of-the-art feature selection algorithms.}
}

@inproceedings{djekic2020environmental,
  year = {2020},
  author = {Ilija Djekic and Jan Van Impe and Alberto Tonda},
  title = {Environmental Modelling in the Food Supply Chain - Future Perspectives},
  booktitle = {Proceedings of FoodSIM 2020, 11th bi-annual International Conference on Modelling and Simulation in Food Engineering},
  abstract = {Interaction of food systems and the environment has been in research focus for many years. In order to explain this interaction, scholars have developed and use various approaches in modelling and understanding this phenomenon. This paper gives an overview of three main perspectives in analyzing this issue and provides some future perspectives associated with sustainable development goals developed by the UN.}
}

@inproceedings{lopezrincon2020batch,
  doi = {10.1145/3377929.3389947},
  url = {https://doi.org/10.1145/3377929.3389947},
  year = {2020},
  month = jul,
  publisher = {{ACM}},
  author = {Alejandro Lopez Rincon and Aletta D. Kraneveld and Alberto Tonda},
  title = {Batch correction of genomic data in chronic fatigue syndrome using {CMA}-{ES}},
  booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
  abstract = {Modern genomic sequencing machines can measure thousands of probes from different specimens. Nevertheless, theoretically comparable datasets can show considerably distinguishable properties, depending on both platform and specimen, a phenomenon known as batch effect. Batch correction is the technique aiming at removing this effect from the data. A possible approach to batch correction is to find a transformation function between different datasets, but optimizing the weights of such a function is not trivial: As there is no explicit gradient to follow, traditional optimization techniques would fail. In this work, we propose to use a state-of-the-art evolutionary algorithm, Covariance Matrix Adaptation Evolution Strategy, to optimize the weights of a transformation function for batch correction. The fitness function is driven by the classification accuracy of an ensemble of algorithms on the transformed data. The case study selected to test the proposed approach is mRNA gene expression data of Chronic Fatigue Syndrome, a disease for which there is currently no established diagnostic test. The transformation function obtained from three datasets, produced from different specimens, remarkably improves the performance of classifiers on the task of diagnosing Chronic Fatigue. The presented results are an important steppingstone towards a reliable diagnostic test for this syndrome.}
}

@inproceedings{squillero2020evolutionary,
  doi = {10.1145/3377929.3389863},
  url = {https://doi.org/10.1145/3377929.3389863},
  year = {2020},
  month = jul,
  publisher = {{ACM}},
  author = {Giovanni Squillero and Alberto Tonda},
  title = {Evolutionary algorithms and machine learning},
  booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
  abstract = {Tutorial on evolutionary algorithms and machine learning. Slides are available from the DOI link.}
}

@incollection{doerr2016tutorials,
  doi = {10.1007/978-3-319-45823-6_95},
  url = {https://doi.org/10.1007/978-3-319-45823-6_95},
  year = {2016},
  publisher = {Springer International Publishing},
  pages = {1012--1022},
  author = {Carola Doerr and Nicolas Bredeche and Enrique Alba and Thomas Bartz-Beielstein and Dimo Brockhoff and Benjamin Doerr and Gusz Eiben and Michael G. Epitropakis and Carlos M. Fonseca and Andreia Guerreiro and Evert Haasdijk and Jacqueline Heinerman and Julien Hubert and Per Kristian Lehre and Luigi Malag{\`{o}} and J. J. Merelo and Julian Miller and Boris Naujoks and Pietro Oliveto and Stjepan Picek and Nelishia Pillay and Mike Preuss and Patricia Ryser-Welch and Giovanni Squillero and J\"{o}rg Stork and Dirk Sudholt and Alberto Tonda and Darrell Whitley and Martin Zaefferer},
  title = {Tutorials at {PPSN} 2016},
  booktitle = {Parallel Problem Solving from Nature {\textendash} {PPSN} {XIV}},
  abstract = {PPSN 2016 hosts a total number of 16 tutorials covering a broad range of current research in evolutionary computation. The tutorials range from introductory to advanced and specialized but can all be attended without prior requirements. All PPSN attendees are cordially invited to take this opportunity to learn about ongoing research activities in our field!}
}

@incollection{merelo2016theuncertainty,
  doi = {10.1007/978-3-662-53525-7_3},
  url = {https://doi.org/10.1007/978-3-662-53525-7_3},
  year = {2016},
  publisher = {Springer Berlin Heidelberg},
  pages = {40--60},
  author = {Juan J. Merelo and Federico Liberatore and Antonio Fern{\'{a}}ndez Ares and Rub{\'{e}}n Garc{\'{\i}}a and Zeineb Chelly and Carlos Cotta and Nuria Rico and Antonio M. Mora and Pablo Garc{\'{\i}}a-S{\'{a}}nchez and Alberto Tonda and Paloma de las Cuevas and Pedro A. Castillo},
  title = {The Uncertainty Quandary: A Study in the Context of the Evolutionary Optimization in Games and Other Uncertain Environments},
  booktitle = {Transactions on Computational Collective Intelligence {XXIV}},
  abstract = {In many optimization processes, the fitness or the considered measure of goodness for the candidate solutions presents uncertainty, that is, it yields different values when repeatedly measured, due to the nature of the evaluation process or the solution itself. This happens quite often in the context of computational intelligence in games, when either bots behave stochastically, or the target game possesses intrinsic random elements, but it shows up also in other problems as long as there is some random component. Thus, it is important to examine the statistical behavior of repeated measurements of performance and, more specifically, the statistical distribution that better fits them. This work analyzes four different problems related to computational intelligence in videogames, where Evolutionary Computation methods have been applied, and the evaluation of each individual is performed by playing the game, and compare them to other problem, neural network optimization, where performance is also a statistical variable. In order to find possible patterns in the statistical behavior of the variables, we track the main features of its distributions, skewness and kurtosis. Contrary to the usual assumption in this kind of problems, we prove that, in general, the values of two features imply that fitness values do not follow a normal distribution; they do present a certain common behavior that changes as evolution proceeds, getting in some cases closer to the standard distribution and in others drifting apart from it. A clear behavior in this case cannot be concluded, other than the fact that the statistical distribution that fitness variables follow is affected by selection in different directions, that parameters vary in a single generation across them, and that, in general, this kind of behavior will have to be taken into account to adequately address uncertainty in fitness in evolutionary algorithms.}
}

@incollection{lopezrincon2021modelling,
  doi = {10.1007/978-3-030-72699-7_23},
  url = {https://doi.org/10.1007/978-3-030-72699-7_23},
  year = {2021},
  publisher = {Springer International Publishing},
  pages = {359--372},
  author = {Alejandro Lopez-Rincon and Daphne S. Roozendaal and Hilde M. Spierenburg and Asta L. Holm and Renee Metcalf and Paula Perez-Pardo and Aletta D. Kraneveld and Alberto Tonda},
  title = {Modelling Asthma Patients' Responsiveness to Treatment Using Feature Selection and Evolutionary Computation},
  booktitle = {Applications of Evolutionary Computation},
  abstract = {For several medical treatments, it is possible to observe transcriptional variations in gene expressions between responders and non-responders. Modelling the correlation between such variations and the patient’s response to drugs as a system of Ordinary Differential Equations could be invaluable to improve the efficacy of treatments and would represent an important step towards personalized medicine. Two main obstacles lie on this path: (i) the number of genes is too large to straightforwardly analyze their interactions; (ii) defining the correct parameters for the mathematical models of gene interaction is a complex optimization problem, even when a limited number of genes is involved. In this paper, we propose a novel approach to creating mathematical models able to explain patients’ response to treatment from transcriptional variations. The approach is based on: (i) a feature selection algorithm, set to identify a minimal set of gene expressions that are highly correlated with treatment outcome, (ii) a state-of-the-art evolutionary optimizer, Covariance Matrix Adaptation Evolution Strategy, applied to finding the parameters of the mathematical model characterizing the relationship between gene expressions and patient responsiveness. The proposed methodology is tested on real-world data describing responsiveness of asthma patients to Omalizumab, a humanized monoclonal antibody that binds to immunoglobulin E. In this case study, the presented approach is shown able to identify 5 genes (out of 28,402) that are transcriptionally relevant to predict treatment outcomes, and to deliver a compact mathematical model that is able to explain the interaction between the different genes involved.}
}

@inproceedings{giovannitti2021exploiting,
  doi = {10.1109/cec45853.2021.9504962},
  url = {https://doi.org/10.1109/cec45853.2021.9504962},
  year = {2021},
  month = jun,
  publisher = {{IEEE}},
  author = {Eliana Giovannitti and Sayyidshahab Nabavi and Giovanni Squillero and Alberto Tonda},
  title = {Exploiting Artificial Swarms for the Virtual Measurement of Backlash in Industrial Robots},
  booktitle = {2021 {IEEE} Congress on Evolutionary Computation ({CEC})},
  abstract = {The backlash is a lost motion in a mechanism created by gaps between its parts. It causes vibrations that increase over time and negatively affect accuracy and performance. The quickest and most precise way to measure the backlash is to use specific sensors, that have to be added to the standard equipment of the robot. However, this solution is little used in practice because raises the manufacturing costs. An alternative solution can be to exploit a virtual sensor, i.e., the information about phenomena that are not directly measured is reconstructed by signals from sensors used for other measurements.This work evaluates the use of bio-inspired swarm algorithms as the processing core of a virtual sensor for the backlash of a robotic joint. Swarm-based approaches, with their relatively modest occupation of memory and low computational load, could be ideal candidates to solve the problem. In this paper, we exploit four state-of-the-art swarm-based optimization algorithms: the Dragonfly Algorithm, the Ant Lion Optimizer, the Grasshopper Optimization Algorithm, and the Grey Wolf Optimizer. The four candidate algorithms are compared on 20 different datasets covering a range of backlash values that reflect an industrial case scenario. Numerical results indicate that, unfortunately, none of the algorithms considered provides satisfactory solutions for the problem analyzed. Therefore, even if promising, these algorithms cannot represent the final choice for the problem of interest.}
}

@inproceedings{lopezrincon2021design,
  doi = {10.1145/3449639.3459359},
  url = {https://doi.org/10.1145/3449639.3459359},
  year = {2021},
  month = jun,
  publisher = {{ACM}},
  author = {Alejandro Lopez Rincon and Carmina A. Perez Romero and Lucero Mendoza Maldonado and Eric Claassen and Johan Garssen and Aletta D. Kraneveld and Alberto Tonda},
  title = {Design of specific primer sets for {SARS}-{CoV}-2 variants using evolutionary algorithms},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  abstract = {Primer sets are short DNA sequences of 18-22 base pairs, that can be used to verify the presence of a virus, and designed to attach to a specific part of a viral DNA. Designing a primer set requires choosing a region of DNA, avoiding the possibility of hybridization to a similar sequence, as well as considering its GC content and $T_m$ (melting temperature). Coronaviruses, such as SARS-CoV-2, have a considerably large genome (around 30 thousand nucleotides) when compared to other viruses. With the rapid rise and spread of SARS-CoV-2 variants, it has become a priority to breach our lack of specific primers available for diagnosis of this new variants. Here, we propose an evolutionary-based approach to primer design, able to rapidly deliver a high-quality primer set for a target sequence of the virus variant. Starting from viral sequences collected from open repositories, the proposed approach is proven able to uncover a specific primer set for the B.1.1.7 SARS-CoV-2 variant. Only recently identified, B.1.1.7 is already considered potentially dangerous, as it presents a considerably higher transmissibility when compared to other variants.}
}


@inproceedings{mouhrim2022towards,
author = {Mouhrim, Nisrine and Smetana, Sergiy and Bhatia, Anita and Mathys, Alexander and Green, Ashley and Peguero, Daniela and Tonda, Alberto},
title = {Towards Multi-Objective Optimization of Sustainable Insect Production Chains},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3528898},
doi = {10.1145/3520304.3528898},
abstract = {Due to the relatively recent history of industrial insect farming, there are still no extensive studies on the optimization of insect production chains. In this context, the present work aims to be a first step in filling this gap. A tentative set of mathematical models is proposed to take into account three different, conflicting objectives: maximizing economic viability, minimizing environmental impacts, and maximizing societal benefits. The state-of-the-art multi-objective algorithm NSGA-II is used to obtain an approximate Pareto fronts of solutions, that are later analyzed to identify suitable trade-offs. While preliminary, the results are encouraging enough for the computer-assisted design and development of sustainable insect production chains. Future works will take into account more extensive models, able to simulate scenarios in different European countries, and include parts of the chain such as transportation of goods to and from the production facilities.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {352–-355},
numpages = {4},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{mouhrim2022evolutionary,
author = {Mouhrim, Nisrine and Tonda, Alberto and Rodr\'{\i}guez-Guerra, Itzel and Kraneveld, Aletta D. and Rincon, Alejandro Lopez},
title = {An Evolutionary Approach to the Discretization of Gene Expression Profiles to Predict the Severity of COVID-19},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3529001},
doi = {10.1145/3520304.3529001},
abstract = {In this work, we propose to use a state-of-the-art evolutionary algorithm to set the discretization thresholds for gene expression profiles, using feedback from a classifier in order to maximize the accuracy of the predictions based on the discretized gene expression levels, while at the same time minimizing the number of different profiles obtained, to ease the understanding of the expert. The methodology is applied to a dataset containing COVID-19 patients that developed either mild or severe symptoms. The results show that the evolutionary approach performs better than a traditional discretization based on statistical analysis, and that it does preserve the sense-making necessary for practitioners to trust the results.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {731–-734},
numpages = {4},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@incollection{tonda2022intercontinental,
year = {2022},
month = apr,
publisher = {EUROSIS},
author = {Alberto Tonda and Christian Reynolds and Nisrine Mouhrim and Rallou Thomopoulos},
title = {An Intercontinental Machine Learning Analysis of Factors Explaining Consumer Awareness About Food Risk},
booktitle = {Proceedings of FoodSIM 2022, 12th bi-annual International Conference on Modelling and Simulation in Food Engineering},
abstract = {This paper investigates to what extent food safety is perceived as a concern at the household level in different countries. It aims to identify the factors that best explain food safety concern, among the various food-related questions asked through a survey. To do so, a machine learning approach is used. The results show that the most significant explanatory variables of safety concern are the estimates of carbon footprints and calories associated with food products and primarily with beef and chicken meat. These results tend to indicate that people who are most concerned about food safety are also those who are best aware of environmental and nutritional impacts of food.}
}

@incollection{barbiero2022predictable,
  doi = {10.1007/978-3-030-95467-3_29},
  url = {https://doi.org/10.1007/978-3-030-95467-3_29},
  year = {2022},
  publisher = {Springer International Publishing},
  pages = {399--412},
  author = {Pietro Barbiero and Giovanni Squillero and Alberto Tonda},
  title = {Predictable Features Elimination: An Unsupervised Approach to Feature Selection},
  booktitle = {Machine Learning,  Optimization,  and Data Science},
  abstract = {We propose an unsupervised, model-agnostic, wrapper method for feature selection. We assume that if a feature can be predicted using the others, it adds little information to the problem, and therefore could be removed without impairing the performance of whatever model will be eventually built. The proposed method iteratively identifies and removes predictable, or nearly-predictable, redundant features, allowing to trade-off complexity with expected quality. The approach do not rely on target labels nor values, and the model used to identify predictable features is not related to the final use of the feature set. Therefore, it can be used for supervised, unsupervised, or semi-supervised problems, or even as a safe, pre-processing step to improve the quality of the results of other feature selection techniques. Experimental results against state-of-the-art feature-selection algorithms show satisfying performance on several non-trivial benchmarks.}
}

%% 2023

@incollection{zhang2023mapelites,
    doi = {10.1007/978-3-031-29573-7_6},
    url = {https://doi.org/10.1007/978-3-031-29573-7_6},
    year = {2023},
    publisher = {Springer Nature Switzerland},
    pages = {84--100},
    author = {Hengzhe Zhang and Qi Chen and Alberto Tonda and Bing Xue and Wolfgang Banzhaf and Mengjie Zhang},
    title = {{MAP}-Elites with~Cosine-Similarity for~Evolutionary Ensemble Learning},
    booktitle = {Lecture Notes in Computer Science},
    abstract = {Evolutionary ensemble learning methods with Genetic Programming have achieved remarkable results on regression and classification tasks by employing quality-diversity optimization techniques like MAP-Elites and Neuro-MAP-Elites. The MAP-Elites algorithm uses dimensionality reduction methods, such as variational auto-encoders, to reduce the high-dimensional semantic space of genetic programming to a two-dimensional behavioral space. Then, it constructs a grid of high-quality and diverse models to form an ensemble model. In MAP-Elites, however, variational auto-encoders rely on Euclidean space topology, which is not effective at preserving high-quality individuals. To solve this problem, this paper proposes a principal component analysis method based on a cosine-kernel for dimensionality reduction. In order to deal with unbalanced distributions of good individuals, we propose a zero-cost reference points synthesizing method. Experimental results on 108 datasets show that combining principal component analysis using a cosine kernel with reference points significantly improves the performance of the MAP-Elites evolutionary ensemble learning algorithm.}
}

@InProceedings{das2023direct,
author="Das, Soumita
and Singha, Bijita
and Tonda, Alberto
and Biswas, Anupam",
editor="Shakya, Subarna
and Papakostas, George
and Kamel, Khaled A.",
title="Direct Comparative Analysis of Nature-Inspired Optimization Algorithms on Community Detection Problem in Social Networks",
booktitle="Mobile Computing and Sustainable Informatics",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="629--642",
abstract="Nature-inspired optimization Algorithms (NIOAs) are nowadays a popular choice for community detection in social networks. Community detection problem in social network is treated as an optimization problem, where the objective is to either maximize the connection within the community or minimize connections between the communities. To apply NIOAs, either of the two, or both objectives are explored. Since NIOAs mostly exploit randomness in their strategies, it is necessary to analyze their performance for specific applications. In this paper, NIOAs are analyzed for the community detection problem. A direct comparison approach is followed to perform the pairwise comparison of NIOAs. The performance is measured in terms of five scores designed based on the prasatul matrix and also with average isolability. Three widely used real-world social networks and four NIOAs are considered for analyzing the quality of communities generated by NIOAs.",
isbn="978-981-99-0835-6"
}

@InProceedings{10.1007/978-3-031-30229-9_45,
author="Rojas-Velazquez, David
and Tonda, Alberto
and Rodriguez-Guerra, Itzel
and Kraneveld, Aletta D.
and Lopez-Rincon, Alejandro",
editor="Correia, Jo{\~a}o
and Smith, Stephen
and Qaddoura, Raneem",
title="Multi-objective Evolutionary Discretization of Gene Expression Profiles: Application to COVID-19 Severity Prediction",
booktitle="Applications of Evolutionary Computation",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="703--717",
abstract="Machine learning models can use information from gene expressions in patients to efficiently predict the severity of symptoms for several diseases. Medical experts, however, still need to understand the reasoning behind the predictions before trusting them. In their day-to-day practice, physicians prefer using gene expression profiles, consisting of a discretized subset of all data from gene expressions: in these profiles, genes are typically reported as either over-expressed or under-expressed, using discretization thresholds computed on data from a healthy control group. A discretized profile allows medical experts to quickly categorize patients at a glance. Building on previous works related to the automatic discretization of patient profiles, we present a novel approach that frames the problem as a multi-objective optimization task: on the one hand, after discretization, the medical expert would prefer to have as few different profiles as possible, to be able to classify patients in an intuitive way; on the other hand, the loss of information has to be minimized. Loss of information can be estimated using the performance of a classifier trained on the discretized gene expression levels. We apply one common state-of-the-art evolutionary multi-objective algorithm, NSGA-II, to the discretization of a dataset of COVID-19 patients that developed either mild or severe symptoms. The results show not only that the solutions found by the approach dominate traditional discretization based on statistical analysis and are more generally valid than those obtained through single-objective optimization, but that the candidate Pareto-optimal solutions preserve the sense-making that practitioners find necessary to trust the results.",
isbn="978-3-031-30229-9"
}

@inproceedings{tonda2023towards,
author = {Tonda, Alberto and Alvarez, Isabelle and Martin, Sophie and Squillero, Giovanni and Lutton, Evelyne},
title = {Towards Evolutionary Control Laws for Viability Problems},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590415},
doi = {10.1145/3583131.3590415},
abstract = {The mathematical theory of viability, developed to formalize problems related to natural and social phenomena, investigates the evolution of dynamical systems under constraints. A main objective of this theory is to design control laws to keep systems inside viable domains. Control laws are traditionally defined as rules, based on the current position in the state space with respect to the boundaries of the viability kernel. However, finding these boundaries is a computationally expensive procedure, feasible only for trivial systems. We propose an approach based on Genetic Programming (GP) to discover control laws for viability problems in analytic form. Such laws could keep a system viable without the need of computing its viability kernel, facilitate communication with stakeholders, and improve explainability. A candidate set of control rules is encoded as GP trees describing equations. Evaluation is noisy, due to stochastic sampling: initial conditions are randomly drawn from the state space of the problem, and for each, a system of differential equations describing the system is solved, creating a trajectory. Candidate control laws are rewarded for keeping viable as many trajectories as possible, for as long as possible. The proposed approach is evaluated on established benchmarks for viability and delivers promising results.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1464–1472},
numpages = {9},
location = {Lisbon, Portugal},
series = {GECCO '23}
}


@InProceedings{barbiero2023interpretable,
  title = 	 {Interpretable Neural-Symbolic Concept Reasoning},
  author =       {Barbiero, Pietro and Ciravegna, Gabriele and Giannini, Francesco and Espinosa Zarlenga, Mateo and Magister, Lucie Charlotte and Tonda, Alberto and Lio, Pietro and Precioso, Frederic and Jamnik, Mateja and Marra, Giuseppe},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {1801--1825},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = Jul,
  dates = 	 {23--29 Jul},
  publisher =    {PMLR},
  abstract = 	 {Deep learning methods are highly accurate, yet their opaque decision process prevents them from earning full human trust. Concept-based models aim to address this issue by learning tasks based on a set of human-understandable concepts. However, state-of-the-art concept-based models rely on high-dimensional concept embedding representations which lack a clear semantic meaning, thus questioning the interpretability of their decision process. To overcome this limitation, we propose the Deep Concept Reasoner (DCR), the first interpretable concept-based model that builds upon concept embeddings. In DCR, neural networks do not make task predictions directly, but they build syntactic rule structures using concept embeddings. DCR then executes these rules on meaningful concept truth degrees to provide a final interpretable and semantically-consistent prediction in a differentiable manner. Our experiments show that DCR: (i) improves up to +25\% w.r.t. state-of-the-art interpretable concept-based models on challenging benchmarks (ii) discovers meaningful logic rules matching known ground truths even in the absence of concept supervision during training, and (iii), facilitates the generation of counterfactual examples providing the learnt rules as guidance.}
}

@inproceedings{barbiero2023interpretableNeSy,
  title = 	 {Interpretable Neural-Symbolic Concept Reasoning},
  author =       {Barbiero, Pietro and Ciravegna, Gabriele and Giannini, Francesco and Espinosa Zarlenga, Mateo and Magister, Lucie Charlotte and Tonda, Alberto and Lio, Pietro and Precioso, Frederic and Jamnik, Mateja and Marra, Giuseppe},
  booktitle = 	 {Proceedings of the Proceedings of the 17th International Workshop on Neural-Symbolic Learning and Reasoning},
  pages = 	 {422--423},
  year = 	 {2023},
  editor = 	 {d'Avila Garcez, Artur S. and Besold, Tarek R. and Gori, Marco and Jiménez-Ruiz, Ernesto},
  volume = 	 {202},
  series = 	 {CEUR Workshop Proceedings},
  month = Jul,
  dates = 	 {3--5 Jul},
  publisher =    {CEUR},
  issn = {1613-0073}
}

@inproceedings{lopezrincon2023bayesian,
  title = {Bayesian Optimization for the Inverse Problem in Electrocardiography},
  booktitle = {2023 IEEE Symposium Series on Computational Intelligence (SSCI)},
  publisher = {IEEE},
  author = {Lopez-Rincon,  Alejandro and Rojas-Velazquez,  David and Garssen,  Johan and van der Laan,  Sander W. and Oberski,  Daniel and Tonda,  Alberto},
  year = {2023},
  month = dec,
  abstract = {The inverse problem in electrocardiography is an illposed problem where the objective is to reconstruct the electrical activity of the epicardial surface of the heart, given the electrical activity on the thorax’ surface. In the forward problem, the electrical propagation from heart to thorax is modeled by the volume conductor equation with Dirichlet boundary conditions in the heart’s surface, and null flux coming from the thorax. The inverse problem, however, does not have a unique solution. In order to find solutions for the inverse problem, techniques such as Tikhonov regularization are classically used, but they often deliver unrealistic solutions. As an alternative, we propose a novel approach, where a fixed solution of the volume conductor model with a source in a forward scheme is used to solve the inverse problem. The unknown values for parameters of the fixed solution can be found using optimization techniques. Due to the characteristics of the problem, where each single evaluation of the cost function is expensive, we use a specialized CMA-ES-based Bayesian optimization technique, that can deliver good results even with a reduced number of function evaluations. Experiments show that the proposed approach can deliver improved results for in-silico simulations.}
}

@inproceedings{calabrese2024towards,
author = {Calabrese, Andrea and Quer, Stefano and Squillero, Giovanni and Tonda, Alberto},
title = {Towards an Evolutionary Approach for Exploting Core Knowledge in Artificial Intelligence},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3638530.3654230},
abstract = {This paper presents a proof of concept for a novel evolutionary methodology inspired by core knowledge. This theory describes human cognition as a small set of innate abilities combined through compositionality. The proposed approach generates predictive descriptions of the interaction between elements in simple 2D videos. It exploits well-known strategies, such as image segmentation, object detection, simple laws of physics (kinematics and dynamics), and evolving rules, including high-level classes and their interactions. The experimental evaluation focuses on two classic video games, Pong and Arkanoid. Analyzing a small number of raw video frames, the methodology identifies objects, classes, and rules, creating a compact, high-level, predictive description of the interactions between the elements in the videos.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {259–262},
numpages = {4},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{chen2024multiobjective,
author = {Chen, Mathilde and Makowski, David and Tonda, Alberto},
title = {Multi-Objective Optimization for Large-scale Allocation of Soybean Crops},
year = {2024},
isbn = {9798400704949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The optimal allocation of crops to different parcels of land is a problem of paramount practical importance, not only to improve production, but also to address the challenges posed by climate change. However, this optimization problem is inherently complex, characterized by a vast search space that renders traditional optimization techniques impractical without oversimplified assumptions. Compounding this challenge, climate change introduces conflicting objectives, as solutions aiming to just maximize total yield may be more susceptible to extreme weather events, and thus obtain more unpredictable year-by-year outcomes. In order to tackle this complex optimization problem, we propose a multi-objective approach, simultaneously maximizing the overall yield, minimizing the year-on-year yield variance, and minimizing the total cultivated surface. The approach exploits an established multi-objective evolutionary algorithm, and employs a machine learning model able to predict yield from weather and soil conditions, trained on historical data, making it possible to tackle allocation problems of large size. An experimental evaluation focusing on the allocation of soybean crops in the European continent for the years 2000-2023 shows that the proposed methodology is able to identify different trade-offs between the conflicting objectives, that an expert analysis later reveals to be realistic and meaningful for driving stakeholder decisions.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1174–1182},
numpages = {9},
location = {Melbourne, VIC, Australia},
series = {GECCO '24}
}

@inproceedings{squillero2024byron,
author = {Squillero, Giovanni and Tonda, Alberto and Masetta, Dimitri and Sacchet, Marco},
title = {Byron: A Fuzzer for Turing-complete Test Programs},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This paper describes Byron, an evolutionary fuzzer of assembly-language programs for the test and verification of programmable devices. Candidate solutions are internally encoded as typed, directed multigraphs, that is, graphs where multiple edges can connect the same pair of vertexes, with an added layer that defines the type of information vertexes can hold, and constraints the possible kinds of edges. Multiple genetic operators and a self-adaptation mechanism make the tool ready to tackle industrial problems.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1691–1694},
numpages = {4},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{giannini2024categorical,
author={Giannini, Francesco
and Fioravanti, Stefano
and Barbiero, Pietro
and Tonda, Alberto
and Li{\`o}, Pietro
and Di Lavore, Elena},
editor={Longo, Luca
and Lapuschkin, Sebastian
and Seifert, Christin},
title={Categorical Foundation of Explainable AI: A Unifying Theory},
booktitle={Explainable Artificial Intelligence},
year={2024},
publisher={Springer Nature Switzerland},
address={Cham},
pages={185--206},
abstract={Explainable AI (XAI) aims to address the human need for safe and reliable AI systems. However, numerous surveys emphasize the absence of a sound mathematical formalization of key XAI notions---remarkably including the term ``explanation'', which still lacks a precise definition. To bridge this gap, this paper introduces a unifying mathematical framework allowing the rigorous definition of key XAI notions and processes, using the well-funded formalism of Category theory. In particular, we show that the introduced framework allows us to: (i) model existing learning schemes and architectures in both XAI and AI in general, (ii) formally define the term ``explanation'', (iii) establish a theoretical basis for XAI taxonomies, and (iv) analyze commonly overlooked aspects of explaining methods. As a consequence, the proposed categorical framework represents a significant step towards a sound theoretical foundation of explainable AI by providing an unambiguous language to describe and model concepts, algorithms, and systems, thus also promoting research in XAI and collaboration between researchers from diverse fields, such as computer science, cognitive science, and abstract mathematics.},
isbn={978-3-031-63800-8}
}

@inproceedings{ruedaarango2024image,
  author={Rueda-Arango, Y. Dianey and Rojas-Velazquez, David and Gorelova, Aleksandra V. and Garssen, Johan and Tonda, Alberto and Lopez-Rincon, Alejandro},
  booktitle={2024 16th International Conference on Human System Interaction (HSI)}, 
  title={Image Generation with Interactive Evolutionary System using Bayesian Optimization}, 
  year={2024},
  pages={1-7},
  doi={10.1109/HSI61632.2024.10613596},
  abstract={Interactive Evolutionary Systems (IES) can generate several designs based on a handful of input parameters. Never-theless, the choice of the parameters is an open problem and it is limited to a few evaluations as they require human input. As a solution, Bayesian Optimization (BO) can be used to tune IES parameters. BO is a statistical method that efficiently models and optimizes expensive black-box derivative-free functions in few evaluations. In the context of creative IES, such as image generators, it can be used in conjunction with user preferences to optimize a complex-structured input space, such as variations of artistic images with uniqueness and creativity that follow the original concept and the artistic intention. Therefore, for this objective, we propose an implementation of BOIES with a metric based on user preferences that interactively evaluates a batch of images to evolve a set of parameters in Stable Diffusion to create variations with a given human-made artwork. Our results proved better than baseline, and against generated images using Neural Style Transfer (NST). The resulting images were consistent in terms of uniqueness, quality, and following a given concept.}
}

@inproceedings{rojasvelazquez2024machinelearning,
  author={Rojas-Velazquez, David and Kidwai, Sarah and de Vries, Luciënne and Tözsér, Péter and Valencia-Rosado, Luis Oswaldo and Garssen, Johan and Tonda, Alberto and Lopez-Rincon, Alejandro},
  booktitle={2024 16th International Conference on Human System Interaction (HSI)}, 
  title={Machine-Learning Analysis of mRNA: An Application to Inflammatory Bowel Disease}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  doi={10.1109/HSI61632.2024.10613568},
  abstract={Inflammatory Bowel Disease (IBD), that includes Crohn's disease (CD) and Ulcerative Colitis (UC), is a global health concern due to the increasing number of cases. Diagnosing IBD is a challenging task due to a considerable number of clinical factors. Delayed or inaccurate IBD diagnosis can worsen the disease and complicate achieving remission, therefore, early diagnosis and prompt treatment are crucial. In this study, we adapted a methodology to analyze 16s rRNA (18,758 features) to analyze mRNA (54,675 features) that consists of three phases: 1) preprocessing, 2) feature selection, and 3) testing. We applied this methodology for analyzing mRNA datasets from the Gene Expression Omnibus (GEO) repository, aiming to discover possible biomarkers for IBD diagnosis. We experimented with three datasets, using one dataset for feature (gene) selection and we tested the results in the other two. We compared results with those obtained from other feature selection methods, such as the F-score-based K-Best and random selection. The Area Under the Curve (AUC) was used to measure the diagnostic accuracy and as a metric to compare results between the methodology and other feature selection methods. The Matthews Correlation Coefficient (MCC) was used as an additional metric to evaluate the performance of the methodology and for comparison with other feature selection methods.}
}


