@article{gandini2010aframework,
  doi = {10.1007/s10836-010-5184-5},
  url = {https://doi.org/10.1007/s10836-010-5184-5},
  year = {2010},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {26},
  number = {6},
  pages = {689--697},
  author = {Stefano Gandini and Walter Ruzzarin and Ernesto Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {A Framework for Automated Detection of Power-related Software Errors in Industrial Verification Processes},
  journal = {Journal of Electronic Testing},
  abstract = {The complexity of cell phones is continually increasing, with regards to both hardware and software parts. As many complex devices, their components are usually designed and verified separately by specialized teams of engineers and programmers. However, even if each isolated part is working flawlessly, it often happens that bugs in one software application arise due to the interaction with other modules. Those software misbehaviors become particularly critical when they affect the residual battery life, causing power dissipation. An automatic approach to detect power-affecting software defects is proposed. The approach is intended to be part of a qualifying verification plan and complete human expertise. Motorola, always at the forefront of researching innovations in the product development chain, experimented the approach on a mobile phone prototype during a partnership with Politecnico di Torino. Software errors unrevealed by all human-designed tests have been detected by the proposed framework, two out of three critical from the power consumption point of view, thus enabling Motorola to further improve its verification plans. Details of the tests and experimental results are presented.}
}

@article{grosso2011functional,
  doi = {10.1007/s10836-011-5219-6},
  url = {https://doi.org/10.1007/s10836-011-5219-6},
  year = {2011},
  month = apr,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {27},
  number = {4},
  pages = {505--516},
  author = {Michelangelo Grosso and Wilson Javier Perez Holguin and Danilo Ravotto and Ernesto Sanchez and Matteo Sonza Reorda and Alberto Tonda and Jaime Velasco Medina},
  title = {Functional Verification of {DMA} Controllers},
  journal = {Journal of Electronic Testing},
  abstract = {Today’s SoCs are composed of a wide variety of modules, such as microprocessor cores, memories, peripherals, and customized blocks directly related to the targeted application. To effectively perform simulation-based design verification of peripheral cores, it is necessary to stimulate the description in a broad range of behavior possibilities, checking the produced results. Different strategies for generating suitable stimuli have been proposed by the research community to functionally verify these modules and their interconnection when embedded in a SoC: however, their verification often remains a largely manual and unstructured operation. In this paper we describe a general approach to develop concise and effective sets of inputs by modeling the configuration modes of a peripheral with a graph, and creating paths able to cover all of its nodes: proper stimuli for the device are then directly derived from the paths. The resulting inputs sequences are aimed at design verification of system peripherals such as DMA controllers, and can be applied via simulation by means of dedicated testbenches or by setting up an environment including a processor, which executes a proper test priogram. In the latter case, the developed programs can be exploited in later stages for testing, by adding suitable observability features. Experimental results demonstrating the method effectiveness are reported.}
}

@article{dicarlo2011increasing,
  doi = {10.1016/j.patrec.2011.05.019},
  url = {https://doi.org/10.1016/j.patrec.2011.05.019},
  year = {2011},
  month = oct,
  publisher = {Elsevier {BV}},
  volume = {32},
  number = {13},
  pages = {1594--1603},
  author = {S. Di Carlo and M. Falasconi and E. Sanchez and A. Scionti and G. Squillero and A. Tonda},
  title = {Increasing pattern recognition accuracy for chemical sensing by evolutionary based drift compensation},
  journal = {Pattern Recognition Letters},
  abstract = {Artificial olfaction systems, which mimic human olfaction by using arrays of gas chemical sensors combined with pattern recognition methods, represent a potentially low-cost tool in many areas of industry such as perfumery, food and drink production, clinical diagnosis, health and safety, environmental monitoring and process control. However, successful applications of these systems are still largely limited to specialized laboratories. Sensor drift, i.e., the lack of a sensor’s stability over time, still limits real industrial setups. This paper presents and discusses an evolutionary based adaptive drift-correction method designed to work with state-of-the-art classification systems. The proposed approach exploits a cutting-edge evolutionary strategy to iteratively tweak the coefficients of a linear transformation which can transparently correct raw sensors’ measures thus mitigating the negative effects of the drift. The method learns the optimal correction strategy without the use of models or other hypotheses on the behavior of the physical chemical sensors.}
}

@article{grosso2012software,
  doi = {10.1007/s10836-012-5287-2},
  url = {https://doi.org/10.1007/s10836-012-5287-2},
  year = {2012},
  month = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {28},
  number = {2},
  pages = {189--200},
  author = {M. Grosso and W. J. Perez Holguin and E. Sanchez and M. Sonza Reorda and A. Tonda and J. Velasco Medina},
  title = {Software-Based Testing for System Peripherals},
  journal = {Journal of Electronic Testing},
  abstract = {Software-based self-testing strategies have been mainly proposed to tackle microprocessor testing, but may also be applied to peripheral testing. However, testing system peripherals (e.g., DMA controllers, interrupt controllers, and internal counters) is a challenging task, since their observability and controllability are even more reduced when compared to microprocessors and to peripherals devoted to I/O communication (e.g., serial or parallel ports). In this paper an approach to develop functional tests for system peripherals is proposed. The presented methodology requires two correlated phases: module configuration and module operation. The first one prepares the peripheral to work in the different operation modes, whereas the second one is in charge of exciting the whole device and observing its behavior. We propose a methodology that guides the test engineer in building a compact set of test programs able to reach high structural fault coverage levels in a short time. Experimental results demonstrating the method effectiveness for two real-world case studies are finally reported.}
}

@article{tonda2012abenchmark,
  doi = {10.1007/s12293-012-0095-x},
  url = {https://doi.org/10.1007/s12293-012-0095-x},
  year = {2012},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {4},
  number = {4},
  pages = {263--277},
  author = {Alberto Tonda and Evelyne Lutton and Giovanni Squillero},
  title = {A benchmark for cooperative coevolution},
  journal = {Memetic Computing},
  abstract = {Cooperative co-evolution algorithms (CCEA) are a thriving sub-field of evolutionary computation. This class of algorithms makes it possible to exploit more efficiently the artificial Darwinist scheme, as soon as an optimisation problem can be turned into a co-evolution of interdependent sub-parts of the searched solution. Testing the efficiency of new CCEA concepts, however, it is not straightforward: while there is a rich literature of benchmarks for more traditional evolutionary techniques, the same does not hold true for this relatively new paradigm. We present a benchmark problem designed to study the behavior and performance of CCEAs, modeling a search for the optimal placement of a set of lamps inside a room. The relative complexity of the problem can be adjusted by operating on a single parameter. The fitness function is a trade-off between conflicting objectives, so the performance of an algorithm can be examined by making use of different metrics. We show how three different cooperative strategies, Parisian Evolution, Group Evolution and Allopatric Group Evolution, can be applied to the problem. Using a Classical Evolution approach as comparison, we analyse the behavior of each algorithm in detail, with respect to the size of the problem.}
}

@article{bucur2014theimpact,
  doi = {10.1016/j.asoc.2013.12.002},
  url = {https://doi.org/10.1016/j.asoc.2013.12.002},
  year = {2014},
  month = mar,
  publisher = {Elsevier {BV}},
  volume = {16},
  pages = {210--222},
  author = {Doina Bucur and Giovanni Iacca and Giovanni Squillero and Alberto Tonda},
  title = {The impact of topology on energy consumption for collection tree protocols: An experimental assessment through evolutionary computation},
  journal = {Applied Soft Computing},
  abstract = {The analysis of worst-case behavior in wireless sensor networks is an extremely difficult task, due to the complex interactions that characterize the dynamics of these systems. In this paper, we present a new methodology for analyzing the performance of routing protocols used in such networks. The approach exploits a stochastic optimization technique, specifically an evolutionary algorithm, to generate a large, yet tractable, set of critical network topologies; such topologies are then used to infer general considerations on the behaviors under analysis. As a case study, we focused on the energy consumption of two well-known ad hoc routing protocols for sensor networks: the multi-hop link quality indicator and the collection tree protocol. The evolutionary algorithm started from a set of randomly generated topologies and iteratively enhanced them, maximizing a measure of “how interesting” such topologies are with respect to the analysis. In the second step, starting from the gathered evidence, we were able to define concrete, protocol-independent topological metrics which correlate well with protocols’ poor performances. Finally, we discovered a causal relation between the presence of cycles in a disconnected network, and abnormal network traffic. Such creative processes were made possible by the availability of a set of meaningful topology examples. Both the proposed methodology and the specific results presented here – that is, the new topological metrics and the causal explanation – can be fruitfully reused in different contexts, even beyond wireless sensor networks.}
}

@article{lutton2014food,
  doi = {10.1016/j.ifset.2014.02.003},
  url = {https://doi.org/10.1016/j.ifset.2014.02.003},
  year = {2014},
  month = oct,
  publisher = {Elsevier {BV}},
  volume = {25},
  pages = {67--77},
  author = {Evelyne Lutton and Alberto Tonda and S{\'{e}}bastien Gaucel and Alain Riaublanc and Nathalie Perrot},
  title = {Food model exploration through evolutionary optimisation coupled with visualisation: Application to the prediction of a milk gel structure},
  journal = {Innovative Food Science {\&} Emerging Technologies},
  abstract = {Obtaining reliable in-silico food models is fundamental for a better understanding of these systems. The complex phenomena involved in these real-world processes reflect in the intricate structure of models, so that thoroughly exploring their behaviour and, for example, finding meaningful correlations between variables, become a relevant challenge for the experts. In this paper, we present a methodology based on visualisation and evolutionary computation to assist experts during model exploration. The proposed approach is tested on an established model of milk gel structures, and we show how experts are eventually able to find a correlation between two parameters, previously considered independent. Reverse-engineering the final outcome, the emergence of such a pattern is proved by physical laws underlying the oil–water interface colonisation. It is interesting to notice that, while the present work is focused on milk gel modelling, the proposed methodology can be straightforwardly generalised to other complex physical phenomena. Industrial relevance: Sustainability is nowadays at the heart of industrial requirements. The development of mathematical approaches should facilitate common approaches to risk/benefit assessment and nutritional quality in food research and industry. These models will enhance knowledge on process–structure–property relationships from the molecular to macroscopic level, and facilitate the creation of in-silico simulators with functional and nutritional properties. The stochastic optimisation techniques (evolutionary algorithms) employed in these works allow the users to thoroughly explore the systems: when coupled with visualisation, they make it possible to provide the experts with a restricted set of significant data, helping them to highlight eventual issues or possible improvements in the model. With regard to the complexity of the food systems and dynamics, the challenge of the mathematical approaches is to realise a complete dynamic description of food processing. In order to reach this objective, it is mandatory to use innovative strategies, exploiting the most recent advances in cognitive and complex system sciences.}
}

@article{gaudesi2016exploiting,
  doi = {10.1109/tciaig.2015.2439061},
  url = {https://doi.org/10.1109/tciaig.2015.2439061},
  year = {2016},
  month = sep,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {8},
  number = {3},
  pages = {288--300},
  author = {Marco Gaudesi and Elio Piccolo and Giovanni Squillero and Alberto Tonda},
  title = {Exploiting Evolutionary Modeling to Prevail in Iterated Prisoner's Dilemma Tournaments},
  journal = {{IEEE} Transactions on Computational Intelligence and {AI} in Games},
  abstract = {The iterated prisoner's dilemma is a famous model of cooperation and conflict in game theory. Its origin can be traced back to the Cold War, and countless strategies for playing it have been proposed so far, either designed by hand or automatically generated by computers. In the 2000s, scholars started focusing on adaptive players, that is, able to classify their opponent's behavior and adopt an effective counter-strategy. The player presented in this paper, pushes such idea even further: it builds a model of the current adversary from scratch, without relying on any pre-defined archetypes, and tweaks it as the game develops using an evolutionary algorithm; at the same time, it exploits the model to lead the game into the most favorable continuation. Models are compact nondeterministic finite state machines; they are extremely efficient in predicting opponents' replies, without being completely correct by necessity. Experimental results show that such a player is able to win several one-to-one games against strong opponents taken from the literature, and that it consistently prevails in round-robin tournaments of different sizes.}
}

@article{bucur2016optimizing,
  doi = {10.1016/j.asoc.2015.11.024},
  url = {https://doi.org/10.1016/j.asoc.2015.11.024},
  year = {2016},
  month = mar,
  publisher = {Elsevier {BV}},
  volume = {40},
  pages = {416--426},
  author = {Doina Bucur and Giovanni Iacca and Marco Gaudesi and Giovanni Squillero and Alberto Tonda},
  title = {Optimizing groups of colluding strong attackers in mobile urban communication networks with evolutionary algorithms},
  journal = {Applied Soft Computing},
  abstract = {In novel forms of the Social Internet of Things, any mobile user within communication range may help routing messages for another user in the network. The resulting message delivery rate depends both on the users’ mobility patterns and the message load in the network. This new type of configuration, however, poses new challenges to security, amongst them, assessing the effect that a group of colluding malicious participants can have on the global message delivery rate in such a network is far from trivial. In this work, after modeling such a question as an optimization problem, we are able to find quite interesting results by coupling a network simulator with an evolutionary algorithm. The chosen algorithm is specifically designed to solve problems whose solutions can be decomposed into parts sharing the same structure. We demonstrate the effectiveness of the proposed approach on two medium-sized Delay-Tolerant Networks, realistically simulated in the urban contexts of two cities with very different route topology: Venice and San Francisco. In all experiments, our methodology produces attack patterns that greatly lower network performance with respect to previous studies on the subject, as the evolutionary core is able to exploit the specific weaknesses of each target configuration.}
}

@article{squillero2016divergence,
  doi = {10.1016/j.ins.2015.09.056},
  url = {https://doi.org/10.1016/j.ins.2015.09.056},
  year = {2016},
  month = feb,
  publisher = {Elsevier {BV}},
  volume = {329},
  pages = {782--799},
  author = {Giovanni Squillero and Alberto Tonda},
  title = {Divergence of character and premature convergence: A survey of methodologies for promoting diversity in evolutionary optimization},
  journal = {Information Sciences},
  abstract = {In the past decades, different evolutionary optimization methodologies have been proposed by scholars and exploited by practitioners, in a wide range of applications. Each paradigm shows distinctive features, typical advantages, and characteristic disadvantages; however, one single problem is shared by almost all of them: the ``lack of speciation''. While natural selection favors variations toward greater divergence, in artificial evolution candidate solutions do homologize. Many authors argued that promoting diversity would be beneficial in evolutionary optimization processes, and that it could help avoiding premature convergence on sub-optimal solutions. The paper surveys the research in this area up to mid 2010s, it re-orders and re-interprets different methodologies into a single framework, and proposes a novel three-axis taxonomy. Its goal is to provide the reader with a unifying view of the many contributions in this important corpus, allowing comparisons and informed choices. Characteristics of the different techniques are discussed, and similarities are highlighted; practical ways to measure and promote diversity are also suggested.}
}


@article{perrot2016someremarks,
  doi = {10.1016/j.tifs.2015.10.003},
  url = {https://doi.org/10.1016/j.tifs.2015.10.003},
  year = {2016},
  month = feb,
  publisher = {Elsevier {BV}},
  volume = {48},
  pages = {88--101},
  author = {Nathalie Perrot and Hugo De Vries and Evelyne Lutton and Harald G.J. van Mil and Mechthild Donner and Alberto Tonda and Sophie Martin and Isabelle Alvarez and Paul Bourgine and Erik van der Linden and Monique A.V. Axelos},
  title = {Some remarks on computational approaches towards sustainable complex agri-food systems},
  journal = {Trends in Food Science {\&} Technology},
  abstract = {Background: Agri-food is one of the most important sectors of the industry in Europe and potentially a major contributor to the global warming. Sustainability issues in this context pose a huge challenge for several reasons: the variety of considered scales, the number of disciplines involved, the uncertainties, the out-of-equilibrium states, the complex quantitative and qualitative factors, the normative issues and the availability of data. Although important insight and breakthroughs have been attained in different scientific domains, an overarching and integrated analysis of these complex problems have yet to be realized. Scope and Approach: This context creates huge opportunities for research in interaction with mathematical programming, integrative models and decision-support tools. The paper propose a computational viewpoint including questions of holistic approach, multiscale reconstruction and optimization. Some directions are discussed. Key Findings and Conclusions: Several research questions based on a mathematical programming framework are emerging: how can such a framework manage uncertainty, cope with complex qualitative and quantitative information essential for social and environmental considerations, encompass diverse scales in space and time, cope with a multivariable dynamic environment and with scarcity of data. Moreover, how can it deal with different perspectives, types of models, research goals and data produced by conceptually disjoint scientific disciplines, ranging from physics and physiology to sociology and ethics? Building models is essential, but highly difficult; it will need a strong iterative interaction combining computational intensive methods, formal reasoning and the experts of the different fields. Some future research directions are proposed, involving all those dimensions: mathematical resilience, human-machine interactive learning and optimization techniques.}
}

@article{deplano2016anatomy,
  doi = {10.1007/s12065-016-0144-3},
  url = {https://doi.org/10.1007/s12065-016-0144-3},
  year = {2016},
  month = sep,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {9},
  number = {4},
  pages = {125--136},
  author = {Igor Deplano and Giovanni Squillero and Alberto Tonda},
  title = {Anatomy of a portfolio optimizer under a limited budget constraint},
  journal = {Evolutionary Intelligence},
  abstract = {Predicting the market’s behavior to profit from trading stocks is far from trivial. Such a task becomes even harder when investors do not have large amounts of money available, and thus cannot influence this complex system in any way. Machine learning paradigms have been already applied to financial forecasting, but usually with no restrictions on the size of the investor’s budget. In this paper, we analyze an evolutionary portfolio optimizer for the management of limited budgets, dissecting each part of the framework, discussing in detail the issues and the motivations that led to the final choices. Expected returns are modeled resorting to artificial neural networks trained on past market data, and the portfolio composition is chosen by approximating the solution to a multi-objective constrained problem. An investment simulator is eventually used to measure the portfolio performance. The proposed approach is tested on real-world data from New York’s, Milan’s and Paris’ stock exchanges, exploiting data from June 2011 to May 2014 to train the framework, and data from June 2014 to July 2015 to validate it. Experimental results demonstrate that the presented tool is able to obtain a more than satisfying profit for the considered time frame.}
}

@article{squillero2017overrealism,
  doi = {10.1007/s10710-017-9295-y},
  url = {https://doi.org/10.1007/s10710-017-9295-y},
  year = {2017},
  month = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {18},
  number = {3},
  pages = {391--393},
  author = {G. Squillero and A. Tonda},
  title = {(Over-)Realism in evolutionary computation: Commentary on {\textquotedblleft}On the Mapping of Genotype to Phenotype in Evolutionary Algorithms{\textquotedblright} by Peter A. Whigham,  Grant Dick,  and James Maclaurin},
  journal = {Genetic Programming and Evolvable Machines},
  abstract = {Inspiring metaphors play an important role in the beginning of an investigation, but are less important in a mature research field as the real phenomena involved are understood. Nowadays, in evolutionary computation, biological analogies should be taken into consideration only if they deliver significant advantages.}
}

@article{versino2017datadriven,
  doi = {10.1016/j.cma.2017.02.016},
  url = {https://doi.org/10.1016/j.cma.2017.02.016},
  year = {2017},
  month = may,
  publisher = {Elsevier {BV}},
  volume = {318},
  pages = {981--1004},
  author = {Daniele Versino and Alberto Tonda and Curt A. Bronkhorst},
  title = {Data driven modeling of plastic deformation},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  abstract = {In this paper the application of machine learning techniques for the development of constitutive material models is being investigated. A flow stress model, for strain rates ranging from 10^{−4} to 10^{12} (quasi-static to highly dynamic), and temperatures ranging from room temperature to over 1000 K, is obtained by beginning directly with experimental stress–strain data for Copper. An incrementally objective and fully implicit time integration scheme is employed to integrate the hypo-elastic constitutive model, which is then implemented into a finite element code for evaluation. Accuracy and performance of the flow stress models derived from symbolic regression are assessed by comparison to Taylor anvil impact data. The results obtained with the free-form constitutive material model are compared to well-established strength models such as the Preston–Tonks–Wallace (PTW) model and the Mechanical Threshold Stress (MTS) model. Preliminary results show candidate free-form models comparing well with data in regions of stress–strain space with sufficient experimental data, pointing to a potential means for both rapid prototyping in future model development, as well as the use of machine learning in capturing more data as a guide for more advanced model development.}
}

@article{barnabe2018multiscale,
  doi = {10.1016/j.ifset.2017.09.015},
  url = {https://doi.org/10.1016/j.ifset.2017.09.015},
  year = {2018},
  month = apr,
  publisher = {Elsevier {BV}},
  volume = {46},
  pages = {41--53},
  author = {M. Barnab{\'{e}} and N. Blanc and T. Chabin and J.-Y. Delenne and A. Duri and X. Frank and V. Hugouvieux and E. Lutton and F. Mabille and S. Nezamabadi and N. Perrot and F. Radjai and T. Ruiz and A. Tonda},
  title = {Multiscale modeling for bioresources and bioproducts},
  journal = {Innovative Food Science {\&} Emerging Technologies},
  abstract = {Designing and processing complex matter and materials are key objectives of bioresource and bioproduct research. Modeling approaches targeting such systems have to account for their two main sources of complexity: their intrinsic multi-scale nature; and the variability and heterogeneity inherent to all living systems. Here we provide insight into methods developed at the Food & Bioproduct Engineering division (CEPIA) of the French National Institute of Agricultural Research (INRA). This brief survey focuses on innovative research lines that tackle complexity by mobilizing different approaches with complementary objectives. On one hand cognitive approaches aim to uncover the basic mechanisms and laws underlying the emerging collective properties and macroscopic behavior of soft-matter and granular systems, using numerical and experimental methods borrowed from physics and mechanics. The corresponding case studies are dedicated to the structuring and phase behavior of biopolymers, powders and granular materials, and to the evolution of these structures caused by external constraints. On the other hand machine learning approaches can deal with process optimizations and outcome predictions by extracting useful information and correlations from huge datasets built from experiments at different length scales and in heterogeneous conditions. These predictive methods are illustrated in the context of cheese ripening, grape maturity prediction and bacterial production.}
}

@article{tonda2017insilico,
  doi = {10.1039/c7fo00830a},
  url = {https://doi.org/10.1039/c7fo00830a},
  year = {2017},
  publisher = {Royal Society of Chemistry ({RSC})},
  volume = {8},
  number = {12},
  pages = {4404--4413},
  author = {Alberto Tonda and Anita Grosvenor and Stefan Clerens and Steven Le Feunteun},
  title = {In silico modeling of protein hydrolysis by endoproteases: a case study on pepsin digestion of bovine lactoferrin},
  journal = {Food {\&} Function},
  abstract = {This paper presents a novel model of protein hydrolysis and release of peptides by endoproteases. It requires the amino-acid sequence of the protein substrate to run, and makes use of simple Monte-Carlo in silico simulations to qualitatively and quantitatively predict the peptides that are likely to be produced during the course of the proteolytic reaction. In the present study, the model is applied to the case of pepsin, the gastric protease. Unlike pancreatic proteases, pepsin has a low substrate specificity and therefore displays a stochastic behavior that is particularly challenging to model and predict. Two versions of the model are studied and compared with peptidomic data obtained during pepsin hydrolysis of bovine lactoferrin. The first version of the model takes into account cleavage probabilities according to the amino acids in position P1–P1′ only, whereas the second version also accounts for the influence of neighbor amino acids (P4, P3, P2, P2′, P3′, P4′) and peptide terminal ends. The second version of the model was able to reproduce many real-world features of the reported behavior of pepsin, such as the peptide size distribution, or the quantity of free amino-acids. More remarkably, 50\% of the experimentally monitored peptides (44/87) lay within the 120 most abundant simulated peptides. The presented methodology has the advantage of being applicable not only to different proteins, but to different enzymes as well, as long as cleavage frequency data are available.}
}

@article{djekic2018review,
  doi = {10.1016/j.jclepro.2017.11.241},
  url = {https://doi.org/10.1016/j.jclepro.2017.11.241},
  year = {2018},
  month = mar,
  publisher = {Elsevier {BV}},
  volume = {176},
  pages = {1012--1025},
  author = {Ilija Djekic and Neus Sanju{\'{a}}n and Gabriela Clemente and Anet Re{\v{z}}ek Jambrak and Aleksandra Djuki{\'{c}}-Vukovi{\'{c}} and Ur{\v{s}}ka Vrabi{\v{c}} Brodnjak and Eugen Pop and Rallou Thomopoulos and Alberto Tonda},
  title = {Review on environmental models in the food chain - Current status and future perspectives},
  journal = {Journal of Cleaner Production},
  abstract = {Diversity of food systems and their interaction with the environment has become a research topic for many years. Scientists use various models to explain environmental issues of food systems. This paper gives an overview of main streams in analyzing this topic. A literature review was performed by analyzing published scientific papers on environmental impacts in the food chain. The selection criteria were focused on different environmental approaches applied in the food chain and on the perspectives of future research. This review shows that on the one side there are generic environmental models developed by environmental scientists and as such applied on food. On the other side, there are models developed by food scientists in order to analyze food-environmental interactions. The environmental research in food industry can be categorized as product, process or system oriented. This study confirmed that the focus of product based approach is mainly performed through life-cycle assessments. The process based approach focuses on food processes such as heat transfer, cleaning and sanitation and various approaches in food waste management. Environmental systems in the food chain were the least investigated stream analyzing levels of environmental practices in place. Future research perspectives are the emerging challenges related to environmental impacts of novel food processing technologies, innovative food packaging and changes in diets and food consumption in connection with climate and environmental changes.}
}

@article{lopezrincon2018evolutionary,
  doi = {10.1016/j.asoc.2017.12.036},
  url = {https://doi.org/10.1016/j.asoc.2017.12.036},
  year = {2018},
  month = apr,
  publisher = {Elsevier {BV}},
  volume = {65},
  pages = {91--100},
  author = {Alejandro Lopez-Rincon and Alberto Tonda and Mohamed Elati and Olivier Schwander and Benjamin Piwowarski and Patrick Gallinari},
  title = {Evolutionary optimization of convolutional neural networks for cancer {miRNA} biomarkers classification},
  journal = {Applied Soft Computing},
  abstract = {Cancer diagnosis is currently undergoing a paradigm shift with the incorporation of molecular biomarkers as part of routine diagnostic panel. This breakthrough discovery directs researches to examine the role of microRNA in cancer, since its deregulation is often associated with almost all human tumors. Such differences frequently recur in tumor-specific microRNA signatures, which are helpful to diagnose tissue of origin and tumor subtypes. Nonetheless, the resulting classification problem is far from trivial, as there are hundreds of microRNA types, and tumors are non-linearly correlated to the presence of several overexpressions. In this paper, we propose to apply an evolutionary optimized convolutional neural network classifier to this complex task. The presented approach is compared against 21 state-of-the-art classifiers, on a real-world dataset featuring 8,129 patients, for 29 different classes of tumors, using 1,046 different biomarkers. As a result of the comparison, we also present a meta-analysis on the dataset, identifying the classes on which the collective performance of the considered classifiers is less effective, and thus possibly singling out types of tumors for which biomarker tests might be less reliable.}
}


@article{garciasanchez2018automated,
  doi = {10.1016/j.knosys.2018.04.030},
  url = {https://doi.org/10.1016/j.knosys.2018.04.030},
  year = {2018},
  month = aug,
  publisher = {Elsevier {BV}},
  volume = {153},
  pages = {133--146},
  author = {Pablo Garc{\'{\i}}a-S{\'{a}}nchez and Alberto Tonda and Antonio M. Mora and Giovanni Squillero and Juan Juli{\'{a}}n Merelo},
  title = {Automated Playtesting in Collectible Card Games Using Evolutionary Algorithms: A Case Study in HearthStone},
  journal = {Knowledge-Based Systems},
  abstract = {Collectible card games have been among the most popular and profitable products of the entertainment industry since the early days of Magic: The GatheringTM in the nineties. Digital versions have also appeared, with HearthStone: Heroes of WarCraftTM being one of the most popular. In Hearthstone, every player can play as a hero, from a set of nine, and build his/her deck before the game from a big pool of available cards, including both neutral and hero-specific cards. This kind of games offers several challenges for researchers in artificial intelligence since they involve hidden information, unpredictable behaviour, and a large and rugged search space. Besides, an important part of player engagement in such games is a periodical input of new cards in the system, which mainly opens the door to new strategies for the players. Playtesting is the method used to check the new card sets for possible design flaws, and it is usually performed manually or via exhaustive search; in the case of Hearthstone, such test plays must take into account the chosen hero, with its specific kind of cards. In this paper, we present a novel idea to improve and accelerate the playtesting process, systematically exploring the space of possible decks using an Evolutionary Algorithm (EA). This EA creates HearthStone decks which are then played by an AI versus established human-designed decks. Since the space of possible combinations that are play-tested is huge, search through the space of possible decks has been shortened via a new heuristic mutation operator, which is based on the behaviour of human players modifying their decks. Results show the viability of our method for exploring the space of possible decks and automating the play-testing phase of game design. The resulting decks, that have been examined for balancedness by an expert player, outperform human-made ones when played by the AI; the introduction of the new heuristic operator helps to improve the obtained solutions, and basing the study on the whole set of heroes shows its validity through the whole range of decks.}
}

@article{karpov2018valis,
  doi = {10.1007/s10710-018-9331-6},
  url = {https://doi.org/10.1007/s10710-018-9331-6},
  year = {2018},
  month = aug,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {19},
  number = {3},
  pages = {453--471},
  author = {Peter Karpov and Giovanni Squillero and Alberto Tonda},
  title = {{VALIS}: an Evolutionary Classification Algorithm},
  journal = {Genetic Programming and Evolvable Machines},
  abstract = {VALIS is an effective and robust classification algorithm with a focus on understandability. Its name stems from Vote-ALlocating Immune System, as it evolves a population of artificial antibodies that can bind to the input data, and performs classification through a voting process. In the beginning of the training, VALIS generates a set of random candidate antibodies; at each iteration, it selects the most useful ones to produce new candidates, while the least, are discarded; the process is iterated until a user-defined stopping condition. The paradigm allows the user to get a visual insight of the learning dynamics, helping to supervise the process, pinpoint problems, and tweak feature engineering. VALIS is tested against nine state-of-the-art classification algorithms on six popular benchmark problems; results demonstrate that it is competitive with well-established black-box techniques, and superior in specific corner cases.}
}

@article{accatino2019tradeoffs,
  doi = {10.1016/j.agsy.2018.08.002},
  url = {https://doi.org/10.1016/j.agsy.2018.08.002},
  year = {2019},
  month = jan,
  publisher = {Elsevier {BV}},
  volume = {168},
  pages = {58--72},
  author = {Francesco Accatino and Alberto Tonda and Camille Dross and Fran{\c{c}}ois L{\'{e}}ger and Muriel Tichit},
  title = {Trade-offs and Synergies Between Livestock Production and Other Ecosystem Services},
  journal = {Agricultural Systems},
  abstract = {One of the biggest challenges today is to satisfy an increasing food demand while preserving ecosystem services. Farming systems have a huge impact on land cover and land use, it is therefore vital to understand how land cover and land use allocation can promote synergies between food production and other ecosystem services. Livestock production has multiple interactions with other ecosystem services and can promote synergies especially in grasslands. We investigated the interactions between livestock production and other ecosystem services and explored strategies to soften trade-offs and enhance synergies. We considered four ecosystem services (livestock production, crop production, carbon sequestration, and timber growth) in France. We considered 709 land units covering a wide range of farming systems where both food production and other ecosystem services are provided. For each land unit, we built ecological production functions that are models measuring the statistical influence of driving variables (i.e. land cover, land use, pesticide expense, and climate) on the provision of ecosystem services. Using an optimization procedure, we studied the extent to which livestock production could be increased without reducing other ecosystem services and without increasing total pesticide expense. We found that a 20\% increase in livestock production could be achieved by all farming systems in France under those general constraints. The 709 land units could be grouped based on similar combinations of increases or decreases in specific ecosystem services during the optimization. 48\% of land units were specialised on food production, 24\% were specialised on other ecosystem services, 16\% were specialised on the mixed provision of food production and other ecosystem services, whereas the remaining 12\% showed decrease or no change in all ecosystem services. Livestock production was either in trade-off or in synergy with the other ecosystem services. The trade-offs could be softened through intensified use of cultivated land and spatial segregation of livestock production. The synergies could be enhanced only through major grassland expansion.}
}

@article{atzeni2018countering,
  doi = {10.1109/access.2018.2874502},
  url = {https://doi.org/10.1109/access.2018.2874502},
  year = {2018},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {6},
  pages = {59540--59556},
  author = {Andrea Atzeni and Fernando Diaz and Andrea Marcelli and Antonio Sanchez and Giovanni Squillero and Alberto Tonda},
  title = {Countering Android Malware: A Scalable Semi-Supervised Approach for Family-Signature Generation},
  journal = {{IEEE} Access},
  abstract = {Reducing the effort required by humans in countering malware is of utmost practical value. We describe a scalable, semi-supervised framework to dig into massive data sets of Android applications and identify new malware families. Until 2010, the industrial standard for the detection of malicious applications has been mainly based on signatures; as each tiny alteration in malware makes them ineffective, new signatures are frequently created –- a task that requires a considerable amount of time and resources from skilled experts. The framework we propose is able to automatically cluster applications in families and suggest formal rules for identifying them with 100\% recall and quite high precision. The families are used either to safely extend experts’ knowledge on new samples or to reduce the number of applications requiring thorough analyses. We demonstrated the effectiveness and the scalability of the approach running experiments on a database of 1.5 million Android applications. In 2018, the framework has been successfully deployed on Koodous, a collaborative anti-malware platform.}
}

@article{thomopoulos2019multicriteria,
  doi = {10.1007/s12393-018-9186-x},
  url = {https://doi.org/10.1007/s12393-018-9186-x},
  year = {2019},
  month = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {11},
  number = {1},
  pages = {44--60},
  author = {R. Thomopoulos and C. Baudrit and N. Boukhelifa and R. Boutrou and P. Buche and E. Guichard and V. Guillard and E. Lutton and P. S. Mirade and A. Ndiaye and N. Perrot and F. Taillandier and T. Thomas-Danguin and A. Tonda},
  title = {Multi-Criteria Reverse Engineering for Food: Genesis and Ongoing Advances},
  journal = {Food Engineering Reviews},
  abstract = {Multi-criteria reverse engineering (MRE) has arisen from the cross-fertilization of advances in mathematics and shifts in social demand. MRE, thus, marks a progressive switch (a) from empirical to formal approaches able to simultaneously factor in diverse parameters, such as environment, economics, and health; (b) from mono-criterion optimization to multi-criteria decision analysis; (c) from forward engineering, observing the results of process conditions, to reverse engineering, selecting the right process conditions for a target output. The food sector has been slow to adopt reverse engineering, but interest is surging now that the industry is looking to shift production towards personalized food. MRE has followed a heterogeneous development trajectory and found applications in different disciplines. The scope of this review spans MRE applications in the food sector covering food packaging and food consumption and focuses on demonstrating potentialities of MRE in a complex field like food. We explain how MRE enables the development of sustainable processes, looking at similar approaches used in sectors other than food. Building on this extensive review, we sketch out some guidelines on approaches to be used in future MRE applications in food, working up from the problem statement.}
}

@article{djekic2019crosseuropean,
  doi = {10.1016/j.jfoodeng.2019.06.007},
  url = {https://doi.org/10.1016/j.jfoodeng.2019.06.007},
  year = {2019},
  month = nov,
  publisher = {Elsevier {BV}},
  volume = {261},
  pages = {109--116},
  author = {Ilija Djekic and Alen Muj{\v{c}}inovi{\'{c}} and Aleksandra Nikoli{\'{c}} and Anet Re{\v{z}}ek Jambrak and Photis Papademas and Aberham Hailu Feyissa and Kamal Kansou and Rallou Thomopoulos and Heiko Briesen and Nickolas G. Kavallieratos and Christos G. Athanassiou and Cristina L.M. Silva and Alexandrina Sirbu and Alexandru Mihnea Moisescu and Igor Tomasevic and Ur{\v{s}}ka Vrabi{\v{c}} Brodnjak and Maria Charalambides and Alberto Tonda},
  title = {Cross-European Initial Survey on the Use of Mathematical Models in Food Industry},
  journal = {Journal of Food Engineering},
  abstract = {Mathematical modelling plays an important role in food engineering having various mathematical models tailored for different food topics. However, mathematical models are followed by limited information on their application in food companies. This paper aims to discuss the extent and the conditions surrounding the usage of mathematical models in the context of European food and drinks industry. It investigates the knowledge, nature and current use of modelling approaches in relation to the industry main characteristics. A total of 203 food companies from 12 European countries were included in this research. Results reveal that the country where the company operates, and size of the company, are more important predictors on the usage of mathematical models followed by the type of food sector. The more developed countries are positioned at the higher level of knowledge and use of available models. Similar pattern was observed at the micro level showing that small or medium sized companies exhibit lack of knowledge, resources and limiting usage of models.}
}

@article{gu2019amathematical,
  doi = {10.1016/j.foodcont.2019.106729},
  url = {https://doi.org/10.1016/j.foodcont.2019.106729},
  year = {2019},
  month = dec,
  publisher = {Elsevier {BV}},
  volume = {106},
  pages = {106729},
  author = {Yingying Gu and Laurent Bouvier and Alberto Tonda and Guillaume Delaplace},
  title = {A Mathematical Model for the Prediction of the Whey Protein Fouling Mass in a Pilot Scale Plate Heat Exchanger},
  journal = {Food Control},
  abstract = {A better understanding of protein fouling during the thermal treatment of whey protein concentrate (WPC) solutions is critical for better fouling control. In order to understand the impact of various parameters on the total whey protein fouling mass, a dimensional analysis was applied to the experimental data obtained from a pilot scale plate heat exchanger, setting total fouling mass as the target variable. A model was developed to predict the total fouling mass, covering a series of variables including whey protein solution concentration (2.5–25 g/L), calcium concentration (70-120 ppm), running time (90-330 min), fouling solution flow rate (200-500 L/h), total fouling surface area, outlet temperature (82-97 °C) and differences in whey protein concentrate powders. In addition to temperature dimensionless parameters, the main parameters involved in the model are the Reynolds number (2000-5000) and the calcium to β-lactoglobulin molar ratio (2.7–34.7). The model developed concerns only pure whey proteins solutions since all the testing solutions were casein free. This model has allowed us to provide guidelines as to how the above parameters influence fouling within the plate heat exchanger, as well as empirical correlations for predicting such fouling development.}
}

@article{gesanguiziou2019annotation,
  doi = {10.1016/j.dib.2019.104204},
  url = {https://doi.org/10.1016/j.dib.2019.104204},
  year = {2019},
  month = aug,
  publisher = {Elsevier {BV}},
  volume = {25},
  pages = {104204},
  author = {Genevi{\`{e}}ve G{\'{e}}san-Guiziou and Aude Alaphilippe and Mathieu Andro and Joël Aubin and Christian Bockstaller and Raphaëlle Botreau and Patrice Buche and Catherine Collet and Nicole Darmon and Monique Delabuis and Agn{\`{e}}s Girard and R{\'{e}}gis Grateau and Kamal Kansou and Vincent Martinet and Jeanne-Marie Membr{\'{e}} and R{\'{e}}gis Sabbadin and Louis-Georges Soler and Marie Thiollet-Scholtus and Alberto Tonda and Hayo Van-Der-Werf},
  title = {Annotation Data About Multi Criteria Assessment Methods Used in the Agri-food Research: The French National Institute for Agricultural Research ({INRA}) Experience},
  journal = {Data in Brief},
  abstract = {This data article contains annotation data characterizing Multi Criteria Assessment (MCA) Methods proposed in the agri-food sector by researchers from INRA, Europe's largest agricultural research institute (INRA, \url{http://institut.inra.fr/en}). MCA can be used to assess and compare agricultural and food systems, and support multi-actor decision making and design of innovative systems for crop production, animal production and processing of agricultural products. These data are stored in a public repository managed by INRA (\url{https://data.inra.fr/; https://doi.org/10.15454/WB51LL}).}
}

@article{djekic2019scientific,
  doi = {10.3390/foods8080301},
  url = {https://doi.org/10.3390/foods8080301},
  year = {2019},
  month = aug,
  publisher = {{MDPI} {AG}},
  volume = {8},
  number = {8},
  pages = {301},
  author = {Ilija Djekic and Milica Poji{\'{c}} and Alberto Tonda and Predrag Putnik and Danijela Bursa{\'{c}} Kova{\v{c}}evi{\'{c}} and Anet Re{\v{z}}ek-Jambrak and Igor Tomasevic},
  title = {Scientific Challenges in Performing Life-Cycle Assessment in the Food Supply Chain},
  journal = {Foods},
  abstract = {This paper gives an overview of scientific challenges that occur when performing life-cycle assessment (LCA) in the food supply chain. In order to evaluate these risks, the Failure Mode and Effect Analysis tool has been used. Challenges related to setting the goal and scope of LCA revealed four hot spots: system boundaries of LCA; used functional units; type and quality of data categories, and main assumptions and limitations of the study. Within the inventory analysis, challenging issues are associated with allocation of material and energy flows and waste streams released to the environment. Impact assessment brings uncertainties in choosing appropriate environmental impacts. Finally, in order to interpret results, a scientifically sound sensitivity analysis should be performed to check how stable calculations and results are. Identified challenges pave the way for improving LCA of food supply chains in order to enable comparison of results.}
}


@article{lopezrincon2019automatic,
  doi = {10.1186/s12859-019-3050-8},
  url = {https://doi.org/10.1186/s12859-019-3050-8},
  year = {2019},
  month = sep,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {20},
  number = {1},
  author = {Alejandro Lopez-Rincon and Marlet Martinez-Archundia and Gustavo U. Martinez-Ruiz and Alexander Schoenhuth and Alberto Tonda},
  title = {Automatic Discovery of 100-{miRNA} Signature for Cancer Classification Using Ensemble Feature Selection},
  journal = {{BMC} Bioinformatics},
  abstract = {Background: MicroRNAs (miRNAs) are noncoding RNA molecules heavily involved in human tumors, in which few of them circulating the human body. Finding a tumor-associated signature of miRNA, that is, the minimum miRNA entities to be measured for discriminating both different types of cancer and normal tissues, is of utmost importance. Feature selection techniques applied in machine learning can help however they often provide naive or biased results. Results: An ensemble feature selection strategy for miRNA signatures is proposed. miRNAs are chosen based on consensus on feature relevance from high-accuracy classifiers of different typologies. This methodology aims to identify signatures that are considerably more robust and reliable when used in clinically relevant prediction tasks. Using the proposed method, a 100-miRNA signature is identified in a dataset of 8023 samples, extracted from TCGA. When running eight-state-of-the-art classifiers along with the 100-miRNA signature against the original 1046 features, it could be detected that global accuracy differs only by 1.4\%. Importantly, this 100-miRNA signature is sufficient to distinguish between tumor and normal tissues. The approach is then compared against other feature selection methods, such as UFS, RFE, EN, LASSO, Genetic Algorithms, and EFS-CLA. The proposed approach provides better accuracy when tested on a 10-fold cross-validation with different classifiers and it is applied to several GEO datasets across different platforms with some classifiers showing more than 90\% classification accuracy, which proves its cross-platform applicability. Conclusions: The 100-miRNA signature is sufficiently stable to provide almost the same classification accuracy as the complete TCGA dataset, and it is further validated on several GEO datasets, across different types of cancer and platforms. Furthermore, a bibliographic analysis confirms that 77 out of the 100 miRNAs in the signature appear in lists of circulating miRNAs used in cancer studies, in stem-loop or mature-sequence form. The remaining 23 miRNAs offer potentially promising avenues for future research.}
}

@article{garciasanchez2020optimizing,
  doi = {10.1016/j.knosys.2019.105032},
  url = {https://doi.org/10.1016/j.knosys.2019.105032},
  year = {2020},
  month = jan,
  publisher = {Elsevier {BV}},
  volume = {188},
  pages = {105032},
  author = {Pablo Garc{\'{\i}}a-S{\'{a}}nchez and Alberto Tonda and Antonio J. Fern{\'{a}}ndez-Leiva and Carlos Cotta},
  title = {Optimizing HearthStone Agents Using an Evolutionary Algorithm},
  journal = {Knowledge-Based Systems},
  abstract = {Digital collectible card games are not only a growing part of the video game industry, but also an interesting research area for the field of computational intelligence. This game genre allows researchers to deal with hidden information, uncertainty and planning, among other aspects. This paper proposes the use of evolutionary algorithms (EAs) to develop agents who play a card game, Hearthstone, by optimizing a data-driven decision-making mechanism that takes into account all the elements currently in play. Agents feature self-learning by means of a competitive coevolutionary training approach, whereby no external sparring element defined by the user is required for the optimization process. One of the agents developed through the proposed approach was runner-up (best 6\%) in an international Hearthstone Artificial Intelligence (AI) competition. Our proposal performed remarkably well, even when it faced state-of-the-art techniques that attempted to take into account future game states, such as Monte-Carlo Tree search. This outcome shows how evolutionary computation could represent a considerable advantage in developing AIs for collectible card games such as Hearthstone.}
}

@article{tonda2019inspyred,
  doi = {10.1007/s10710-019-09367-z},
  url = {https://doi.org/10.1007/s10710-019-09367-z},
  year = {2019},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {21},
  number = {1-2},
  pages = {269--272},
  author = {Alberto Tonda},
  title = {Inspyred: Bio-inspired algorithms in Python},
  journal = {Genetic Programming and Evolvable Machines},
  abstract = {Review of the Python package ``inspyred'', for the Springer journal Genetic Programming and Evolvable Machines.}
}

@article{deponte2020two,
  doi = {10.1016/j.compchemeng.2020.106733},
  url = {https://doi.org/10.1016/j.compchemeng.2020.106733},
  year = {2020},
  month = apr,
  publisher = {Elsevier {BV}},
  volume = {135},
  pages = {106733},
  author = {Hannes Deponte and Alberto Tonda and Nathalie Gottschalk and Laurent Bouvier and Guillaume Delaplace and Wolfgang Augustin and Stephan Scholl},
  title = {Two Complementary Methods for the Computational Modeling of Cleaning Processes in Food Industry},
  journal = {Computers {\&} Chemical Engineering},
  abstract = {Insufficient cleaning in the food industry can create serious hygienic risks. However, when attempting to avoid these risks, food-processing plants frequently tend to clean for too long, at extremely high temperatures, or with too many chemicals, resulting in high cleaning costs and severe environmental impacts. Therefore, the optimization of cleaning processes in the food industry has significant economic and ecological potential. Unfortunately, in-situ assessments of cleaning processes are difficult, and the multitude of different cleaning situations complicates the definition of a comprehensive approach. In this study, two methodological approaches for the comprehensive modeling of cleaning processes are introduced. The resulting models facilitate comparisons of different cleaning processes and they can be scaled up for processes with similar conditions, using cleaning time as a response. A dimensional analysis is performed to obtain general results and to allow transfer of the approaches to other cleaning situations. The models are established according to the statistical rules for the deduction of multiple regression equations for the prediction of the response based on the input parameters. The terms of the model equation are confirmed with a significance analysis. A machine learning approach is also used to create model equations with symbolic regression. Both methods and the obtained model equations are validated. The two applied approaches reveal similar significant terms and models. Significant dimensionless numbers are the Reynolds number, the density number that describes the ratio of the density of the soil to the density of the cleaning agent, and the soil number, which is a new dimensionless number that characterizes the properties of food soils. The methodology of both approaches is transparent; therefore, the resulting equations can be compared and similarities are found. Both methods are deemed applicable for the computational modeling of cleaning processes in food industry.}
}

@article{lopezrincon2020machine,
  doi = {10.3390/cancers12071785},
  url = {https://doi.org/10.3390/cancers12071785},
  year = {2020},
  month = jul,
  publisher = {{MDPI} {AG}},
  volume = {12},
  number = {7},
  pages = {1785},
  author = {Alejandro Lopez-Rincon and Lucero Mendoza-Maldonado and Marlet Martinez-Archundia and Alexander Sch\"{o}nhuth and Aletta D. Kraneveld and Johan Garssen and Alberto Tonda},
  title = {Machine Learning-Based Ensemble Recursive Feature Selection of Circulating {miRNAs} for Cancer Tumor Classification},
  journal = {Cancers},
  abstract = {Circulating microRNAs (miRNA) are small noncoding RNA molecules that can be detected in bodily fluids without the need for major invasive procedures on patients. miRNAs have shown great promise as biomarkers for tumors to both assess their presence and to predict their type and subtype. Recently, thanks to the availability of miRNAs datasets, machine learning techniques have been successfully applied to tumor classification. The results, however, are difficult to assess and interpret by medical experts because the algorithms exploit information from thousands of miRNAs. In this work, we propose a novel technique that aims at reducing the necessary information to the smallest possible set of circulating miRNAs. The dimensionality reduction achieved reflects a very important first step in a potential, clinically actionable, circulating miRNA-based precision medicine pipeline. While it is currently under discussion whether this first step can be taken, we demonstrate here that it is possible to perform classification tasks by exploiting a recursive feature elimination procedure that integrates a heterogeneous ensemble of high-quality, state-of-the-art classifiers on circulating miRNAs. Heterogeneous ensembles can compensate inherent biases of classifiers by using different classification algorithms. Selecting features then further eliminates biases emerging from using data from different studies or batches, yielding more robust and reliable outcomes. The proposed approach is first tested on a tumor classification problem in order to separate 10 different types of cancer, with samples collected over 10 different clinical trials, and later is assessed on a cancer subtype classification task, with the aim to distinguish triple negative breast cancer from other subtypes of breast cancer. Overall, the presented methodology proves to be effective and compares favorably to other state-of-the-art feature selection methods.}
}

@article{carvalho2021modelling,
  doi = {10.3390/foods10010082},
  url = {https://doi.org/10.3390/foods10010082},
  year = {2021},
  month = jan,
  publisher = {{MDPI} {AG}},
  volume = {10},
  number = {1},
  pages = {82},
  author = {Otilia Carvalho and Maria N. Charalambides and Ilija Djeki{\'{c}} and Christos Athanassiou and Serafim Bakalis and Jose Benedito and Aurelien Briffaz and Cristina Casta{\~{n}}{\'{e}} and Guy Della Valle and Isabel Maria Nunes de Sousa and Ferruh Erdogdu and Aberham Hailu Feyissa and Nickolas G. Kavallieratos and Alexandros Koulouris and Milica Poji{\'{c}} and Anabela Raymundo and Jordi Riudavets and Fabrizio Sarghini and Pasquale Trematerra and Alberto Tonda},
  title = {Modelling Processes and Products in the Cereal Chain},
  journal = {Foods},
  abstract = {In recent years, modelling techniques have become more frequently adopted in the field of food processing, especially for cereal-based products, which are among the most consumed foods in the world. Predictive models and simulations make it possible to explore new approaches and optimize proceedings, potentially helping companies reduce costs and limit carbon emissions. Nevertheless, as the different phases of the food processing chain are highly specialized, advances in modelling are often unknown outside of a single domain, and models rarely take into account more than one step. This paper introduces the first high-level overview of modelling techniques employed in different parts of the cereal supply chain, from farming to storage, from drying to milling, from processing to consumption. This review, issued from a networking project including researchers from over 30 different countries, aims at presenting the current state of the art in each domain, showing common trends and synergies, to finally suggest promising future venues for research.}
}

@article{lopezrincon2021classification,
  doi = {10.1038/s41598-020-80363-5},
  url = {https://doi.org/10.1038/s41598-020-80363-5},
  year = {2021},
  month = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {11},
  number = {1},
  author = {Alejandro Lopez-Rincon and Alberto Tonda and Lucero Mendoza-Maldonado and Daphne G. J. C. Mulders and Richard Molenkamp and Carmina A. Perez-Romero and Eric Claassen and Johan Garssen and Aletta D. Kraneveld},
  title = {Classification and Specific Primer Design for Accurate Detection of {SARS}-{CoV}-2 Using Deep Learning},
  journal = {Scientific Reports},
  abstract = {In this paper, deep learning is coupled with explainable artificial intelligence techniques for the discovery of representative genomic sequences in SARS-CoV-2. A convolutional neural network classifier is first trained on 553 sequences from the National Genomics Data Center repository, separating the genome of different virus strains from the Coronavirus family with 98.73\% accuracy. The network’s behavior is then analyzed, to discover sequences used by the model to identify SARS-CoV-2, ultimately uncovering sequences exclusive to it. The discovered sequences are validated on samples from the National Center for Biotechnology Information and Global Initiative on Sharing All Influenza Data repositories, and are proven to be able to separate SARS-CoV-2 from different virus strains with near-perfect accuracy. Next, one of the sequences is selected to generate a primer set, and tested against other state-of-the-art primer sets, obtaining competitive results. Finally, the primer is synthesized and tested on patient samples (n = 6 previously tested positive), delivering a sensitivity similar to routine diagnostic methods, and 100\% specificity. The proposed methodology has a substantial added value over existing methods, as it is able to both automatically identify promising primer sets for a virus from a limited amount of data, and deliver effective results in a minimal amount of time. Considering the possibility of future pandemics, these characteristics are invaluable to promptly create specific detection methods for diagnostics.}
}

@article{iacca2021evolutionary,
  doi = {10.1016/j.simpa.2021.100107},
  url = {https://doi.org/10.1016/j.simpa.2021.100107},
  year = {2021},
  month = jul,
  publisher = {Elsevier {BV}},
  pages = {100107},
  author = {Giovanni Iacca and Kateryna Konotopska and Doina Bucur and Alberto Tonda},
  title = {An Evolutionary Framework for Maximizing Influence Propagation in Social Networks},
  journal = {Software Impacts},
  abstract = {Social networks are one the main sources of information transmission nowadays. However, not all nodes in social networks are equal: in fact, some nodes are more influential than others, i.e., their information tends to spread more. Finding the most influential nodes in a network – the so-called Influence Maximization problem – is an NP-hard problem with great social and economical implications. Here, we introduce a framework based on Evolutionary Algorithms that includes various graph-aware techniques (spread approximations, domain-specific operators, and node filtering) that facilitate the optimization process. The framework can be applied straightforwardly to various social network datasets, e.g., those in the SNAP repository.}
}

@article{hannachi2020vers,
  TITLE = {{Vers une Action Collective {\`a} l'{\'E}chelle des Paysages}},
  AUTHOR = {Hannachi, Mourad and Souch{\`e}re, V{\'e}ronique and Bu{\`e}che, Samuel and Dupayage, Marc and Boquet, Bastien and Pardoux, J.-P. and Berthet, Elsa and Deredec, Anne and Tonda, Alberto and Pluquet, P. and Leroy, J.P. and Albaut, Aur{\'e}lie and Blarel, Jacques and Lecuyer, J{\'e}r{\^o}me and Gazet, Claude and Leuba, Muriel and Gagliardi, {\'E}lodie and Leleu, Karine and Leclercq, Philippe and Quilliot, {\'E}milien and Pernel, J{\'e}r{\^o}me and Declemy, Marc and Chauvel, Bruno and Walker, Anne-Sophie},
  JOURNAL = {{Phytoma. La D{\'e}fense des V{\'e}g{\'e}taux}},
  VOLUME = {733},
  YEAR = {2020},
  MONTH = Apr,
}

@article{mora2022looking,
title = {Looking for Archetypes: Applying Game Data Mining to Hearthstone Decks},
journal = {Entertainment Computing},
pages = {100498},
year = {2022},
month = may,
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2022.100498},
url = {https://www.sciencedirect.com/science/article/pii/S1875952122000222},
author = {Antonio M. Mora and Alberto Tonda and Antonio J. Fernández-Ares and Pablo García-Sánchez},
abstract = {Digital Collectible Cards Games such as Hearthstone have become a very prolific test-bed for Artificial Intelligence algorithms. The main researches have focused on the implementation of autonomous agents (bots) able to effectively play the game. However, this environment is also very attractive for the use of Data Mining (DM) and Machine Learning (ML) techniques, for analysing and extracting useful knowledge from game data. The objective of this work is to apply existing Game Mining techniques in order to study more than 600,000 real decks (groups of cards) created by players with many different skill levels. Data visualisation and analysis tools have been applied, namely, Graph representations and Clustering techniques. Then, an expert player has conducted a deep analysis of the results yielded by these methods, aiming to identify the use of standard - and well-known - archetypes defined by the play methods will also make it possible for the expert to discover hidden relationships between cards that could lead to finding better combinations of them, enhancing players’ decks or, otherwise, identify unbalanced cards that could lead to a disappointing game experience. Moreover, although this work is mostly focused on data analysis and visualization, the obtained results can be applied to improve Hearthstone Bots’ behaviour, e.g. predicting opponent’s actions after identifying a specific archetype in his/her deck.}
}

@article{giovannitti2022virtual,
  doi = {10.1007/s10845-022-01934-z},
  url = {https://doi.org/10.1007/s10845-022-01934-z},
  year = {2022},
  month = apr,
  publisher = {Springer Science and Business Media {LLC}},
  author = {Eliana Giovannitti and Sayyidshahab Nabavi and Giovanni Squillero and Alberto Tonda},
  title = {A Virtual Sensor for Backlash in Robotic Manipulators},
  journal = {Journal of Intelligent Manufacturing},
  abstract = {Gear backlash is a quite serious problem in industrial robots, it causes vibrations and impairs the robot positioning accuracy. Backlash estimation allows targeted maintenance interventions, preserving robot performances and avoiding unforeseen equipment breakdowns. However, a direct measure of the backlash is hard to obtain, and dedicated auxiliary sensors are required for the measurement. This paper presents a method for estimating backlash in robotic joints that does not require the installation of extra devices. It only relies on data gathered from the motor encoder, which is always present in a robotic joint. The approach is based on the observation of a characteristic vibration pattern arising on the motor speed signal when backlash affects the joint transmission. By looking at the amplitude of this vibration some information about the entity of the backlash in the joint is gathered. Experimental results on simulated data are reported in the study to show the robustness of the method, also with respect of noise. Furthermore, tests on real-world data, gathered from robots installed in a production plant, demonstrate the efficacy of the technique. The approach is cost-effective, fast, and easily automatable, therefore convenient for the industrial world.}
}

@article{mejeanperrot2022decision,
title = {A Decision-Support System to Predict Grape Berry Quality and Wine Potential for a Chenin Vineyard},
journal = {Computers and Electronics in Agriculture},
volume = {200},
pages = {107167},
year = {2022},
month = sep,
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107167},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922004847},
author = {Nathalie {Mejean Perrot} and Alberto Tonda and Ilaria Brunetti and Hervé Guillemin and Bruno Perret and Etienne Goulet and Laurence Guerin and Daniel Picque},
abstract = {Grape berry ripening is a complex process, and predicting the quality of wine starting from the ripening kinetics of grape berries is a challenging task. To tackle this problem, we present a decision-support system based on coupling expert know-how with probability laws encapsulated in a probabilistic model, a dynamic Bayesian network. The proposed approach predicts the ripening kinetics of grape berries starting from initial measurements and weather conditions, and then exploits the information to evaluate the potential of the wine that will produced from them. The results show that the dynamic Bayesian network predicts the total acidity concentration and the sugar content of the grape berries with a small amount of error (mean of 6\% for total acidity concentration, 10\% for sugar content) that is considered satisfying by the experts, making it possible to predict the ideal moment for harvesting the grapes up to two weeks in advance. Moreover, feeding the results from the probabilistic model to a fuzzy expert model, the predicted trajectories are compared to an ideal trajectory described by wine experts and formalized mathematically. From this comparison, it is possible to anticipate drifts in wine sensory quality right from the step of grape ripening.}
}

@article{sicard2023aprimer,
  doi = {10.1111/jfpe.14325},
  url = {https://doi.org/10.1111/jfpe.14325},
  year = {2023},
  month = mar,
  publisher = {Wiley},
  author = {Jason Sicard and Sophie Barbe and Rachel Boutrou and Laurent Bouvier and Guillaume Delaplace and Gwenaëlle Lashermes and Laëtitia Th{\'{e}}ron and Olivier Vitrac and Alberto Tonda},
  title = {A primer on predictive techniques for food and bioresources transformation processes},
  journal = {Journal of Food Process Engineering},
  abstract = {To meet current societal demand for more sustainable transformation processes and bioresources, these processes must be optimized and new ones developed. The evolution of various systems (raw material, food, or process attributes) can be predicted to optimize the uses of biomass for better quality, safety, economic benefit, and sustainability. Predictive modeling can guide the necessary changes and influence industrials, governmental policies and consumers decision-making. However, achieving good predictive capability requires reflection on the models and model validation, which can be difficult. This review aims to help scientists begin to predict by presenting the techniques currently used in predictive science for food and related bioproducts. First, a guideline helps readers initiate a prediction process along with final tips and a warning about the risks involved, with a particular focus on the crucial validation step. Three broad categories of techniques are then presented: empirical, mechanistic, and artificial intelligence (or ``data-driven''). For each category, the advantages and limitations of current techniques for prediction are explained in light of their current domains of applications, illustrated with literature studies and a detailed example. Thus this article provides engineering researchers information about predictive modeling which is a recent relevant development in optimization of both food and nonfood bioresources processes.}
}

@article{shi2023handling,
  doi = {10.1007/s10980-023-01635-9},
  url = {https://doi.org/10.1007/s10980-023-01635-9},
  year = {2023},
  month = mar,
  publisher = {Springer Science and Business Media {LLC}},
  author = {Yong Shi and Alberto Tonda and Francesco Accatino},
  title = {Handling ecosystem service trade-offs: the importance of the spatial scale at which no-loss constraints are posed},
  journal = {Landscape Ecology},
  abstract = {Context: Managing land use to promote an ecosystem service (ES) without reducing others is challenging. The spatial scale at which no-loss constraints are imposed is relevant. Objectives: We examined the influence of the spatial scale of no-loss constraints on ESs when one ES was optimised. Specifically, we investigated how carbon sequestration could be maximized at different spatial scales in France with constraints of no-loss on other ESs. Methods: We used a statistical model linking land use and land cover variables to ESs [carbon sequestration (CS), crop production (CP), livestock production, timber growth] in French small agricultural regions (SARs). We optimised CS at the country scale posing no-loss constraints on other ESs at increasing spatial scales, i.e., SARs (scenario ``SARs''), department (``DEP''), administrative region (``REG''), and France (``FRANCE''). We analysed differences between optimized and initial configurations. Results: Optimized CS at the country scale increased with the spatial scale at which no-loss constraints were posed (+0.51\% for DEP and +2.05\% for FRANCE). The variability of ES variation among the SARs similarly increased. This suggested that constraints at larger scales lead to ES segregation. Correlations among ES variations changed with the scenarios (Spearman’s $\rho$ between CS and CP was -0.43 for DEP and -0.70 for FRANCE). This indicated that different land use strategies produce different degrees of enhancement/softening of ES trade-offs/synergies. Conclusions: A trade-off was highlighted: larger spatial scales promoted better performance of the target ES but also spatial inequality. We argue that addressing smaller scales will lead to land-sharing solutions that avoid the local environmental impacts of land-sparing strategies.}
}

@article{smetana2023environmental,
    author = {Smetana, Sergiy and Bhatia, Anita and Batta, Uday and Mouhrim, Nisrine and Tonda, Alberto},
    title = "{Environmental impact potential of insect production chains for food and feed in Europe}",
    journal = {Animal Frontiers},
    volume = {13},
    number = {4},
    pages = {112-120},
    year = {2023},
    month = {08},
    issn = {2160-6056},
    doi = {10.1093/af/vfad033},
    url = {https://doi.org/10.1093/af/vfad033},
    abstract = {Insects can address sustainability issues associated with current food systems by providing an alternative protein source to address hunger and disease. Only the production systems that rely on side-stream heat and alternate energy sources may benefit from replacing compound feed production with insect value chains. 75\% percent to 93\% of the effects of compound feed production on global warming potential, land use, and fossil resource shortages can be avoided. To fully assess the potential of insect production, it is critical to consider a wide range of sustainability indicators, including social, economic, and environmental aspects.}
}

@article{tonda2023anintercontinental,
    title = {An intercontinental machine learning analysis of factors explaining consumer awareness of food risk},
    journal = {Future Foods},
    volume = {7},
    pages = {100233},
    year = {2023},
    issn = {2666-8335},
    doi = {https://doi.org/10.1016/j.fufo.2023.100233},
    url = {https://www.sciencedirect.com/science/article/pii/S2666833523000199},
    author = {Alberto Tonda and Christian Reynolds and Rallou Thomopoulos},
    abstract = {Food safety is a common concern at the household level, with important variations across different countries and cultures. Nevertheless, identifying the factors that best explain similarities and differences in consumer awareness pertaining to this topic is not straightforward. Starting from a questionnaire administered in seven countries from four continents (Argentina, Brazil, Colombia, Ghana, India, Peru, and the United Kingdom), we present an analysis of the answers related to food safety concerns, aimed at identifying possible explanatory factors. As classical statistical approaches can be limited when dealing with complex datasets, we propose an analysis with machine learning techniques, that can take into account both categorical and numerical values. With the questionnaire as a base, we task a machine learning algorithm, Random Forest, with predicting consumers’ answers to the target questions using information from all other answers. Once the algorithm is trained, it becomes possible to obtain a ranking of the questions considered the most important for the prediction, with the top-ranked questions likely representing explanatory factors. Top-ranked questions are then analyzed using a Random Forest regression algorithm, to test possible correlations. The results show that the most significant explanatory variables of safety concerns seem to be estimates of carbon footprints and calories associated with food products, and primarily with beef and chicken meat. These results tend to indicate that people who are most concerned about food safety are also those who are highly aware of environmental and nutritional impacts of food, hinting at differences in food education as a possible underlying explanation for the data.}
}

@article{papoutsoglou2023machine,
    doi = {10.3389/fmicb.2023.1261889},
    url = {https://doi.org/10.3389/fmicb.2023.1261889},
    year = {2023},
    month = sep,
    publisher = {Frontiers Media {SA}},
    volume = {14},
    author = {Georgios Papoutsoglou and Sonia Tarazona and Marta B. Lopes and Thomas Klammsteiner and Eliana Ibrahimi and Julia Eckenberger and Pierfrancesco Novielli and Alberto Tonda and Andrea Simeon and Rajesh Shigdel and St{\'{e}}phane B{\'{e}}reux and Giacomo Vitali and Sabina Tangaro and Leo Lahti and Andriy Temko and Marcus J. Claesson and Magali Berland},
    title = {Machine learning approaches in microbiome research: challenges and best practices},
    journal = {Frontiers in Microbiology},
    abstract = {Microbiome data predictive analysis within a machine learning (ML) workflow presents numerous domain-specific challenges involving preprocessing, feature selection, predictive modeling, performance estimation, model interpretation, and the extraction of biological information from the results. To assist decision-making, we offer a set of recommendations on algorithm selection, pipeline creation and evaluation, stemming from the COST Action ML4Microbiome. We compared the suggested approaches on a multi-cohort shotgun metagenomics dataset of colorectal cancer patients, focusing on their performance in disease diagnosis and biomarker discovery. It is demonstrated that the use of compositional transformations and filtering methods as part of data preprocessing does not always improve the predictive performance of a model. In contrast, the multivariate feature selection, such as the Statistically Equivalent Signatures algorithm, was effective in reducing the classification error. When validated on a separate test dataset, this algorithm in combination with random forest modeling, provided the most accurate performance estimates. Lastly, we showed how linear modeling by logistic regression coupled with visualization techniques such as Individual Conditional Expectation (ICE) plots can yield interpretable results and offer biological insights. These findings are significant for clinicians and non-experts alike in translational applications.}
}

@article{perezromero2023aninnovative,
    doi = {10.1038/s41598-023-42348-y},
    url = {https://doi.org/10.1038/s41598-023-42348-y},
    year = {2023},
    month = sep,
    publisher = {Springer Science and Business Media {LLC}},
    volume = {13},
    number = {1},
    author = {Carmina Angelica Perez-Romero and Lucero Mendoza-Maldonado and Alberto Tonda and Etienne Coz and Patrick Tabeling and Jessica Vanhomwegen and John MacSharry and Joanna Szafran and Lucina Bobadilla-Morales and Alfredo Corona-Rivera and Eric Claassen and Johan Garssen and Aletta D. Kraneveld and Alejandro Lopez-Rincon},
    title = {An Innovative {AI}-based primer design tool for precise and accurate detection of {SARS}-{CoV}-2 variants of concern},
    journal = {Scientific Reports},
    abstract = {As the COVID-19 pandemic winds down, it leaves behind the serious concern that future, even more disruptive pandemics may eventually surface. One of the crucial steps in handling the SARS-CoV-2 pandemic was being able to detect the presence of the virus in an accurate and timely manner, to then develop policies counteracting the spread. Nevertheless, as the pandemic evolved, new variants with potentially dangerous mutations appeared. Faced by these developments, it becomes clear that there is a need for fast and reliable techniques to create highly specific molecular tests, able to uniquely identify VOCs. Using an automated pipeline built around evolutionary algorithms, we designed primer sets for SARS-CoV-2 (main lineage) and for VOC, B.1.1.7 (Alpha) and B.1.1.529 (Omicron). Starting from sequences openly available in the GISAID repository, our pipeline was able to deliver the primer sets for the main lineage and each variant in a matter of hours. Preliminary in-silico validation showed that the sequences in the primer sets featured high accuracy. A pilot test in a laboratory setting confirmed the results: the developed primers were favorably compared against existing commercial versions for the main lineage, and the specific versions for the VOCs B.1.1.7 and B.1.1.529 were clinically tested successfully.}
}

@article{kidwai2023arobust,
  title = {A robust mRNA signature obtained via recursive ensemble feature selection predicts the responsiveness of omalizumab in moderate‐to‐severe asthma},
  volume = {13},
  ISSN = {2045-7022},
  url = {http://dx.doi.org/10.1002/clt2.12306},
  DOI = {10.1002/clt2.12306},
  number = {11},
  journal = {Clinical and Translational Allergy},
  publisher = {Wiley},
  author = {Kidwai,  Sarah and Barbiero,  Pietro and Meijerman,  Irma and Tonda,  Alberto and Perez‐Pardo,  Paula and Lio ́,  Pietro and van der Maitland‐Zee,  Anke H. and Oberski,  Daniel L. and Kraneveld,  Aletta D. and Lopez‐Rincon,  Alejandro},
  year = {2023},
  month = nov,
  abstract = {Background: Not being well controlled by therapy with inhaled corticosteroids and long-acting \beta2 agonist bronchodilators is a major concern for severe-asthma patients. The current treatment option for these patients is the use of biologicals such as anti-IgE treatment, omalizumab, as an add-on therapy. Despite the accepted use of omalizumab, patients do not always benefit from it. Therefore, there is a need to identify reliable biomarkers as predictors of omalizumab response. Methods: Two novel computational algorithms, machine-learning based Recursive Ensemble Feature Selection (REFS) and rule-based algorithm Logic Explainable Networks (LEN), were used on open accessible mRNA expression data from moderate-to-severe asthma patients to identify genes as predictors of omalizumab response. Results: With REFS, the number of features was reduced from 28,402 genes to 5 genes while obtaining a cross-validated accuracy of 0.975. The 5 responsiveness predictive genes encode the following proteins: Coiled-coil domain- containing protein 113 (CCDC113), Solute Carrier Family 26 Member 8 (SLC26A), Protein Phosphatase 1 Regulatory Subunit 3D (PPP1R3D), C-Type lectin Domain Family 4 member C (CLEC4C) and LOC100131780 (not annotated). The LEN algorithm found 4 identical genes with REFS: CCDC113, SLC26A8 PPP1R3D and LOC100131780. Literature research showed that the 4 identified responsiveness predicting genes are associated with mucosal immunity, cell metabolism, and airway remodeling.}
}

@article{perrot2023predicting,
  title = {Predicting odor profile of food from its chemical composition: Towards an approach based on artificial intelligence and flavorists expertise},
  volume = {20},
  ISSN = {1551-0018},
  url = {http://dx.doi.org/10.3934/mbe.2023908},
  DOI = {10.3934/mbe.2023908},
  number = {12},
  journal = {Mathematical Biosciences and Engineering},
  publisher = {American Institute of Mathematical Sciences (AIMS)},
  author = {Perrot,  N. Mejean and Roche,  Alice and Tonda,  Alberto and Lutton,  Evelyne and Thomas-Danguin,  Thierry},
  year = {2023},
  pages = {20528–20552},
  abstract = {Odor is central to food quality. Still, a major challenge is to understand how the odorants present in a given food contribute to its specific odor profile, and how to predict this olfactory outcome from the chemical composition. In this proof-of-concept study, we seek to develop an integrative model that combines expert knowledge, fuzzy logic, and machine learning to predict the quantitative odor description of complex mixtures of odorants. The model output is the intensity of relevant odor sensory attributes calculated on the basis of the content in odor-active comounds. The core of the model is the mathematically formalized knowledge of four senior flavorists, which provided a set of optimized rules describing the sensory-relevant combinations of odor qualities the experts have in mind to elaborate the target odor sensory attributes. The model first queries analytical and sensory databases in order to standardize, homogenize, and quantitatively code the odor descriptors of the odorants. Then the standardized odor descriptors are translated into a limited number of odor qualities used by the experts thanks to an ontology. A third step consists of aggregating all the information in terms of odor qualities across all the odorants found in a given product. The final step is a set of knowledge-based fuzzy membership functions representing the flavorist expertise and ensuring the prediction of the intensity of the target odor sensory descriptors on the basis of the products' aggregated odor qualities; several methods of optimization of the fuzzy membership functions have been tested. Finally, the model was applied to predict the odor profile of 16 red wines from two grape varieties for which the content in odorants was available. The results showed that the model can predict the perceptual outcome of food odor with a certain level of accuracy, and may also provide insights into combinations of odorants not mentioned by the experts.}
}

@article{mouhrim2023optimization,
  title = {Optimization models for sustainable insect production chains},
  ISSN = {2352-4588},
  url = {http://dx.doi.org/10.1163/23524588-20230148},
  DOI = {10.1163/23524588-20230148},
  journal = {Journal of Insects as Food and Feed},
  publisher = {Brill},
  author = {Mouhrim,  N. and Peguero,  D.A. and Green,  A. and Silva,  B. and Bhatia,  A. and Ristic,  D. and Tonda,  A. and Mathys,  A. and Smetana,  S.},
  year = {2023},
  month = nov,
  pages = {1–19},
  abstract = {Insect value chains are a complex system with non-linear links between many economic, environmental, and social variables. Multi-objective optimization (MOO) algorithms for finding optimal options for complex system functioning can provide a valuable insight in the development of sustainable insect chains. This review proposes a framework for MOO application that is based on gradual implementation, beginning with factors that have an immediate impact on insect production (feed qualities, resource utilization, yield), and progressing to integrated units (environmental, social, and economic impacts). The review introduces the key hotspots of insect production chains, which have been developed in suitable MOO objectives. They represent aspects of resource use, feed quality and its conversion by insects, labor safety and wage fairness, as well as environmental impacts. The capacity of the suggested MOO framework to describe all facets of sustainability may have certain limits. To determine the framework’s applicability and the specific MOO algorithms that can perform the function, modeling and further testing on real insect production chains would be necessary for the intended objectives.}
}

@article{rojasvelazquez2024methodology,
  title = {Methodology for biomarker discovery with reproducibility in microbiome data using machine learning},
  volume = {25},
  ISSN = {1471-2105},
  url = {http://dx.doi.org/10.1186/s12859-024-05639-3},
  DOI = {10.1186/s12859-024-05639-3},
  number = {1},
  journal = {BMC Bioinformatics},
  publisher = {Springer Science and Business Media LLC},
  author = {Rojas-Velazquez,  David and Kidwai,  Sarah and Kraneveld,  Aletta D. and Tonda,  Alberto and Oberski,  Daniel and Garssen,  Johan and Lopez-Rincon,  Alejandro},
  year = {2024},
  month = jan,
  abstract = {Background: In recent years, human microbiome studies have received increasing attention as this field is considered a potential source for clinical applications. With the advancements in omics technologies and AI, research focused on the discovery for potential biomarkers in the human microbiome using machine learning tools has produced positive outcomes. Despite the promising results, several issues can still be found in these studies such as datasets with small number of samples, inconsistent results, lack of uniform processing and methodologies, and other additional factors lead to lack of reproducibility in biomedical research. In this work, we propose a methodology that combines the DADA2 pipeline for 16s rRNA sequences processing and the Recursive Ensemble Feature Selection (REFS) in multiple datasets to increase reproducibility and obtain robust and reliable results in biomedical research.
  Results: Three experiments were performed analyzing microbiome data from patients/cases in Inflammatory Bowel Disease (IBD), Autism Spectrum Disorder (ASD), and Type 2 Diabetes (T2D). In each experiment, we found a biomarker signature in one dataset and applied to 2 other as further validation. The effectiveness of the proposed methodology was compared with other feature selection methods such as K-Best with F-score and random selection as a base line. The Area Under the Curve (AUC) was employed as a measure of diagnostic accuracy and used as a metric for comparing the results of the proposed methodology with other feature selection methods. Additionally, we use the Matthews Correlation Coefficient (MCC) as a metric to evaluate the performance of the methodology as well as for comparison with other feature selection methods.
  Conclusions: We developed a methodology for reproducible biomarker discovery for 16s rRNA microbiome sequence analysis, addressing the issues related with data dimensionality, inconsistent results and validation across independent datasets. The findings from the three experiments, across 9 different datasets, show that the proposed methodology achieved higher accuracy compared to other feature selection methods. This methodology is a first approach to increase reproducibility, to provide robust and reliable results.}
}

@article{jarmatz2024development,
  title = {Development of a soft sensor for fouling prediction in pipe fittings using the example of particulate deposition from suspension flow},
  volume = {145},
  ISSN = {0960-3085},
  url = {http://dx.doi.org/10.1016/j.fbp.2024.02.009},
  DOI = {10.1016/j.fbp.2024.02.009},
  journal = {Food and Bioproducts Processing},
  publisher = {Elsevier BV},
  author = {Jarmatz,  Niklas and Augustin,  Wolfgang and Scholl,  Stephan and Tonda,  Alberto and Delaplace,  Guillaume},
  year = {2024},
  month = may,
  pages = {116–127},
  abstract = {Fouling is the unwanted accumulation of material on a processing surface which is an especially problematic issue in the food industry. Characterizing or predicting fouling through traditional methods or models is a challenge due to the complexity of fouling mechanisms. Machine learning (ML) techniques can overcome this challenge by creating models for prediction directly from experimental data. Unfortunately, the results can be hard to interpret depending on the algorithm. Here, a soft sensor is generated from an extensive data set to predict the fouling of a model particle material system. This is performed inside two different pipe fittings, an inaccessible and accessible fitting (e.g., for sensor measurements). Additionally, dimensional analysis (DA) is conducted to identify the correlations responsible for fouling while keeping descriptors with physical meaning. The resulting dimensionless numbers (DNs) are further processed by three ML algorithms: linear regression (LR), symbolic regression (SR), and random forest (RF). The soft sensor generated using a RF outperformed the other two regressors for the dimensional (Q2=0.90±0.08) and for the dimensionless data (Q2=0.88±0.09). The parameter time and particle mass fraction were determined to be most influential. Furthermore, seven DNs were obtained allowing a reduced experimental design.}
}