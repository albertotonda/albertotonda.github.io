@incollection{barbiero2022predictable,
 abstract = {We propose an unsupervised, model-agnostic, wrapper method for feature selection. We assume that if a feature can be predicted using the others, it adds little information to the problem, and therefore could be removed without impairing the performance of whatever model will be eventually built. The proposed method iteratively identifies and removes predictable, or nearly-predictable, redundant features, allowing to trade-off complexity with expected quality. The approach do not rely on target labels nor values, and the model used to identify predictable features is not related to the final use of the feature set. Therefore, it can be used for supervised, unsupervised, or semi-supervised problems, or even as a safe, pre-processing step to improve the quality of the results of other feature selection techniques. Experimental results against state-of-the-art feature-selection algorithms show satisfying performance on several non-trivial benchmarks.},
 author = {Pietro Barbiero and Giovanni Squillero and Alberto Tonda},
 booktitle = {Machine Learning,  Optimization,  and Data Science},
 doi = {10.1007/978-3-030-95467-3_29},
 pages = {399--412},
 publisher = {Springer International Publishing},
 title = {Predictable Features Elimination: An Unsupervised Approach to Feature Selection},
 url = {https://doi.org/10.1007/978-3-030-95467-3_29},
 year = {2022}
}
