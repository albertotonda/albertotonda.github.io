<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Optimization | Alberto Tonda</title><link>https://albertotonda.github.io/tags/optimization/</link><atom:link href="https://albertotonda.github.io/tags/optimization/index.xml" rel="self" type="application/rss+xml"/><description>Optimization</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 07 Apr 2025 00:00:00 +0000</lastBuildDate><image><url>https://albertotonda.github.io/media/icon_hu_426aaca0d42cf8d6.png</url><title>Optimization</title><link>https://albertotonda.github.io/tags/optimization/</link></image><item><title>Optimization algorithms for artificial intelligence</title><link>https://albertotonda.github.io/teaching/optimization-algorithms-for-artificial-intelligence/</link><pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate><guid>https://albertotonda.github.io/teaching/optimization-algorithms-for-artificial-intelligence/</guid><description>&lt;p>The aim of this class is to provide Ph.D. students with an overview of modern optimization algorithms, highlighting strengths and weaknesses of each method. Students will discover that optimization can be applied to continuous or discrete numerical values, and even to more complex and expressive structures, such as trees or graphs.&lt;/p>
&lt;h2 id="introduction-to-the-class">Introduction to the class&lt;/h2>
&lt;p>
&lt;br>
&lt;/p>
&lt;h2 id="continuous-optimization">Continuous optimization&lt;/h2>
&lt;p>
&lt;br>
&lt;/p>
&lt;h2 id="linear-programming-and-discrete-optimization">Linear programming and discrete optimization&lt;/h2>
&lt;p>
&lt;br>
&lt;br>
&lt;/p>
&lt;h2 id="multi-objective-optimization">Multi-objective optimization&lt;/h2>
&lt;p>
&lt;br>
&lt;/p>
&lt;h2 id="optimization-of-complex-structures">Optimization of complex structures&lt;/h2>
&lt;p>
&lt;br>
&lt;/p>
&lt;h2 id="advanced-optimization-techniques">Advanced optimization techniques&lt;/h2>
&lt;p>
&lt;/p>
&lt;h2 id="optimization-in-machine-learning">Optimization in machine learning&lt;/h2>
&lt;p>
&lt;br>
&lt;br>
&lt;/p>
&lt;h2 id="hyperparameter-optimization-and-automl">Hyperparameter optimization and AutoML&lt;/h2>
&lt;p>
&lt;br>
&lt;/p></description></item><item><title>COST Action ROAR-NET</title><link>https://albertotonda.github.io/project/2023-roar-net/</link><pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate><guid>https://albertotonda.github.io/project/2023-roar-net/</guid><description>&lt;p>Randomised Optimisation Algorithms Research Network (CA22137 ROAR-NET) is a COST Action, a 4-year networking project funded by the
. This COST Action aims at making randomised optimisation algorithms widely competitive in practice by identifying and reducing obstacles to their adoption at the scientific, technical, economic, and human levels. It focuses on meeting the needs of practitioners, from whose activities the economic value of optimisation solvers stems. These needs are taken as the driving force for new theoretical, methodological, and technical advances leading to the sustainable development of widely available software tools, training materials and programmes, and ultimately to more extensive acceptance and deployment of these methods.&lt;/p>
&lt;p>In this project, I am a regular participant. For more information, check out the
.&lt;/p></description></item></channel></rss>